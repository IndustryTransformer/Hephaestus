{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import hephaestus as hp\n",
    "\n",
    "import hashlib\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>carat</th><th>cut</th><th>color</th><th>clarity</th><th>depth</th><th>table</th><th>price</th><th>x</th><th>y</th><th>z</th></tr><tr><td>f64</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.23</td><td>&quot;Ideal&quot;</td><td>&quot;E&quot;</td><td>&quot;SI2&quot;</td><td>61.5</td><td>55.0</td><td>326</td><td>3.95</td><td>3.98</td><td>2.43</td></tr><tr><td>0.21</td><td>&quot;Premium&quot;</td><td>&quot;E&quot;</td><td>&quot;SI1&quot;</td><td>59.8</td><td>61.0</td><td>326</td><td>3.89</td><td>3.84</td><td>2.31</td></tr><tr><td>0.23</td><td>&quot;Good&quot;</td><td>&quot;E&quot;</td><td>&quot;VS1&quot;</td><td>56.9</td><td>65.0</td><td>327</td><td>4.05</td><td>4.07</td><td>2.31</td></tr><tr><td>0.29</td><td>&quot;Premium&quot;</td><td>&quot;I&quot;</td><td>&quot;VS2&quot;</td><td>62.4</td><td>58.0</td><td>334</td><td>4.2</td><td>4.23</td><td>2.63</td></tr><tr><td>0.31</td><td>&quot;Good&quot;</td><td>&quot;J&quot;</td><td>&quot;SI2&quot;</td><td>63.3</td><td>58.0</td><td>335</td><td>4.34</td><td>4.35</td><td>2.75</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 10)\n",
       "┌───────┬─────────┬───────┬─────────┬───┬───────┬──────┬──────┬──────┐\n",
       "│ carat ┆ cut     ┆ color ┆ clarity ┆ … ┆ price ┆ x    ┆ y    ┆ z    │\n",
       "│ ---   ┆ ---     ┆ ---   ┆ ---     ┆   ┆ ---   ┆ ---  ┆ ---  ┆ ---  │\n",
       "│ f64   ┆ str     ┆ str   ┆ str     ┆   ┆ i64   ┆ f64  ┆ f64  ┆ f64  │\n",
       "╞═══════╪═════════╪═══════╪═════════╪═══╪═══════╪══════╪══════╪══════╡\n",
       "│ 0.23  ┆ Ideal   ┆ E     ┆ SI2     ┆ … ┆ 326   ┆ 3.95 ┆ 3.98 ┆ 2.43 │\n",
       "│ 0.21  ┆ Premium ┆ E     ┆ SI1     ┆ … ┆ 326   ┆ 3.89 ┆ 3.84 ┆ 2.31 │\n",
       "│ 0.23  ┆ Good    ┆ E     ┆ VS1     ┆ … ┆ 327   ┆ 4.05 ┆ 4.07 ┆ 2.31 │\n",
       "│ 0.29  ┆ Premium ┆ I     ┆ VS2     ┆ … ┆ 334   ┆ 4.2  ┆ 4.23 ┆ 2.63 │\n",
       "│ 0.31  ┆ Good    ┆ J     ┆ SI2     ┆ … ┆ 335   ┆ 4.34 ┆ 4.35 ┆ 2.75 │\n",
       "└───────┴─────────┴───────┴─────────┴───┴───────┴──────┴──────┴──────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.read_csv(\"../data/diamonds.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = hp.make_lower_remove_special_chars(df)\n",
    "val_tokens = hp.get_unique_utf8_values(df)\n",
    "col_tokens = hp.get_col_tokens(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = np.array(\n",
    "    [\n",
    "        \"missing\",\n",
    "        \"<mask>\",\n",
    "        \"<numeric_mask>\" \"<pad>\",\n",
    "        \"<unk>\",\n",
    "        \":\",\n",
    "        \",\",\n",
    "        \"<row-start>\",\n",
    "        \"<row-end>\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([',', ':', '<mask>', '<numeric>', '<numeric_mask><pad>',\n",
       "       '<row-end>', '<row-start>', '<unk>', 'carat', 'clarity', 'color',\n",
       "       'cut', 'd', 'depth', 'e', 'f', 'fair', 'g', 'good', 'h', 'i', 'i1',\n",
       "       'ideal', 'if', 'j', 'missing', 'premium', 'price', 'si1', 'si2',\n",
       "       'table', 'very good', 'vs1', 'vs2', 'vvs1', 'vvs2', 'x', 'y', 'z'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            val_tokens,\n",
    "            col_tokens,\n",
    "            special_tokens,\n",
    "            np.array(\n",
    "                [\n",
    "                    \"<numeric>\",\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>hash</th></tr><tr><td>u32</td></tr></thead><tbody><tr><td>685</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 1)\n",
       "┌──────┐\n",
       "│ hash │\n",
       "│ ---  │\n",
       "│ u32  │\n",
       "╞══════╡\n",
       "│ 685  │\n",
       "└──────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "    df.with_columns(\n",
    "        pl.concat_str(pl.all().exclude(\"price\").cast(pl.Utf8)).alias(\"all_cols\")\n",
    "    )\n",
    "    .with_columns(\n",
    "        pl.col(\"all_cols\")\n",
    "        .apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "        .alias(\"hash\")\n",
    "    )\n",
    "    .drop(\"all_cols\")\n",
    ")\n",
    "df.select(pl.col(\"hash\").is_duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.8\n",
    "n_train = int(train_fraction * len(df))\n",
    "train_test_df = df.select(pl.all().exclude([\"price\", \"hash\"]))\n",
    "\n",
    "train, test = train_test_df.head(n_train), train_test_df.tail(\n",
    "    len(train_test_df) - n_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "ds = hp.TabularDataset(\n",
    "    train,\n",
    "    tokens,\n",
    "    special_tokens=special_tokens,\n",
    "    shuffle_cols=True,\n",
    "    max_row_length=50,\n",
    ")\n",
    "\n",
    "print(len(ds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_built():\n",
    "    device_name = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device_name = \"cuda\"\n",
    "else:\n",
    "    device_name = \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringNumericEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dict,\n",
    "        device: torch.device,\n",
    "        tokenizer,\n",
    "        bert_model_name=\"bert-base-uncased\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert_tokenizer = tokenizer\n",
    "        self.word_embeddings = nn.Embedding(*state_dict[\"weight\"].shape).to(device)\n",
    "        self.word_embeddings.load_state_dict(state_dict)  # .to(device)\n",
    "        self.numeric_embedding = nn.Sequential(\n",
    "            nn.Linear(1, 128),  # First hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),  # Second hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, state_dict[\"weight\"].shape[1]),  # Output layer\n",
    "        ).to(device)\n",
    "\n",
    "        # self.numeric_embedding = nn.Linear(1, d_model).to(device)\n",
    "\n",
    "    def forward(self, input: hp.StringNumeric):\n",
    "        tensor_list = []\n",
    "        for idx, val in enumerate(input):\n",
    "            if val.is_numeric:\n",
    "                val = torch.Tensor([val.value]).float().to(self.device)\n",
    "                val = self.numeric_embedding(val)\n",
    "                val = val.reshape(1, 1, -1)  # val.shape[0])\n",
    "                tensor_list.append(val)\n",
    "            else:\n",
    "                tokens_ids = self.bert_tokenizer.encode_plus(\n",
    "                    val.value, return_tensors=\"pt\", add_special_tokens=False\n",
    "                )\n",
    "                tensor_list.append(\n",
    "                    self.word_embeddings(tokens_ids[\"input_ids\"].to(self.device))\n",
    "                )\n",
    "\n",
    "        # return tensor_list\n",
    "        return torch.cat(tensor_list, dim=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridBertModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device: torch.device,\n",
    "        bert_model_name=\"bert-base-uncased\",\n",
    "    ):\n",
    "        super(HybridBertModel, self).__init__()\n",
    "\n",
    "        # BERT Tokenizer and Model\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert_lm = BertForPreTraining.from_pretrained(\"bert-base-uncased\")\n",
    "        # self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.tokenizer.add_tokens(\n",
    "            [\n",
    "                \"<numeric>\",\n",
    "                \"<numeric-mask>\",\n",
    "                \"<row-start>\",\n",
    "                \"<row-end>\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Add tokens to BERT model\n",
    "\n",
    "        # self.bert = BertModel.from_pretrained(bert_model_name).to(device)\n",
    "        self.bert_lm.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.bert_embedding_state_dict = (\n",
    "            self.bert_lm.bert.embeddings.word_embeddings.state_dict()\n",
    "        )\n",
    "        self.embedding_dim = self.bert_lm.bert.config.hidden_size\n",
    "        self.string_numeric_embd = StringNumericEmbedding(\n",
    "            state_dict=self.bert_embedding_state_dict,\n",
    "            device=device,\n",
    "            tokenizer=self.tokenizer,\n",
    "            bert_model_name=bert_model_name,\n",
    "        )\n",
    "        # self.decoder = nn.Linear(self.embedding_dim, len(self.tokenizer)).to(device)\n",
    "        # Numeric Neural Net for numbers prediction after BERT\n",
    "        self.numeric_predictor = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, 128), nn.ReLU(), nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input: hp.StringNumeric):\n",
    "        input = self.string_numeric_embd(input)\n",
    "        bert_output = self.bert_lm.bert(inputs_embeds=input)\n",
    "        last_hidden_state = bert_output.last_hidden_state\n",
    "        pooled_output = bert_output.pooler_output\n",
    "\n",
    "        bert_logits = self.bert_lm.cls(last_hidden_state, pooled_output)\n",
    "        numeric_prediction = self.numeric_predictor(last_hidden_state)\n",
    "        # mlm_output = self.decoder(mlm_output.last_hidden_state)\n",
    "        return bert_logits, numeric_prediction\n",
    "\n",
    "\n",
    "# Sample usage:\n",
    "\n",
    "# Assuming we have our input prepared as:\n",
    "# input_data = [\n",
    "#     # hp.StringNumeric(\"<row-start>\"),\n",
    "#     hp.StringNumeric(\"Hello\"),\n",
    "#     # hp.StringNumeric(42.0),\n",
    "#     hp.StringNumeric(\"world\"),\n",
    "#     # hp.StringNumeric(12),\n",
    "#     # hp.StringNumeric(\"<row-end>\"),\n",
    "# ]\n",
    "input_data = [\n",
    "    hp.StringNumeric(i)\n",
    "    for i in \"Hello Greg, my name is Kai, nice to [MASK] [MASK]!\".split()\n",
    "]\n",
    "input_data.append(hp.StringNumeric(42.0))\n",
    "model = HybridBertModel(device=device).to(device)\n",
    "# input_data_tensor = StringNumeric(input_data, device=device)\n",
    "cat_preds, numeric_preds = model(input_data)\n",
    "# print(type(mlm_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7592,  6754,  1010,  2026,  2171,  2003, 11928,  1010,  3835,  2000,\n",
       "           103,   103,   999, 30522]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello greg, my name is kai, nice to [MASK] [MASK]! <numeric>'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(token_ids=x[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', greg, my name is kai, welcome to be you!,'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
