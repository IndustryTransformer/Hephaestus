{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir='runs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat      cut color clarity  depth  table  price     x     y     z\n",
       "0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Load and preprocess the dataset (assuming you have a CSV file)\n",
    "df = pd.read_csv(\"../data/diamonds.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'price', 'x', 'y',\n",
       "       'z'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "        init.xavier_uniform_(module.weight, gain=5)\n",
    "        if module.bias is not None:\n",
    "            init.constant_(module.bias, 0.001)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        init.uniform_(module.weight, -0.4, 0.4)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        init.normal_(module.weight, mean=1, std=0.2)\n",
    "        init.constant_(module.bias, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ideal': 0,\n",
       " 'Premium': 1,\n",
       " 'Good': 2,\n",
       " 'Very Good': 3,\n",
       " 'Fair': 4,\n",
       " 'E': 5,\n",
       " 'I': 6,\n",
       " 'J': 7,\n",
       " 'H': 8,\n",
       " 'F': 9,\n",
       " 'G': 10,\n",
       " 'D': 11,\n",
       " 'SI2': 12,\n",
       " 'SI1': 13,\n",
       " 'VS1': 14,\n",
       " 'VS2': 15,\n",
       " 'VVS2': 16,\n",
       " 'VVS1': 17,\n",
       " 'I1': 18,\n",
       " 'IF': 19,\n",
       " 'cut': 20,\n",
       " 'color': 21,\n",
       " 'clarity': 22,\n",
       " 'carat': 23,\n",
       " 'depth': 24,\n",
       " 'table': 25,\n",
       " 'x': 26,\n",
       " 'y': 27,\n",
       " 'z': 28,\n",
       " 'PAD': 29,\n",
       " '[NUMERIC_MASK]': 30,\n",
       " '[MASK]': 31,\n",
       " 'price': 32}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_columns = [\"cut\", \"color\", \"clarity\"]\n",
    "num_columns = [\"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\"]\n",
    "cat_values = pd.unique(df[cat_columns].values.ravel(\"K\"))\n",
    "target_column = \"price\"\n",
    "tokens = list(\n",
    "    chain(\n",
    "        cat_values,\n",
    "        cat_columns,\n",
    "        num_columns,\n",
    "        [\"PAD\", \"[NUMERIC_MASK]\", \"[MASK]\"],\n",
    "        [target_column],\n",
    "    )\n",
    ")\n",
    "token_dict = {token: i for i, token in enumerate(tokens)}\n",
    "token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rnd_df = df.sample(frac=1, random_state=42)\n",
    "scaled_df = rnd_df.loc[:, cat_columns + num_columns + [target_column]]\n",
    "for col in cat_columns:\n",
    "    scaled_df[col] = scaled_df[col].map(token_dict)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numeric_scaled = scaler.fit_transform(scaled_df[num_columns].copy())\n",
    "scaled_df[num_columns] = numeric_scaled\n",
    "\n",
    "X = scaled_df.drop(\"price\", axis=1)\n",
    "y = scaled_df[\"price\"]\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "# Categorical columns\n",
    "X_train_cat_tensor = torch.tensor(X_train[cat_columns].values, dtype=torch.int32).to(\n",
    "    device\n",
    ")\n",
    "X_test_cat_tensor = torch.tensor(X_test[cat_columns].values, dtype=torch.int32).to(\n",
    "    device\n",
    ")\n",
    "# Numeric columns\n",
    "X_train_num_tensor = torch.tensor(X_train[num_columns].values, dtype=torch.float32).to(\n",
    "    device\n",
    ")\n",
    "X_test_num_tensor = torch.tensor(X_test[num_columns].values, dtype=torch.float32).to(\n",
    "    device\n",
    ")\n",
    "y_train_tensor = (\n",
    "    torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    ")\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>carat</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>-1.177071</td>\n",
       "      <td>0.244725</td>\n",
       "      <td>-0.652139</td>\n",
       "      <td>-1.570008</td>\n",
       "      <td>-1.518684</td>\n",
       "      <td>-1.514447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cut  color  clarity     carat     depth     table         x         y  \\\n",
       "1388    0     10       17 -1.177071  0.244725 -0.652139 -1.570008 -1.518684   \n",
       "\n",
       "             z  \n",
       "1388 -1.514447  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0, 10, 17], dtype=torch.int32),\n",
       " tensor([-1.1771,  0.2447, -0.6521, -1.5700, -1.5187, -1.5144]),\n",
       " tensor([559.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cat_tensor[0], X_train_num_tensor[0], y_train_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_token_dict = {v: k for k, v in token_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([32, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_model // n_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # self.initialize_parameters()\n",
    "        self.apply(initialize_parameters)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, input_feed_forward=False):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        if input_feed_forward:\n",
    "            q = (\n",
    "                self.q_linear(q)\n",
    "                .view(batch_size, -1, self.n_heads, self.d_head)\n",
    "                .transpose(1, 2)\n",
    "            )\n",
    "            k = (\n",
    "                self.k_linear(k)\n",
    "                .view(batch_size, -1, self.n_heads, self.d_head)\n",
    "                .transpose(1, 2)\n",
    "            )\n",
    "            v = (\n",
    "                self.v_linear(v)\n",
    "                .view(batch_size, -1, self.n_heads, self.d_head)\n",
    "                .transpose(1, 2)\n",
    "            )\n",
    "\n",
    "        attn_output, _ = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        attn_output = (\n",
    "            attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        )\n",
    "        out = self.out_linear(attn_output)\n",
    "        return out\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "        d_k = q.size(-1)\n",
    "        scaled_attention_logits = matmul_qk / (d_k**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += mask * -1e9\n",
    "\n",
    "        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        # self.initialize_parameters()\n",
    "        self.apply(initialize_parameters)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, input_feed_forward=False):\n",
    "        attn_output = self.multi_head_attention(q, k, v, mask, input_feed_forward)\n",
    "        out1 = self.layernorm1(q + attn_output)\n",
    "\n",
    "        ff_output = self.feed_forward(out1)\n",
    "        out2 = self.layernorm2(out1 + ff_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "# Parameters\n",
    "d_model = 64  # Embedding dimension\n",
    "n_heads = 4  # Number of attention heads\n",
    "seq_len_q = 10  # Sequence length for the query tensor\n",
    "seq_len_k = 20  # Sequence length for the key tensor\n",
    "batch_size = 32  # Batch size\n",
    "\n",
    "# Random data\n",
    "q = torch.rand((batch_size, seq_len_q, d_model))\n",
    "k = torch.rand((batch_size, seq_len_k, d_model))\n",
    "v = k  # Usually, value and key are the same in many applications\n",
    "\n",
    "# Model\n",
    "encoder_layer = TransformerEncoderLayer(d_model, n_heads)\n",
    "\n",
    "# Forward pass\n",
    "output = encoder_layer(q, k, v)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mask_tensor(tensor, model, probability=0.8):\n",
    "    if tensor.dtype == torch.float32:\n",
    "        is_numeric = True\n",
    "    elif tensor.dtype == torch.int32:\n",
    "        is_numeric = False\n",
    "    else:\n",
    "        raise ValueError(f\"Task {tensor.dtype} not supported.\")\n",
    "\n",
    "    tensor = tensor.clone()\n",
    "    bit_mask = torch.rand(tensor.shape) > probability\n",
    "    if is_numeric:\n",
    "        tensor[bit_mask] = torch.tensor(float(\"-Inf\"))\n",
    "    else:\n",
    "        tensor[bit_mask] = model.cat_mask_token\n",
    "    return tensor.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 9, 32]), torch.Size([3, 6]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TabTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokens,\n",
    "        numeric_col_tokens,\n",
    "        cat_col_tokens,\n",
    "        token_dict,\n",
    "        d_model=64,\n",
    "        n_heads=4,\n",
    "        device=device,\n",
    "    ):\n",
    "        super(TabTransformer, self).__init__()\n",
    "        self.device = device\n",
    "        self.d_model = d_model\n",
    "        self.tokens = tokens\n",
    "        self.token_dict = token_dict\n",
    "        self.decoder_dict = {v: k for k, v in token_dict.items()}\n",
    "        # Masks\n",
    "        self.cat_mask_token = torch.tensor(self.token_dict[\"[MASK]\"]).to(device)\n",
    "        self.numeric_mask_token = torch.tensor(self.token_dict[\"[NUMERIC_MASK]\"]).to(\n",
    "            device\n",
    "        )\n",
    "\n",
    "        self.col_tokens = cat_col_tokens + numeric_col_tokens\n",
    "        self.n_tokens = len(tokens)  # TODO Make this\n",
    "        # Embedding layers for categorical features\n",
    "        self.embeddings = nn.Embedding(self.n_tokens, self.d_model).to(device)\n",
    "        self.n_numeric_cols = len(numeric_col_tokens)\n",
    "        self.n_cat_cols = len(cat_col_tokens)\n",
    "        self.n_columns = self.n_numeric_cols + self.n_cat_cols\n",
    "        # self.numeric_embeddings = NumericEmbedding(d_model=self.d_model)\n",
    "        self.col_indices = torch.tensor(\n",
    "            [self.tokens.index(col) for col in self.col_tokens], dtype=torch.long\n",
    "        ).to(device)\n",
    "        self.numeric_indices = torch.tensor(\n",
    "            [self.tokens.index(col) for col in numeric_col_tokens], dtype=torch.long\n",
    "        ).to(device)\n",
    "        self.transformer_encoder1 = TransformerEncoderLayer(\n",
    "            d_model, n_heads=n_heads\n",
    "        ).to(device)\n",
    "        self.transformer_encoder2 = TransformerEncoderLayer(\n",
    "            d_model, n_heads=n_heads\n",
    "        ).to(device)\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 2, 1),\n",
    "            # nn.ReLU(),\n",
    "        ).to(device)\n",
    "\n",
    "        self.mlm_decoder = nn.Sequential(nn.Linear(d_model, self.n_tokens)).to(\n",
    "            device\n",
    "        )  # TODO try making more complex\n",
    "\n",
    "        self.mnm_decoder = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                self.n_columns * self.d_model, self.d_model * 4\n",
    "            ),  # Try making more complex\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.d_model * 4, self.n_numeric_cols),\n",
    "        ).to(device)\n",
    "\n",
    "        self.flatten_layer = nn.Linear(len(self.col_tokens), 1).to(device)\n",
    "        self.apply(initialize_parameters)\n",
    "\n",
    "    def forward(self, num_inputs, cat_inputs, task=\"regression\"):\n",
    "        # Embed column indices\n",
    "        repeated_col_indices = self.col_indices.unsqueeze(0).repeat(\n",
    "            num_inputs.size(0), 1\n",
    "        )\n",
    "        col_embeddings = self.embeddings(repeated_col_indices)\n",
    "\n",
    "        repeated_numeric_indices = self.numeric_indices.unsqueeze(0).repeat(\n",
    "            num_inputs.size(0), 1\n",
    "        )\n",
    "        numeric_col_embeddings = self.embeddings(repeated_numeric_indices)\n",
    "\n",
    "        cat_embeddings = self.embeddings(cat_inputs)\n",
    "\n",
    "        expanded_num_inputs = num_inputs.unsqueeze(2).repeat(1, 1, self.d_model)\n",
    "        inf_mask = (expanded_num_inputs == float(\"-inf\")).all(dim=2)\n",
    "        base_numeric = torch.zeros_like(expanded_num_inputs)\n",
    "\n",
    "        num_embeddings = (\n",
    "            numeric_col_embeddings[~inf_mask] * expanded_num_inputs[~inf_mask]\n",
    "        )\n",
    "        base_numeric[~inf_mask] = num_embeddings\n",
    "        base_numeric[inf_mask] = self.embeddings(self.numeric_mask_token)\n",
    "\n",
    "        query_embeddings = torch.cat([cat_embeddings, base_numeric], dim=1)\n",
    "        out = self.transformer_encoder1(\n",
    "            col_embeddings,\n",
    "            # query_embeddings,\n",
    "            query_embeddings,\n",
    "            query_embeddings\n",
    "            # col_embeddings, query_embeddings, query_embeddings\n",
    "        )\n",
    "        out = self.transformer_encoder2(out, out, out)\n",
    "\n",
    "        if task == \"regression\":\n",
    "            out = self.regressor(out)\n",
    "            out = self.flatten_layer(out.squeeze(-1))\n",
    "\n",
    "            return out\n",
    "        elif task == \"mlm\":\n",
    "            cat_out = self.mlm_decoder(out)\n",
    "            # print(f\"Out shape: {out.shape}, cat_out shape: {cat_out.shape}\")\n",
    "            numeric_out = out.view(out.size(0), -1)\n",
    "            # print(f\"numeric_out shape: {numeric_out.shape}\")\n",
    "            numeric_out = self.mnm_decoder(numeric_out)\n",
    "            return cat_out, numeric_out\n",
    "        else:\n",
    "            raise ValueError(f\"Task {task} not supported.\")\n",
    "\n",
    "\n",
    "no_price_tokens = tokens.copy()\n",
    "no_price_tokens.remove(\"price\")\n",
    "\n",
    "numeric_col_tokens = (\n",
    "    df.head().drop(\"price\", axis=1).select_dtypes(include=np.number).columns.to_list()\n",
    ")\n",
    "cat_col_tokens = df.head().select_dtypes(exclude=np.number).columns.to_list()\n",
    "\n",
    "model = TabTransformer(\n",
    "    no_price_tokens,\n",
    "    numeric_col_tokens=numeric_col_tokens,\n",
    "    cat_col_tokens=cat_col_tokens,\n",
    "    token_dict=token_dict,\n",
    ").to(device)\n",
    "batch_size = 3\n",
    "test_num = X_train_num_tensor[0:batch_size, :]\n",
    "test_num_mask = mask_tensor(test_num, model)\n",
    "test_cat = X_train_cat_tensor[0:batch_size, :]\n",
    "test_cat_mask = mask_tensor(test_cat, model)\n",
    "with torch.no_grad():\n",
    "    x = model(\n",
    "        test_num_mask,\n",
    "        test_cat_mask,\n",
    "        task=\"mlm\",\n",
    "    )\n",
    "x[0].shape, x[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ideal', 'G', 'VVS1']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[model.decoder_dict[i.item()] for i in X_train_cat_tensor[0 : 0 + 1, :][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actual': {'cut': 'Ideal',\n",
       "  'color': 'G',\n",
       "  'clarity': 'VVS1',\n",
       "  'carat': -1.1770710945129395,\n",
       "  'depth': 0.2447250783443451,\n",
       "  'table': -0.6521385312080383,\n",
       "  'x': -1.5700081586837769,\n",
       "  'y': -1.5186843872070312,\n",
       "  'z': -1.5144472122192383},\n",
       " 'masked': {'cut': 'Ideal',\n",
       "  'color': 'G',\n",
       "  'clarity': '[MASK]',\n",
       "  'carat': -1.1770710945129395,\n",
       "  'depth': -inf,\n",
       "  'table': -0.6521385312080383,\n",
       "  'x': -1.5700081586837769,\n",
       "  'y': -1.5186843872070312,\n",
       "  'z': -inf},\n",
       " 'pred': {'cut': 'E',\n",
       "  'color': 'J',\n",
       "  'clarity': 'IF',\n",
       "  'carat': -26.44237518310547,\n",
       "  'depth': -0.8387407660484314,\n",
       "  'table': 0.017967773601412773,\n",
       "  'x': 39.614593505859375,\n",
       "  'y': 66.12740325927734,\n",
       "  'z': -68.3674545288086}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_mask_pred(i, model, probability):\n",
    "    numeric_values = X_train_num_tensor[i : i + 1, :]\n",
    "    categorical_values = X_train_cat_tensor[i : i + 1, :]\n",
    "    numeric_masked = mask_tensor(numeric_values, model, probability=probability)\n",
    "    categorical_masked = mask_tensor(categorical_values, model, probability=probability)\n",
    "    # Predictions\n",
    "    with torch.no_grad():\n",
    "        cat_preds, numeric_preds = model(numeric_masked, categorical_masked, task=\"mlm\")\n",
    "    # Get the predicted tokens from cat_preds\n",
    "    cat_preds = cat_preds.argmax(dim=2)\n",
    "    # Get the words from the tokens\n",
    "    decoder_dict = model.decoder_dict\n",
    "    cat_preds = [decoder_dict[i.item()] for i in cat_preds[0]]\n",
    "\n",
    "    results_dict = {k: cat_preds[i] for i, k in enumerate(model.col_tokens)}\n",
    "    for i, k in enumerate(model.col_tokens[model.n_cat_cols :]):\n",
    "        results_dict[k] = numeric_preds[0][i].item()\n",
    "    # Get the masked values\n",
    "    categorical_masked = [decoder_dict[i.item()] for i in categorical_masked[0]]\n",
    "    numeric_masked = numeric_masked[0].tolist()\n",
    "    masked_values = categorical_masked + numeric_masked\n",
    "    # zip the masked values with the column names\n",
    "    masked_values = dict(zip(model.col_tokens, masked_values))\n",
    "    # Get the original values\n",
    "    categorical_values = [decoder_dict[i.item()] for i in categorical_values[0]]\n",
    "    numeric_values = numeric_values[0].tolist()\n",
    "    original_values = categorical_values + numeric_values\n",
    "    # zip the original values with the column names\n",
    "    original_values = dict(zip(model.col_tokens, original_values))\n",
    "    # print(numeric_masked)\n",
    "    # print(categorical_masked)\n",
    "    result_dict = {\n",
    "        \"actual\": original_values,\n",
    "        \"masked\": masked_values,\n",
    "        \"pred\": results_dict,\n",
    "    }\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "show_mask_pred(0, model, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Tabualr Modeling\n",
    "base_model_name = \"BetterPreTrain\"\n",
    "\n",
    "model_time = dt.now()\n",
    "model_time = model_time.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "model_name = f\"{base_model_name}_{model_time}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 Loss: 14.4455\n",
      "Epoch 5/100 Loss: 5.9586\n",
      "Epoch 7/100 Loss: 3.5236\n",
      "Epoch 10/100 Loss: 2.4071\n",
      "Epoch 12/100 Loss: 1.9122\n",
      "Epoch 14/100 Loss: 1.5690\n",
      "Epoch 16/100 Loss: 1.3454\n",
      "Epoch 19/100 Loss: 1.1029\n",
      "Epoch 21/100 Loss: 1.0029\n",
      "Epoch 23/100 Loss: 0.9342\n",
      "Epoch 25/100 Loss: 0.8361\n",
      "Epoch 28/100 Loss: 0.8184\n",
      "Epoch 30/100 Loss: 0.7799\n",
      "Epoch 32/100 Loss: 0.7069\n",
      "Epoch 35/100 Loss: 0.6857\n",
      "Epoch 37/100 Loss: 0.6665\n",
      "Epoch 39/100 Loss: 0.6344\n",
      "Epoch 41/100 Loss: 0.6081\n",
      "Epoch 44/100 Loss: 0.5640\n",
      "Epoch 46/100 Loss: 0.5319\n",
      "Epoch 48/100 Loss: 0.5379\n",
      "Epoch 50/100 Loss: 0.5033\n",
      "Epoch 53/100 Loss: 0.5069\n",
      "Epoch 55/100 Loss: 0.5129\n",
      "Epoch 57/100 Loss: 0.4745\n",
      "Epoch 60/100 Loss: 0.4816\n",
      "Epoch 62/100 Loss: 0.4588\n",
      "Epoch 64/100 Loss: 0.4521\n",
      "Epoch 66/100 Loss: 0.4237\n",
      "Epoch 69/100 Loss: 0.3926\n",
      "Epoch 71/100 Loss: 0.3906\n",
      "Epoch 73/100 Loss: 0.4563\n",
      "Epoch 75/100 Loss: 0.3339\n",
      "Epoch 78/100 Loss: 0.3578\n",
      "Epoch 80/100 Loss: 0.3725\n",
      "Epoch 82/100 Loss: 0.4078\n",
      "Epoch 85/100 Loss: 0.3512\n",
      "Epoch 87/100 Loss: 0.3696\n",
      "Epoch 89/100 Loss: 0.3221\n",
      "Epoch 91/100 Loss: 0.3257\n",
      "Epoch 94/100 Loss: 0.3252\n",
      "Epoch 96/100 Loss: 0.2872\n",
      "Epoch 98/100 Loss: 0.3051\n",
      "Epoch 100/100 Loss: 0.2538\n"
     ]
    }
   ],
   "source": [
    "# Masked Tabualr Modeling\n",
    "epochs = 100\n",
    "batch_size = 1000\n",
    "lr = 0.001\n",
    "mse_loss = nn.MSELoss()\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "summary_writer = SummaryWriter(\"runs/\" + model_name)\n",
    "\n",
    "batch_count = 0\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, X_train_num_tensor.size(0), batch_size):\n",
    "        numeric_values = X_train_num_tensor[i : i + batch_size, :]\n",
    "        categorical_values = X_train_cat_tensor[i : i + batch_size, :]\n",
    "        numeric_masked = mask_tensor(numeric_values, model, probability=0.8)\n",
    "        categorical_masked = mask_tensor(categorical_values, model, probability=0.8)\n",
    "        optimizer.zero_grad()\n",
    "        cat_preds, numeric_preds = model(numeric_masked, categorical_masked, task=\"mlm\")\n",
    "        cat_targets = torch.cat(\n",
    "            (\n",
    "                categorical_values,\n",
    "                model.numeric_indices.expand(categorical_values.size(0), -1),\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        cat_preds = cat_preds.permute(0, 2, 1)  # TODO investigate as possible bug\n",
    "        # print(\n",
    "        #     f\"cat_preds.shape: {cat_preds.shape}, cat_targets.shape: {cat_targets.shape}\"\n",
    "        # )\n",
    "        cat_loss = ce_loss(cat_preds, cat_targets)\n",
    "        numeric_loss = mse_loss(numeric_preds, numeric_values)\n",
    "        loss = cat_loss + numeric_loss  # TODO Look at scaling\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_count += 1\n",
    "        learning_rate = optimizer.param_groups[0][\"lr\"]\n",
    "        summary_writer.add_scalar(\"LossTrain/agg_mask\", loss.item(), batch_count)\n",
    "        summary_writer.add_scalar(\"LossTrain/mlm_loss\", cat_loss.item(), batch_count)\n",
    "        summary_writer.add_scalar(\n",
    "            \"LossTrain/mnm_loss\", numeric_loss.item(), batch_count\n",
    "        )\n",
    "        summary_writer.add_scalar(\"Metrics/mtm_lr\", learning_rate, batch_count)\n",
    "        if batch_count % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} Loss: {loss.item():,.4f}\")\n",
    "            # Test set\n",
    "            with torch.no_grad():\n",
    "                numeric_values = X_test_num_tensor\n",
    "                categorical_values = X_test_cat_tensor\n",
    "                numeric_masked = mask_tensor(numeric_values, model, probability=0.8)\n",
    "                categorical_masked = mask_tensor(\n",
    "                    categorical_values, model, probability=0.8\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                cat_preds, numeric_preds = model(\n",
    "                    numeric_masked, categorical_masked, task=\"mlm\"\n",
    "                )\n",
    "                cat_targets = torch.cat(\n",
    "                    (\n",
    "                        categorical_values,\n",
    "                        model.numeric_indices.expand(categorical_values.size(0), -1),\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "                cat_preds = cat_preds.permute(0, 2, 1)\n",
    "                # print(\n",
    "                #     f\"cat_preds.shape: {cat_preds.shape}, cat_targets.shape: {cat_targets.shape}\"\n",
    "                # )\n",
    "                cat_loss = ce_loss(cat_preds, cat_targets)\n",
    "                numeric_loss = mse_loss(numeric_preds, numeric_values)\n",
    "                loss = cat_loss + numeric_loss\n",
    "                summary_writer.add_scalar(\"LossTest/agg_loss\", loss.item(), batch_count)\n",
    "            summary_writer.add_scalar(\"LossTest/mlm_loss\", cat_loss.item(), batch_count)\n",
    "            summary_writer.add_scalar(\n",
    "                \"LossTest/mnm_loss\", numeric_loss.item(), batch_count\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actual': {'cut': 'Ideal',\n",
       "  'color': 'G',\n",
       "  'clarity': 'VVS1',\n",
       "  'carat': -1.1770710945129395,\n",
       "  'depth': 0.2447250783443451,\n",
       "  'table': -0.6521385312080383,\n",
       "  'x': -1.5700081586837769,\n",
       "  'y': -1.5186843872070312,\n",
       "  'z': -1.5144472122192383},\n",
       " 'masked': {'cut': 'Ideal',\n",
       "  'color': 'G',\n",
       "  'clarity': 'VVS1',\n",
       "  'carat': -1.1770710945129395,\n",
       "  'depth': 0.2447250783443451,\n",
       "  'table': -0.6521385312080383,\n",
       "  'x': -1.5700081586837769,\n",
       "  'y': -1.5186843872070312,\n",
       "  'z': -1.5144472122192383},\n",
       " 'pred': {'cut': 'Ideal',\n",
       "  'color': 'G',\n",
       "  'clarity': 'VVS1',\n",
       "  'carat': -1.3263070583343506,\n",
       "  'depth': 0.4353136718273163,\n",
       "  'table': -0.547584056854248,\n",
       "  'x': -1.5333627462387085,\n",
       "  'y': -2.008849620819092,\n",
       "  'z': -2.2112772464752197}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_mask_pred(\n",
    "    0, model, 0.8\n",
    ")  # Check for learning... XFKAT # Why is color `GOOD`????? TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning the previous model seems to work but when I pre-train, we run into issues. Let's try again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 Loss: 421,813.50 Test loss: 421,813.50\n",
      "Epoch 5/40 Loss: 349,646.06 Test loss: 349,646.06\n",
      "Epoch 7/40 Loss: 337,694.91 Test loss: 337,694.91\n",
      "Epoch 10/40 Loss: 349,826.78 Test loss: 349,826.78\n",
      "Epoch 12/40 Loss: 328,518.94 Test loss: 328,518.94\n",
      "Epoch 14/40 Loss: 310,538.12 Test loss: 310,538.12\n",
      "Epoch 16/40 Loss: 325,130.81 Test loss: 325,130.81\n",
      "Epoch 19/40 Loss: 313,784.94 Test loss: 313,784.94\n",
      "Epoch 21/40 Loss: 389,621.28 Test loss: 389,621.28\n",
      "Epoch 23/40 Loss: 322,446.25 Test loss: 322,446.25\n",
      "Epoch 25/40 Loss: 309,423.19 Test loss: 309,423.19\n",
      "Epoch 28/40 Loss: 318,806.47 Test loss: 318,806.47\n",
      "Epoch 30/40 Loss: 403,155.91 Test loss: 403,155.91\n",
      "Epoch 32/40 Loss: 358,183.34 Test loss: 358,183.34\n",
      "Epoch 35/40 Loss: 315,748.34 Test loss: 315,748.34\n",
      "Epoch 37/40 Loss: 309,868.69 Test loss: 309,868.69\n",
      "Epoch 39/40 Loss: 317,470.12 Test loss: 317,470.12\n"
     ]
    }
   ],
   "source": [
    "# Regression Model\n",
    "epochs = 40\n",
    "batch_size = 1000\n",
    "lr = 0.1\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# model_time = dt.now()\n",
    "# model_time = model_time.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "# model_name = f\"FullDSNFT{model_time}\"\n",
    "\n",
    "summary_writer = SummaryWriter(\"runs/\" + model_name)\n",
    "\n",
    "small_test = True\n",
    "train_set_size = 1000  # X_train_num_tensor.size(0)\n",
    "batch_count = 0\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, train_set_size, batch_size):\n",
    "        # for i in range(0, batch_size, batch_size):\n",
    "        num_inputs = X_train_num_tensor[i : i + batch_size, :]\n",
    "        cat_inputs = X_train_cat_tensor[i : i + batch_size, :]\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(num_inputs, cat_inputs)\n",
    "        loss = loss_fn(y_pred, y_train_tensor[i : i + batch_size, :])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_count += 1\n",
    "        learning_rate = optimizer.param_groups[0][\"lr\"]\n",
    "        summary_writer.add_scalar(\"LossTrain/regression_loss\", loss.item(), batch_count)\n",
    "        summary_writer.add_scalar(\"Metrics/regression_lr\", learning_rate, batch_count)\n",
    "\n",
    "    # Test set\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_num_tensor, X_test_cat_tensor)\n",
    "        loss = loss_fn(y_pred, y_test_tensor)\n",
    "        summary_writer.add_scalar(\"LossTest/regression_loss\", loss.item(), batch_count)\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs} Loss: {loss.item():,.2f} \"\n",
    "            + f\"Test loss: {loss.item():,.2f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 290,563.16\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_num_tensor[0:10, :], X_test_cat_tensor[0:10, :])\n",
    "    loss = loss_fn(y_pred, y_test_tensor[0:10])\n",
    "    print(f\"Test loss: {loss.item():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 1,780.74 Actual: 1,754.00 Diff: 26.74\n",
      "Predicted: 6,250.25 Actual: 6,927.00 Diff: -676.75\n",
      "Predicted: 1,176.17 Actual: 1,264.00 Diff: -87.83\n",
      "Predicted: 2,540.79 Actual: 2,278.00 Diff: 262.79\n",
      "Predicted: 3,224.54 Actual: 2,858.00 Diff: 366.54\n",
      "Predicted: 6,740.07 Actual: 8,133.00 Diff: -1,392.93\n",
      "Predicted: 813.26 Actual: 840.00 Diff: -26.74\n",
      "Predicted: 16,256.74 Actual: 16,792.00 Diff: -535.26\n",
      "Predicted: 827.36 Actual: 815.00 Diff: 12.36\n",
      "Predicted: 643.58 Actual: 734.00 Diff: -90.42\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\n",
    "        f\"Predicted: {y_pred[i].item():,.2f} Actual: {y_test_tensor[i].item():,.2f}\",\n",
    "        f\"Diff: {y_pred[i].item() - y_test_tensor[i].item():,.2f}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39m1\u001b[39;49m \u001b[39m/\u001b[39;49m \u001b[39m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "1 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000, Loss: 31,439,220.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rs/qflxwtyx6kvfj8jcqx5zm5hr0000gn/T/ipykernel_12919/1167013342.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_cat[col] = le.fit_transform(X_train_cat.loc[:, col])\n",
      "/var/folders/rs/qflxwtyx6kvfj8jcqx5zm5hr0000gn/T/ipykernel_12919/1167013342.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_cat[col] = le.transform(X_test_cat.loc[:, col])\n",
      "/var/folders/rs/qflxwtyx6kvfj8jcqx5zm5hr0000gn/T/ipykernel_12919/1167013342.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_cat[col] = le.fit_transform(X_train_cat.loc[:, col])\n",
      "/var/folders/rs/qflxwtyx6kvfj8jcqx5zm5hr0000gn/T/ipykernel_12919/1167013342.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_cat[col] = le.transform(X_test_cat.loc[:, col])\n",
      "/var/folders/rs/qflxwtyx6kvfj8jcqx5zm5hr0000gn/T/ipykernel_12919/1167013342.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_cat[col] = le.fit_transform(X_train_cat.loc[:, col])\n",
      "/var/folders/rs/qflxwtyx6kvfj8jcqx5zm5hr0000gn/T/ipykernel_12919/1167013342.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_cat[col] = le.transform(X_test_cat.loc[:, col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/2000, Loss: 669,738.62\n",
      "Epoch 401/2000, Loss: 439,377.12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 107\u001b[0m\n\u001b[1;32m    102\u001b[0m outputs \u001b[39m=\u001b[39m simple_model(\n\u001b[1;32m    103\u001b[0m     X_train_num_tensor,  \u001b[39m# [0:batch_size],\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     X_train_cat_tensor,  \u001b[39m# [0:batch_size],\u001b[39;00m\n\u001b[1;32m    105\u001b[0m )  \u001b[39m# Pass numeric and categorical tensors separately\u001b[39;00m\n\u001b[1;32m    106\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, y_train_tensor)\n\u001b[0;32m--> 107\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    108\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m200\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/pytorch/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/pytorch/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load and preprocess the dataset (assuming you have a CSV file)\n",
    "data = pd.read_csv(\"../data/diamonds.csv\")\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(\"price\", axis=1)\n",
    "y = data[\"price\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Preprocess categorical features\n",
    "cat_columns = [\"cut\", \"color\", \"clarity\"]\n",
    "X_train_cat = X_train[cat_columns]\n",
    "X_test_cat = X_test[cat_columns]\n",
    "\n",
    "label_encoders = {}\n",
    "for col in cat_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_train_cat[col] = le.fit_transform(X_train_cat.loc[:, col])\n",
    "    X_test_cat[col] = le.transform(X_test_cat.loc[:, col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Preprocess numeric features\n",
    "num_columns = [\"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\"]\n",
    "scaler = StandardScaler()\n",
    "X_train_num = scaler.fit_transform(X_train[num_columns])\n",
    "X_test_num = scaler.transform(X_test[num_columns])\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_cat_tensor = torch.tensor(X_train_cat.values, dtype=torch.int64)\n",
    "X_train_num_tensor = torch.tensor(X_train_num, dtype=torch.float32)\n",
    "X_test_cat_tensor = torch.tensor(X_test_cat.values, dtype=torch.int64)\n",
    "X_test_num_tensor = torch.tensor(X_test_num, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "# Define the neural network model\n",
    "class DiamondPricePredictor(nn.Module):\n",
    "    def __init__(self, num_input_dim, cat_embedding_sizes, hidden_dim):\n",
    "        super(DiamondPricePredictor, self).__init__()\n",
    "\n",
    "        # Embedding layers for categorical features\n",
    "        self.embeddings = nn.ModuleList(\n",
    "            [\n",
    "                nn.Embedding(num_classes, emb_size)\n",
    "                for num_classes, emb_size in cat_embedding_sizes\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        total_emb_dim = sum(emb_size for _, emb_size in cat_embedding_sizes)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(num_input_dim + total_emb_dim, hidden_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "        # self.fc1 = nn.Linear(num_input_dim + total_emb_dim, hidden_dim)\n",
    "        # self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, num_inputs, cat_inputs):\n",
    "        embeddings = [\n",
    "            embedding(cat_inputs[:, i]) for i, embedding in enumerate(self.embeddings)\n",
    "        ]\n",
    "        cat_features = torch.cat(embeddings, dim=1)\n",
    "        x = torch.cat([num_inputs, cat_features], dim=1)\n",
    "\n",
    "        x = self.predictor(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "num_input_dim = X_train_num.shape[1]\n",
    "cat_embedding_sizes = [\n",
    "    (len(le.classes_), min(50, (len(le.classes_) + 1) // 2))\n",
    "    for le in label_encoders.values()\n",
    "]\n",
    "hidden_dim = 64\n",
    "simple_model = DiamondPricePredictor(num_input_dim, cat_embedding_sizes, hidden_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(simple_model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 2000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = simple_model(\n",
    "        X_train_num_tensor,  # [0:batch_size],\n",
    "        X_train_cat_tensor,  # [0:batch_size],\n",
    "    )  # Pass numeric and categorical tensors separately\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():,.2f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Data: 30,910,758.00\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_num_tensor, X_test_cat_tensor)\n",
    "    mse = mean_squared_error(y_test_tensor, test_predictions)\n",
    "    print(f\"Mean Squared Error on Test Data: {mse:,.2f}\")  # 735,349.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Params in Tabular Model:261,809 Number of Params in Simple Model:45,900\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Number of Params in Tabular Model:{count_parameters(model):,}\",\n",
    "    f\"Number of Params in Simple Model:{count_parameters(simple_model):,}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 278,657.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the diamonds dataset\n",
    "diamonds_data = pd.read_csv(\"../data/diamonds.csv\")\n",
    "\n",
    "# Encode categorical features using LabelEncoder\n",
    "label_encoders = {}\n",
    "categorical_features = [\"cut\", \"color\", \"clarity\"]\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    diamonds_data[feature] = le.fit_transform(diamonds_data[feature])\n",
    "    label_encoders[feature] = le\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = diamonds_data.drop(\"price\", axis=1)\n",
    "y = diamonds_data[\"price\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train the XGBoost regressor\n",
    "xgb_regressor = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "xgb_regressor.fit(\n",
    "    X_train,  # <[0:batch_size],\n",
    "    y_train,  # [0:batch_size],\n",
    ")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:,.2f}\")\n",
    "\n",
    "# You can also access feature importance scores\n",
    "# feature_importances = xgb_regressor.feature_importances_\n",
    "# print(\"Feature Importance:\")\n",
    "# for feature, importance in zip(X.columns, feature_importances):\n",
    "#     print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas==2.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,700,000.00'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{1.7e6:,.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "lcc_arn": "arn:aws:sagemaker:us-west-2:385115691352:studio-lifecycle-config/base-installs-widgets",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
