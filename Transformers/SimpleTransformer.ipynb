{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat      cut color clarity  depth  table  price     x     y     z\n",
       "0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the dataset (assuming you have a CSV file)\n",
    "df = pd.read_csv(\"../data/diamonds.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'price', 'x', 'y',\n",
       "       'z'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = [\"cut\", \"color\", \"clarity\"]\n",
    "num_columns = [\"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\"]\n",
    "cat_values = pd.unique(df[cat_columns].values.ravel(\"K\"))\n",
    "target_column = \"price\"\n",
    "tokens = list(chain(cat_values, cat_columns, num_columns, [target_column]))\n",
    "token_dict = {token: i for i, token in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(len(token_dict), 64)\n",
    "cat_values_emb = torch.tensor(\n",
    "    [[token_dict[token] for token in row] for row in df[cat_columns].values],\n",
    "    dtype=torch.long,\n",
    ")\n",
    "\n",
    "col_names_emb = torch.tensor([token_dict[col] for col in df.columns], dtype=torch.long)\n",
    "embedding(col_names_emb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"price\", axis=1)\n",
    "y = df[\"price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Preprocess categorical features\n",
    "X_train_cat = X_train[cat_columns].copy()\n",
    "X_test_cat = X_test[cat_columns].copy()\n",
    "\n",
    "label_encoders = {}\n",
    "for col in cat_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_train_cat[col] = X_train_cat[col].map(token_dict)\n",
    "    X_test_cat[col] = X_test_cat[col].map(token_dict)\n",
    "    # label_encoders[col] = le\n",
    "\n",
    "# Preprocess numeric features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_num = scaler.fit_transform(X_train[num_columns].copy())\n",
    "X_test_num = scaler.transform(X_test[num_columns].copy())\n",
    "\n",
    "X_train_cat_tensor = torch.tensor(\n",
    "    X_train_cat.values, dtype=torch.int64\n",
    ")  # Use int64 dtype for categorical indices\n",
    "X_train_num_tensor = torch.tensor(X_train_num, dtype=torch.float32)\n",
    "X_test_cat_tensor = torch.tensor(\n",
    "    X_test_cat.values, dtype=torch.int64\n",
    ")  # Use int64 dtype for categorical indices\n",
    "X_test_num_tensor = torch.tensor(X_test_num, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50052</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41645</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42377</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17244</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44081</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23713</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31375</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21772</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10788 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cut  color  clarity\n",
       "1388     0     10       17\n",
       "50052    3      9       16\n",
       "41645    0      5       16\n",
       "42377    1      5       16\n",
       "17244    0      5       12\n",
       "...    ...    ...      ...\n",
       "44081    3      5       14\n",
       "23713    3      9       16\n",
       "31375    2     10       17\n",
       "21772    0      9       14\n",
       "4998     2      7       12\n",
       "\n",
       "[10788 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.randn(10, 6)\n",
    "repeated_tensor = input_tensor.unsqueeze(1).repeat(1, 4, 1)\n",
    "\n",
    "print(repeated_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_num_tensor[0:10, :].unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NumericEmbedding(nn.Module):\n",
    "    def __init__(self, d_model=64):\n",
    "        super(NumericEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(1, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 64),\n",
    "        )\n",
    "\n",
    "    def forward(self, numeric_tensor, first_four=False):\n",
    "        if first_four:\n",
    "            if numeric_tensor.ndim == 2:\n",
    "                numeric_tensor = numeric_tensor.unsqueeze(2).repeat(1, 1, 4)\n",
    "            elif numeric_tensor.ndim == 1:\n",
    "                numeric_tensor = numeric_tensor.repeat(1, 1, 4)\n",
    "            else:\n",
    "                raise ValueError(\"numeric_tensor must be 1D or 2D\")\n",
    "\n",
    "            zero_embd = torch.zeros(\n",
    "                numeric_tensor.size(0),\n",
    "                numeric_tensor.size(1),\n",
    "                self.d_model - numeric_tensor.size(2),\n",
    "            )\n",
    "            out = torch.cat([numeric_tensor, zero_embd], dim=2)\n",
    "            return out\n",
    "        else:\n",
    "            numeric_tensor = numeric_tensor.unsqueeze(2)\n",
    "            return self.linear(numeric_tensor)\n",
    "\n",
    "\n",
    "numeric_embedding = NumericEmbedding()\n",
    "test_embd = numeric_embedding(X_train_num_tensor[0:10, :])\n",
    "test_embd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1604, 0.3445, 0.6978, 0.1998, 0.9735],\n",
       "         [0.0732, 0.3774, 0.1966, 0.3286, 0.8781],\n",
       "         [0.0419, 0.2574, 0.0638, 0.5357, 0.6163],\n",
       "         [0.1152, 0.6667, 0.7150, 0.9101, 0.0605]],\n",
       "\n",
       "        [[0.1252, 0.6849, 0.1274, 0.4090, 0.4373],\n",
       "         [0.0774, 0.4272, 0.1606, 0.8175, 0.1027],\n",
       "         [0.0342, 0.7487, 0.0200, 0.7496, 0.6971],\n",
       "         [0.7681, 0.6832, 0.4315, 0.6218, 0.6027]],\n",
       "\n",
       "        [[0.7934, 0.8341, 0.0767, 0.0806, 0.9866],\n",
       "         [0.4993, 0.8700, 0.8879, 0.0983, 0.0296],\n",
       "         [0.6950, 0.9392, 0.5446, 0.4494, 0.7049],\n",
       "         [0.3332, 0.5412, 0.8769, 0.1278, 0.1010]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mat = torch.rand(3, 4, 5)\n",
    "test_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1604, 0.3445, 0.6978, 0.1998, 0.9735]),\n",
       " tensor([1.6040, 3.4446, 6.9776, 1.9976, 9.7349]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mat[0, 0], test_mat[0, 0] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 6]), torch.Size([10, 6, 1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train_num_tensor[0:10, :].shape, X_train_num_tensor[0:10, :].unsqueeze(2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalars = torch.tensor([10, 2, 3, 4])\n",
    "scalars = scalars.unsqueeze(1).unsqueeze(0)\n",
    "scalars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.6040, 3.4446, 6.9776, 1.9976, 9.7349],\n",
       "         [0.1464, 0.7547, 0.3932, 0.6572, 1.7562],\n",
       "         [0.1258, 0.7723, 0.1913, 1.6071, 1.8490],\n",
       "         [0.4609, 2.6668, 2.8600, 3.6402, 0.2420]],\n",
       "\n",
       "        [[1.2524, 6.8493, 1.2741, 4.0903, 4.3732],\n",
       "         [0.1548, 0.8543, 0.3212, 1.6350, 0.2055],\n",
       "         [0.1025, 2.2462, 0.0600, 2.2489, 2.0914],\n",
       "         [3.0726, 2.7328, 1.7261, 2.4870, 2.4109]],\n",
       "\n",
       "        [[7.9344, 8.3407, 0.7668, 0.8060, 9.8656],\n",
       "         [0.9986, 1.7401, 1.7758, 0.1966, 0.0592],\n",
       "         [2.0851, 2.8177, 1.6337, 1.3481, 2.1147],\n",
       "         [1.3330, 2.1647, 3.5076, 0.5110, 0.4040]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_test_mat = scalars * test_mat\n",
    "double_test_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6040, 3.4446, 6.9776, 1.9976, 9.7349])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_test_mat[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([32, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        q = (\n",
    "            self.q_linear(q)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_head)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        k = (\n",
    "            self.k_linear(k)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_head)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        v = (\n",
    "            self.v_linear(v)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_head)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        attn_output, _ = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        attn_output = (\n",
    "            attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        )\n",
    "        out = self.out_linear(attn_output)\n",
    "        return out\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "        d_k = q.size(-1)\n",
    "        scaled_attention_logits = matmul_qk / (d_k**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += mask * -1e9\n",
    "\n",
    "        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn_output = self.multi_head_attention(q, k, v, mask)\n",
    "        out1 = self.layernorm1(q + attn_output)\n",
    "\n",
    "        ff_output = self.feed_forward(out1)\n",
    "        out2 = self.layernorm2(out1 + ff_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "# Parameters\n",
    "d_model = 64  # Embedding dimension\n",
    "num_heads = 4  # Number of attention heads\n",
    "seq_len_q = 10  # Sequence length for the query tensor\n",
    "seq_len_k = 20  # Sequence length for the key tensor\n",
    "batch_size = 32  # Batch size\n",
    "\n",
    "# Random data\n",
    "q = torch.rand((batch_size, seq_len_q, d_model))\n",
    "k = torch.rand((batch_size, seq_len_k, d_model))\n",
    "v = k  # Usually, value and key are the same in many applications\n",
    "\n",
    "# Model\n",
    "encoder_layer = TransformerEncoderLayer(d_model, num_heads)\n",
    "\n",
    "# Forward pass\n",
    "output = encoder_layer(q, k, v)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat      cut color clarity  depth  table     x     y     z\n",
       "0   0.23    Ideal     E     SI2   61.5   55.0  3.95  3.98  2.43\n",
       "1   0.21  Premium     E     SI1   59.8   61.0  3.89  3.84  2.31\n",
       "2   0.23     Good     E     VS1   56.9   65.0  4.05  4.07  2.31\n",
       "3   0.29  Premium     I     VS2   62.4   58.0  4.20  4.23  2.63\n",
       "4   0.31     Good     J     SI2   63.3   58.0  4.34  4.35  2.75"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head().drop(\"price\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1, 2, 3] + [4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TabTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokens,\n",
    "        numeric_col_tokens,\n",
    "        cat_col_tokens,\n",
    "        d_model=64,\n",
    "    ):\n",
    "        super(TabTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.tokens = tokens\n",
    "        self.col_tokens = cat_col_tokens + numeric_col_tokens\n",
    "        self.n_tokens = len(tokens)  # TODO Make this\n",
    "        # Embedding layers for categorical features\n",
    "        self.embeddings = nn.Embedding(self.n_tokens, self.d_model)\n",
    "        self.n_num_embeddings = 6\n",
    "        # self.numeric_embeddings = NumericEmbedding(d_model=self.d_model)\n",
    "        self.col_indices = torch.tensor(\n",
    "            [self.tokens.index(col) for col in self.col_tokens], dtype=torch.long\n",
    "        )\n",
    "        self.numeric_indices = torch.tensor(\n",
    "            [self.tokens.index(col) for col in numeric_col_tokens], dtype=torch.long\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoderLayer(d_model, num_heads=4)\n",
    "\n",
    "        self.numeric_predictor = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 2, 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Linear(len(self.col_tokens), 1)\n",
    "\n",
    "    def forward(self, num_inputs, cat_inputs):\n",
    "        # Embed column indices\n",
    "        repeated_col_indices = self.col_indices.unsqueeze(0).repeat(\n",
    "            num_inputs.size(0), 1\n",
    "        )\n",
    "        col_embeddings = self.embeddings(repeated_col_indices)\n",
    "\n",
    "        repeated_numeric_indices = self.numeric_indices.unsqueeze(0).repeat(\n",
    "            num_inputs.size(0), 1\n",
    "        )\n",
    "        numeric_col_embeddings = self.embeddings(repeated_numeric_indices)\n",
    "\n",
    "        cat_embeddings = self.embeddings(cat_inputs)\n",
    "        # num_embeddings = self.numeric_embeddings(num_inputs)\n",
    "        expanded_num_inputs = num_inputs.unsqueeze(2).repeat(1, 1, 64)\n",
    "        # print(\n",
    "        #     f\"NumCol: {numeric_col_embeddings.shape}\",\n",
    "        #     f\"NumericInputs: {expanded_num_inputs.shape}\",\n",
    "        #     sep=\"\\n\",\n",
    "        # )\n",
    "        num_embeddings = numeric_col_embeddings * expanded_num_inputs\n",
    "        # Put a multiplier of the numeric embeddings and the\n",
    "        # numeric columns to embed the numeric weights instead of the lame ass\n",
    "        # embedder you currently have.\n",
    "        # return col_embeddings, cat_embeddings, num_embeddings\n",
    "        query_embeddings = torch.cat([cat_embeddings, num_embeddings], dim=1)\n",
    "        out = self.transformer_encoder(\n",
    "            col_embeddings,\n",
    "            query_embeddings,\n",
    "            query_embeddings\n",
    "            # col_embeddings, query_embeddings, query_embeddings\n",
    "        )\n",
    "        out = self.numeric_predictor(out)\n",
    "        out = self.flatten_layer(out.squeeze(-1))\n",
    "        return out\n",
    "\n",
    "\n",
    "no_price_tokens = tokens.copy()\n",
    "no_price_tokens.remove(\"price\")\n",
    "\n",
    "numeric_col_tokens = (\n",
    "    df.head().drop(\"price\", axis=1).select_dtypes(include=np.number).columns.to_list()\n",
    ")\n",
    "cat_col_tokens = df.head().select_dtypes(exclude=np.number).columns.to_list()\n",
    "\n",
    "model = TabTransformer(\n",
    "    no_price_tokens,\n",
    "    numeric_col_tokens=numeric_col_tokens,\n",
    "    cat_col_tokens=cat_col_tokens,\n",
    ")\n",
    "x = model(X_train_num_tensor[0:32, :], X_train_cat_tensor[0:32, :])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 Loss: 26,001,790.00\n",
      "Epoch 5/100 Loss: 21,874,858.00\n",
      "Epoch 7/100 Loss: 7,554,384.50\n",
      "Epoch 10/100 Loss: 1,631,678.00\n",
      "Epoch 12/100 Loss: 742,269.44\n",
      "Epoch 14/100 Loss: 485,216.91\n",
      "Epoch 16/100 Loss: 288,242.59\n",
      "Epoch 19/100 Loss: 412,795.25\n",
      "Epoch 21/100 Loss: 324,930.38\n",
      "Epoch 23/100 Loss: 284,899.25\n",
      "Epoch 25/100 Loss: 402,650.69\n",
      "Epoch 28/100 Loss: 402,396.22\n",
      "Epoch 30/100 Loss: 281,483.47\n",
      "Epoch 32/100 Loss: 370,100.12\n",
      "Epoch 35/100 Loss: 271,378.22\n",
      "Epoch 37/100 Loss: 295,787.22\n",
      "Epoch 39/100 Loss: 347,235.66\n",
      "Epoch 41/100 Loss: 288,962.88\n",
      "Epoch 44/100 Loss: 332,677.62\n",
      "Epoch 46/100 Loss: 305,817.09\n",
      "Epoch 48/100 Loss: 251,425.47\n",
      "Epoch 50/100 Loss: 325,339.59\n",
      "Epoch 53/100 Loss: 367,920.28\n",
      "Epoch 55/100 Loss: 269,086.44\n",
      "Epoch 57/100 Loss: 340,613.38\n",
      "Epoch 60/100 Loss: 259,414.55\n",
      "Epoch 62/100 Loss: 282,344.12\n",
      "Epoch 64/100 Loss: 332,069.25\n",
      "Epoch 66/100 Loss: 290,322.22\n",
      "Epoch 69/100 Loss: 318,891.00\n",
      "Epoch 71/100 Loss: 289,071.50\n",
      "Epoch 73/100 Loss: 245,454.20\n",
      "Epoch 75/100 Loss: 272,653.91\n",
      "Epoch 78/100 Loss: 330,911.62\n",
      "Epoch 80/100 Loss: 256,954.80\n",
      "Epoch 82/100 Loss: 325,428.41\n",
      "Epoch 85/100 Loss: 249,061.75\n",
      "Epoch 87/100 Loss: 281,022.84\n",
      "Epoch 89/100 Loss: 320,764.88\n",
      "Epoch 91/100 Loss: 278,280.97\n",
      "Epoch 94/100 Loss: 327,983.91\n",
      "Epoch 96/100 Loss: 287,022.94\n",
      "Epoch 98/100 Loss: 237,614.53\n",
      "Epoch 100/100 Loss: 233,334.53\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "batch_size = 1000\n",
    "lr = 0.001\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "batch_count = 0\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, X_train_num_tensor.size(0), batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(\n",
    "            X_train_num_tensor[i : i + batch_size, :],\n",
    "            X_train_cat_tensor[i : i + batch_size, :],\n",
    "        )\n",
    "        loss = loss_fn(y_pred, y_train_tensor[i : i + batch_size, :])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_count += 1\n",
    "        if batch_count % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} Loss: {loss.item():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 985,492.12\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_num_tensor[0:10, :], X_test_cat_tensor[0:10, :])\n",
    "    loss = loss_fn(y_pred, y_test_tensor[0:10])\n",
    "    print(f\"Test loss: {loss.item():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 601.22 Actual: 559.00 Diff: 42.22\n",
      "Predicted: 2,328.35 Actual: 2,201.00 Diff: 127.35\n",
      "Predicted: 1,207.12 Actual: 1,238.00 Diff: -30.88\n",
      "Predicted: 1,467.03 Actual: 1,304.00 Diff: 163.03\n",
      "Predicted: 9,597.38 Actual: 6,901.00 Diff: 2,696.38\n",
      "Predicted: 3,875.39 Actual: 3,011.00 Diff: 864.39\n",
      "Predicted: 1,825.58 Actual: 1,765.00 Diff: 60.58\n",
      "Predicted: 1,854.22 Actual: 1,679.00 Diff: 175.22\n",
      "Predicted: 2,105.56 Actual: 2,102.00 Diff: 3.56\n",
      "Predicted: 6,114.66 Actual: 4,789.00 Diff: 1,325.66\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\n",
    "        f\"Predicted: {y_pred[i].item():,.2f} Actual: {y_test_tensor[i].item():,.2f}\",\n",
    "        f\"Diff: {y_pred[i].item() - y_test_tensor[i].item():,.2f}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 754.16 Actual: 559.00\n",
      "Predicted: 2,390.27 Actual: 2,201.00\n",
      "Predicted: 993.24 Actual: 1,238.00\n",
      "Predicted: 1,583.13 Actual: 1,304.00\n",
      "Predicted: 9,786.17 Actual: 6,901.00\n",
      "Predicted: 3,888.29 Actual: 3,011.00\n",
      "Predicted: 1,574.12 Actual: 1,765.00\n",
      "Predicted: 1,755.46 Actual: 1,679.00\n",
      "Predicted: 2,334.63 Actual: 2,102.00\n",
      "Predicted: 5,939.16 Actual: 4,789.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Predicted: {y_pred[i].item():,.2f} Actual: {y_test_tensor[i].item():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 688.47 Actual: 559.00\n",
      "Predicted: 2,547.17 Actual: 2,201.00\n",
      "Predicted: 1,044.95 Actual: 1,238.00\n",
      "Predicted: 1,790.95 Actual: 1,304.00\n",
      "Predicted: 10,160.45 Actual: 6,901.00\n",
      "Predicted: 3,790.90 Actual: 3,011.00\n",
      "Predicted: 1,729.77 Actual: 1,765.00\n",
      "Predicted: 1,749.63 Actual: 1,679.00\n",
      "Predicted: 2,328.92 Actual: 2,102.00\n",
      "Predicted: 5,928.75 Actual: 4,789.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Predicted: {y_pred[i].item():,.2f} Actual: {y_test_tensor[i].item():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted: 688.47 Actual: 559.00\n",
    "# Predicted: 2,547.17 Actual: 2,201.00\n",
    "# Predicted: 1,044.95 Actual: 1,238.00\n",
    "# Predicted: 1,790.95 Actual: 1,304.00\n",
    "# Predicted: 10,160.45 Actual: 6,901.00\n",
    "# Predicted: 3,790.90 Actual: 3,011.00\n",
    "# Predicted: 1,729.77 Actual: 1,765.00\n",
    "# Predicted: 1,749.63 Actual: 1,679.00\n",
    "# Predicted: 2,328.92 Actual: 2,102.00\n",
    "# Predicted: 5,928.75 Actual: 4,789.00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
