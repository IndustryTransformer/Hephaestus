{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat      cut color clarity  depth  table  price     x     y     z\n",
       "0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Load and preprocess the dataset (assuming you have a CSV file)\n",
    "df = pd.read_csv(\"../data/diamonds.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'price', 'x', 'y',\n",
       "       'z'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = [\"cut\", \"color\", \"clarity\"]\n",
    "num_columns = [\"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\"]\n",
    "cat_values = pd.unique(df[cat_columns].values.ravel(\"K\"))\n",
    "target_column = \"price\"\n",
    "tokens = list(\n",
    "    chain(\n",
    "        cat_values,\n",
    "        cat_columns,\n",
    "        num_columns,\n",
    "        [\"PAD\", \"[NUMERIC_MASK]\", \"[MASK]\"],\n",
    "        [target_column],\n",
    "    )\n",
    ")\n",
    "token_dict = {token: i for i, token in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(len(token_dict), 64)\n",
    "cat_values_emb = torch.tensor(\n",
    "    [[token_dict[token] for token in row] for row in df[cat_columns].values],\n",
    "    dtype=torch.long,\n",
    ")\n",
    "\n",
    "col_names_emb = torch.tensor([token_dict[col] for col in df.columns], dtype=torch.long)\n",
    "embedding(col_names_emb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"price\", axis=1)\n",
    "y = df[\"price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Preprocess categorical features\n",
    "X_train_cat = X_train[cat_columns].copy()\n",
    "X_test_cat = X_test[cat_columns].copy()\n",
    "\n",
    "label_encoders = {}\n",
    "for col in cat_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_train_cat[col] = X_train_cat[col].map(token_dict)\n",
    "    X_test_cat[col] = X_test_cat[col].map(token_dict)\n",
    "    # label_encoders[col] = le\n",
    "\n",
    "# Preprocess numeric features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_num = scaler.fit_transform(X_train[num_columns].copy())\n",
    "X_test_num = scaler.transform(X_test[num_columns].copy())\n",
    "\n",
    "X_train_cat_tensor = torch.tensor(\n",
    "    X_train_cat.values, dtype=torch.int64\n",
    ")  # Use int64 dtype for categorical indices\n",
    "X_train_num_tensor = torch.tensor(X_train_num, dtype=torch.float32)\n",
    "X_test_cat_tensor = torch.tensor(\n",
    "    X_test_cat.values, dtype=torch.int64\n",
    ")  # Use int64 dtype for categorical indices\n",
    "X_test_num_tensor = torch.tensor(X_test_num, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1388</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50052</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41645</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42377</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17244</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44081</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23713</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31375</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21772</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10788 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cut  color  clarity\n",
       "1388     0     10       17\n",
       "50052    3      9       16\n",
       "41645    0      5       16\n",
       "42377    1      5       16\n",
       "17244    0      5       12\n",
       "...    ...    ...      ...\n",
       "44081    3      5       14\n",
       "23713    3      9       16\n",
       "31375    2     10       17\n",
       "21772    0      9       14\n",
       "4998     2      7       12\n",
       "\n",
       "[10788 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 4, 6])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.randn(10, 6)\n",
    "repeated_tensor = input_tensor.unsqueeze(1).repeat(1, 4, 1)\n",
    "\n",
    "print(repeated_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_num_tensor[0:10, :].unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0636, 0.8199, 0.6281, 0.3663, 0.6602],\n",
       "         [0.1109, 0.7213, 0.5941, 0.9433, 0.3055],\n",
       "         [0.1782, 0.3100, 0.0353, 0.2705, 0.1160],\n",
       "         [0.4531, 0.0281, 0.4697, 0.0220, 0.4760]],\n",
       "\n",
       "        [[0.3182, 0.3856, 0.6484, 0.1468, 0.0170],\n",
       "         [0.9080, 0.3354, 0.5932, 0.9195, 0.1159],\n",
       "         [0.1039, 0.7766, 0.1595, 0.1899, 0.0124],\n",
       "         [0.8718, 0.6338, 0.5656, 0.9191, 0.6315]],\n",
       "\n",
       "        [[0.7445, 0.0606, 0.9957, 0.6818, 0.9350],\n",
       "         [0.3552, 0.2315, 0.9873, 0.0667, 0.6262],\n",
       "         [0.7692, 0.8921, 0.2996, 0.6761, 0.6816],\n",
       "         [0.4189, 0.6757, 0.7876, 0.0845, 0.2168]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mat = torch.rand(3, 4, 5)\n",
    "test_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0636, 0.8199, 0.6281, 0.3663, 0.6602]),\n",
       " tensor([0.6361, 8.1994, 6.2809, 3.6626, 6.6018]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mat[0, 0], test_mat[0, 0] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 6]), torch.Size([10, 6, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train_num_tensor[0:10, :].shape, X_train_num_tensor[0:10, :].unsqueeze(2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalars = torch.tensor([10, 2, 3, 4])\n",
    "scalars = scalars.unsqueeze(1).unsqueeze(0)\n",
    "scalars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6361, 8.1994, 6.2809, 3.6626, 6.6018],\n",
       "         [0.2218, 1.4426, 1.1882, 1.8866, 0.6110],\n",
       "         [0.5345, 0.9301, 0.1058, 0.8116, 0.3479],\n",
       "         [1.8125, 0.1124, 1.8787, 0.0880, 1.9041]],\n",
       "\n",
       "        [[3.1816, 3.8555, 6.4840, 1.4676, 0.1699],\n",
       "         [1.8161, 0.6708, 1.1864, 1.8389, 0.2318],\n",
       "         [0.3117, 2.3298, 0.4784, 0.5696, 0.0372],\n",
       "         [3.4872, 2.5353, 2.2625, 3.6764, 2.5261]],\n",
       "\n",
       "        [[7.4446, 0.6058, 9.9571, 6.8184, 9.3498],\n",
       "         [0.7105, 0.4629, 1.9747, 0.1333, 1.2523],\n",
       "         [2.3077, 2.6762, 0.8987, 2.0282, 2.0449],\n",
       "         [1.6757, 2.7030, 3.1503, 0.3378, 0.8671]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_test_mat = scalars * test_mat\n",
    "double_test_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6361, 8.1994, 6.2809, 3.6626, 6.6018])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_test_mat[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([32, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_model // num_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        q = (\n",
    "            self.q_linear(q)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_head)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        k = (\n",
    "            self.k_linear(k)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_head)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        v = (\n",
    "            self.v_linear(v)\n",
    "            .view(batch_size, -1, self.num_heads, self.d_head)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        attn_output, _ = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        attn_output = (\n",
    "            attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        )\n",
    "        out = self.out_linear(attn_output)\n",
    "        return out\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "        d_k = q.size(-1)\n",
    "        scaled_attention_logits = matmul_qk / (d_k**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += mask * -1e9\n",
    "\n",
    "        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn_output = self.multi_head_attention(q, k, v, mask)\n",
    "        out1 = self.layernorm1(q + attn_output)\n",
    "\n",
    "        ff_output = self.feed_forward(out1)\n",
    "        out2 = self.layernorm2(out1 + ff_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "# Parameters\n",
    "d_model = 64  # Embedding dimension\n",
    "num_heads = 4  # Number of attention heads\n",
    "seq_len_q = 10  # Sequence length for the query tensor\n",
    "seq_len_k = 20  # Sequence length for the key tensor\n",
    "batch_size = 32  # Batch size\n",
    "\n",
    "# Random data\n",
    "q = torch.rand((batch_size, seq_len_q, d_model))\n",
    "k = torch.rand((batch_size, seq_len_k, d_model))\n",
    "v = k  # Usually, value and key are the same in many applications\n",
    "\n",
    "# Model\n",
    "encoder_layer = TransformerEncoderLayer(d_model, num_heads)\n",
    "\n",
    "# Forward pass\n",
    "output = encoder_layer(q, k, v)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out shape: torch.Size([3, 9, 64]), cat_out shape: torch.Size([3, 9, 32])\n",
      "numeric_out shape: torch.Size([3, 576])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 9, 32]), torch.Size([3, 6]))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TabTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokens,\n",
    "        numeric_col_tokens,\n",
    "        cat_col_tokens,\n",
    "        token_dict,\n",
    "        d_model=64,\n",
    "    ):\n",
    "        super(TabTransformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.tokens = tokens\n",
    "        self.token_dict = token_dict\n",
    "        # Masks\n",
    "        self.cat_mask_token = torch.tensor(self.token_dict[\"[MASK]\"])\n",
    "        self.numeric_mask_token = torch.tensor(self.token_dict[\"[NUMERIC_MASK]\"])\n",
    "\n",
    "        self.col_tokens = cat_col_tokens + numeric_col_tokens\n",
    "        self.n_tokens = len(tokens)  # TODO Make this\n",
    "        # Embedding layers for categorical features\n",
    "        self.embeddings = nn.Embedding(self.n_tokens, self.d_model)\n",
    "        self.n_numeric_cols = len(numeric_col_tokens)\n",
    "        self.n_cat_cols = len(cat_col_tokens)\n",
    "        self.n_columns = self.n_numeric_cols + self.n_cat_cols\n",
    "        # self.numeric_embeddings = NumericEmbedding(d_model=self.d_model)\n",
    "        self.col_indices = torch.tensor(\n",
    "            [self.tokens.index(col) for col in self.col_tokens], dtype=torch.long\n",
    "        )\n",
    "        self.numeric_indices = torch.tensor(\n",
    "            [self.tokens.index(col) for col in numeric_col_tokens], dtype=torch.long\n",
    "        )\n",
    "        self.transformer_encoder = TransformerEncoderLayer(d_model, num_heads=4)\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 2, 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.mlm_decoder = nn.Sequential(\n",
    "            nn.Linear(d_model, self.n_tokens)\n",
    "        )  # TODO try making more complex\n",
    "\n",
    "        self.mnm_decoder = nn.Sequential(\n",
    "            nn.Linear(self.n_columns * self.d_model, 128),  # Try making more complex\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 6),\n",
    "        )\n",
    "\n",
    "        self.flatten_layer = nn.Linear(len(self.col_tokens), 1)\n",
    "\n",
    "    def forward(self, num_inputs, cat_inputs, mnm=None, mlm=None, task=\"regression\"):\n",
    "        # Embed column indices\n",
    "        repeated_col_indices = self.col_indices.unsqueeze(0).repeat(\n",
    "            num_inputs.size(0), 1\n",
    "        )\n",
    "        col_embeddings = self.embeddings(repeated_col_indices)\n",
    "\n",
    "        repeated_numeric_indices = self.numeric_indices.unsqueeze(0).repeat(\n",
    "            num_inputs.size(0), 1\n",
    "        )\n",
    "        numeric_col_embeddings = self.embeddings(repeated_numeric_indices)\n",
    "\n",
    "        if mlm is not None:\n",
    "            # print(f\"cat_inputs.shape: {cat_inputs.shape}, mlm.shape: {mlm.shape}\")\n",
    "            cat_inputs.masked_fill_(mlm, self.cat_mask_token)  # `_` is inplace.\n",
    "\n",
    "        cat_embeddings = self.embeddings(cat_inputs)\n",
    "        expanded_num_inputs = num_inputs.unsqueeze(2).repeat(1, 1, 64)\n",
    "        num_embeddings = numeric_col_embeddings * expanded_num_inputs\n",
    "\n",
    "        if mnm is not None:\n",
    "            # print(\n",
    "            #     f\"mnm.shape: {mnm.shape}, num_embeddings.shape: {num_embeddings.shape}\"\n",
    "            # )\n",
    "            # print(num_embeddings.sum())\n",
    "            numeric_mask_embedding = self.embeddings(self.numeric_mask_token)\n",
    "            # numeric_mask_embedding = torch.ones_like(numeric_mask_embedding)\n",
    "            # numeric_mask_embedding = torch.zeros_like(numeric_mask_embedding)\n",
    "            num_embeddings[mnm] = numeric_mask_embedding\n",
    "            # print(num_embeddings.sum())\n",
    "\n",
    "        query_embeddings = torch.cat([cat_embeddings, num_embeddings], dim=1)\n",
    "        out = self.transformer_encoder(\n",
    "            col_embeddings,\n",
    "            query_embeddings,\n",
    "            query_embeddings\n",
    "            # col_embeddings, query_embeddings, query_embeddings\n",
    "        )\n",
    "        if task == \"regression\":\n",
    "            out = self.regressor(out)\n",
    "            out = self.flatten_layer(out.squeeze(-1))\n",
    "\n",
    "            return out\n",
    "        elif task == \"mlm\":\n",
    "            cat_out = self.mlm_decoder(out)\n",
    "            # print(f\"Out shape: {out.shape}, cat_out shape: {cat_out.shape}\")\n",
    "            numeric_out = out.view(out.size(0), -1)\n",
    "            # print(f\"numeric_out shape: {numeric_out.shape}\")\n",
    "            numeric_out = self.mnm_decoder(numeric_out)\n",
    "            return cat_out, numeric_out\n",
    "        else:\n",
    "            raise ValueError(f\"Task {task} not supported.\")\n",
    "\n",
    "\n",
    "no_price_tokens = tokens.copy()\n",
    "no_price_tokens.remove(\"price\")\n",
    "\n",
    "numeric_col_tokens = (\n",
    "    df.head().drop(\"price\", axis=1).select_dtypes(include=np.number).columns.to_list()\n",
    ")\n",
    "cat_col_tokens = df.head().select_dtypes(exclude=np.number).columns.to_list()\n",
    "\n",
    "model = TabTransformer(\n",
    "    no_price_tokens,\n",
    "    numeric_col_tokens=numeric_col_tokens,\n",
    "    cat_col_tokens=cat_col_tokens,\n",
    "    token_dict=token_dict,\n",
    ")\n",
    "batch_size = 3\n",
    "mnm = torch.rand(batch_size, X_train_num_tensor.size(1)) > 0.8\n",
    "mlm = torch.rand(batch_size, X_train_cat_tensor.size(1)) > 0.8\n",
    "with torch.no_grad():\n",
    "    x = model(\n",
    "        X_train_num_tensor[0:batch_size, :],\n",
    "        X_train_cat_tensor[0:batch_size, :],\n",
    "        mnm,\n",
    "        mlm,\n",
    "        task=\"mlm\",\n",
    "    )\n",
    "x[0].shape, x[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 31])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.rand(3, 31, 64)\n",
    "A.reshape(3, 64, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Tabualr Modeling\n",
    "epochs = 20\n",
    "batch_size = 1000\n",
    "lr = 0.001\n",
    "mse_loss = nn.MSELoss()\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model_time = dt.now()\n",
    "model_time = model_time.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "model_name = f\"Regression.8_{model_time}\"\n",
    "\n",
    "summary_writer = SummaryWriter(\"runs/\" + model_name)\n",
    "\n",
    "numeric_column_t\n",
    "batch_count = 0\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, X_train_num_tensor.size(0), batch_size):\n",
    "        num_inputs = X_train_num_tensor[i : i + batch_size, :]\n",
    "        cat_inputs = X_train_cat_tensor[i : i + batch_size, :]\n",
    "        mlm = torch.rand(cat_inputs.shape) > 0.8  # Greater than 1 for testing\n",
    "        mnm = torch.rand(num_inputs.shape) > 0.8\n",
    "        # mlm = None\n",
    "        # mnm = None\n",
    "        optimizer.zero_grad()\n",
    "        cat_preds, numeric_preds = model(num_inputs, cat_inputs, mnm=mnm, mlm=mlm)\n",
    "        cat_targets = torch.cat(cat_inputs, model.numeric_indices.s)\n",
    "        cat_loss = ce_loss(cat_preds, cat_inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_count += 1\n",
    "        learning_rate = optimizer.param_groups[0][\"lr\"]\n",
    "        summary_writer.add_scalar(\"Loss/train\", loss.item(), batch_count)\n",
    "        summary_writer.add_scalar(\"Metrics/LearningRate\", learning_rate, batch_count)\n",
    "        if batch_count % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} Loss: {loss.item():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.numeric_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  9, 15,  ..., 26, 27, 28],\n",
       "        [ 0, 11, 15,  ..., 26, 27, 28],\n",
       "        [ 3,  8, 15,  ..., 26, 27, 28],\n",
       "        ...,\n",
       "        [ 3,  9, 19,  ..., 26, 27, 28],\n",
       "        [ 1,  7, 13,  ..., 26, 27, 28],\n",
       "        [ 1,  9, 13,  ..., 26, 27, 28]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((cat_inputs, model.numeric_indices.expand(cat_inputs.size(0), -1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([152, 6])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.numeric_indices.expand(cat_inputs.size(0), -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([152, 3])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Loss: 25,652,796.00\n",
      "Epoch 5/20 Loss: 19,682,204.00\n",
      "Epoch 7/20 Loss: 5,207,685.00\n",
      "Epoch 10/20 Loss: 1,074,898.25\n",
      "Epoch 12/20 Loss: 809,297.25\n",
      "Epoch 14/20 Loss: 589,495.50\n",
      "Epoch 16/20 Loss: 436,240.62\n",
      "Epoch 19/20 Loss: 441,002.25\n"
     ]
    }
   ],
   "source": [
    "# Regression Model\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 1000\n",
    "lr = 0.001\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model_time = dt.now()\n",
    "model_time = model_time.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "model_name = f\"just_mnm.8_{model_time}\"\n",
    "\n",
    "summary_writer = SummaryWriter(\"runs/\" + model_name)\n",
    "\n",
    "\n",
    "batch_count = 0\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, X_train_num_tensor.size(0), batch_size):\n",
    "        num_inputs = X_train_num_tensor[i : i + batch_size, :]\n",
    "        cat_inputs = X_train_cat_tensor[i : i + batch_size, :]\n",
    "        # mlm = torch.rand(cat_inputs.shape) > 0.8  # Greater than 1 for testing\n",
    "        mnm = torch.rand(num_inputs.shape) > 0.8\n",
    "        mlm = None\n",
    "        # mnm = None\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(num_inputs, cat_inputs, mnm=mnm, mlm=mlm)\n",
    "        loss = loss_fn(y_pred, y_train_tensor[i : i + batch_size, :])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_count += 1\n",
    "        learning_rate = optimizer.param_groups[0][\"lr\"]\n",
    "        summary_writer.add_scalar(\"Loss/train\", loss.item(), batch_count)\n",
    "        summary_writer.add_scalar(\"Metrics/LearningRate\", learning_rate, batch_count)\n",
    "        if batch_count % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} Loss: {loss.item():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_shape = X_train_num_tensor[i : i + batch_size, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.rand(test_shape) > 0.8).requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 40\n",
    "# batch_size = 1000\n",
    "# lr = 0.5\n",
    "# loss_fn = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# batch_count = 0\n",
    "# model.train()\n",
    "# for epoch in range(epochs):\n",
    "#     for i in range(0, X_train_num_tensor.size(0), batch_size):\n",
    "#         optimizer.zero_grad()\n",
    "#         y_pred = model(\n",
    "#             X_train_num_tensor[i : i + batch_size, :],\n",
    "#             X_train_cat_tensor[i : i + batch_size, :],\n",
    "#         )\n",
    "#         loss = loss_fn(y_pred, y_train_tensor[i : i + batch_size, :])\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         batch_count += 1\n",
    "#         if batch_count % 100 == 0:\n",
    "#             print(f\"Epoch {epoch+1}/{epochs} Loss: {loss.item():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.zeros(4, 4, 4)\n",
    "B = torch.ones(4, 4)\n",
    "\n",
    "# Insert B into the slice where the first dimension is 2\n",
    "A[2, :, :] = B\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1,131,268.25\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_num_tensor[0:10, :], X_test_cat_tensor[0:10, :])\n",
    "    loss = loss_fn(y_pred, y_test_tensor[0:10])\n",
    "    print(f\"Test loss: {loss.item():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 659.33 Actual: 559.00 Diff: 100.33\n",
      "Predicted: 2,475.55 Actual: 2,201.00 Diff: 274.55\n",
      "Predicted: 1,317.27 Actual: 1,238.00 Diff: 79.27\n",
      "Predicted: 1,623.84 Actual: 1,304.00 Diff: 319.84\n",
      "Predicted: 9,862.08 Actual: 6,901.00 Diff: 2,961.08\n",
      "Predicted: 4,085.95 Actual: 3,011.00 Diff: 1,074.95\n",
      "Predicted: 1,895.64 Actual: 1,765.00 Diff: 130.64\n",
      "Predicted: 1,961.49 Actual: 1,679.00 Diff: 282.49\n",
      "Predicted: 2,296.45 Actual: 2,102.00 Diff: 194.45\n",
      "Predicted: 5,818.79 Actual: 4,789.00 Diff: 1,029.79\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\n",
    "        f\"Predicted: {y_pred[i].item():,.2f} Actual: {y_test_tensor[i].item():,.2f}\",\n",
    "        f\"Diff: {y_pred[i].item() - y_test_tensor[i].item():,.2f}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 659.33 Actual: 559.00\n",
      "Predicted: 2,475.55 Actual: 2,201.00\n",
      "Predicted: 1,317.27 Actual: 1,238.00\n",
      "Predicted: 1,623.84 Actual: 1,304.00\n",
      "Predicted: 9,862.08 Actual: 6,901.00\n",
      "Predicted: 4,085.95 Actual: 3,011.00\n",
      "Predicted: 1,895.64 Actual: 1,765.00\n",
      "Predicted: 1,961.49 Actual: 1,679.00\n",
      "Predicted: 2,296.45 Actual: 2,102.00\n",
      "Predicted: 5,818.79 Actual: 4,789.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Predicted: {y_pred[i].item():,.2f} Actual: {y_test_tensor[i].item():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 659.33 Actual: 559.00\n",
      "Predicted: 2,475.55 Actual: 2,201.00\n",
      "Predicted: 1,317.27 Actual: 1,238.00\n",
      "Predicted: 1,623.84 Actual: 1,304.00\n",
      "Predicted: 9,862.08 Actual: 6,901.00\n",
      "Predicted: 4,085.95 Actual: 3,011.00\n",
      "Predicted: 1,895.64 Actual: 1,765.00\n",
      "Predicted: 1,961.49 Actual: 1,679.00\n",
      "Predicted: 2,296.45 Actual: 2,102.00\n",
      "Predicted: 5,818.79 Actual: 4,789.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Predicted: {y_pred[i].item():,.2f} Actual: {y_test_tensor[i].item():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted: 688.47 Actual: 559.00\n",
    "# Predicted: 2,547.17 Actual: 2,201.00\n",
    "# Predicted: 1,044.95 Actual: 1,238.00\n",
    "# Predicted: 1,790.95 Actual: 1,304.00\n",
    "# Predicted: 10,160.45 Actual: 6,901.00\n",
    "# Predicted: 3,790.90 Actual: 3,011.00\n",
    "# Predicted: 1,729.77 Actual: 1,765.00\n",
    "# Predicted: 1,749.63 Actual: 1,679.00\n",
    "# Predicted: 2,328.92 Actual: 2,102.00\n",
    "# Predicted: 5,928.75 Actual: 4,789.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False, False, False],\n",
       "         [False,  True,  True, False, False],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False, False,  True]],\n",
       "\n",
       "        [[False,  True, False,  True,  True],\n",
       "         [False, False,  True, False, False],\n",
       "         [ True, False,  True, False,  True],\n",
       "         [ True, False, False, False, False]],\n",
       "\n",
       "        [[False,  True, False, False, False],\n",
       "         [False, False, False, False,  True],\n",
       "         [False, False, False,  True, False],\n",
       "         [False, False,  True, False, False]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(3, 4, 5) > 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "rand() received an invalid combination of arguments - got (), but expected one of:\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mrand()\n",
      "\u001b[0;31mTypeError\u001b[0m: rand() received an invalid combination of arguments - got (), but expected one of:\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "torch.rand()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
