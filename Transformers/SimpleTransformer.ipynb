{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir='runs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat      cut color clarity  depth  table  price     x     y     z\n",
       "0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Load and preprocess the dataset (assuming you have a CSV file)\n",
    "df = pd.read_csv(\"../data/diamonds.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'price', 'x', 'y',\n",
       "       'z'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(module):\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "        init.xavier_uniform_(module.weight, gain=5)\n",
    "        if module.bias is not None:\n",
    "            init.constant_(module.bias, 0.001)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        init.uniform_(module.weight, -0.4, 0.4)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        init.normal_(module.weight, mean=1, std=0.2)\n",
    "        init.constant_(module.bias, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_columns = [\"cut\", \"color\", \"clarity\"]\n",
    "num_columns = [\"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\"]\n",
    "cat_values = pd.unique(df[cat_columns].values.ravel(\"K\"))\n",
    "target_column = \"price\"\n",
    "tokens = list(\n",
    "    chain(\n",
    "        cat_values,\n",
    "        cat_columns,\n",
    "        num_columns,\n",
    "        [\"PAD\", \"[NUMERIC_MASK]\", \"[MASK]\"],\n",
    "        [target_column],\n",
    "    )\n",
    ")\n",
    "token_dict = {token: i for i, token in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop(\"price\", axis=1)\n",
    "y = df[\"price\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Preprocess categorical features\n",
    "X_train_cat = X_train[cat_columns].copy()\n",
    "X_test_cat = X_test[cat_columns].copy()\n",
    "\n",
    "label_encoders = {}\n",
    "for col in cat_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_train_cat[col] = X_train_cat[col].map(token_dict)\n",
    "    X_test_cat[col] = X_test_cat[col].map(token_dict)\n",
    "    # label_encoders[col] = le\n",
    "\n",
    "# Preprocess numeric features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_num = scaler.fit_transform(X_train[num_columns].copy())\n",
    "X_test_num = scaler.transform(X_test[num_columns].copy())\n",
    "\n",
    "X_train_cat_tensor = torch.tensor(X_train_cat.values, dtype=torch.int64).to(\n",
    "    device\n",
    ")  # Use int64 dtype for categorical indices\n",
    "X_train_num_tensor = torch.tensor(X_train_num, dtype=torch.float32).to(device)\n",
    "X_test_cat_tensor = torch.tensor(X_test_cat.values, dtype=torch.int64).to(\n",
    "    device\n",
    ")  # Use int64 dtype for categorical indices\n",
    "X_test_num_tensor = torch.tensor(X_test_num, dtype=torch.float32).to(device)\n",
    "y_train_tensor = (\n",
    "    torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    ")\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.sparse.Embedding"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([32, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_model // n_heads\n",
    "\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # self.initialize_parameters()\n",
    "        self.apply(initialize_parameters)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        q = (\n",
    "            self.q_linear(q)\n",
    "            .view(batch_size, -1, self.n_heads, self.d_head)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        k = (\n",
    "            self.k_linear(k)\n",
    "            .view(batch_size, -1, self.n_heads, self.d_head)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "        v = (\n",
    "            self.v_linear(v)\n",
    "            .view(batch_size, -1, self.n_heads, self.d_head)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        attn_output, _ = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        attn_output = (\n",
    "            attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        )\n",
    "        out = self.out_linear(attn_output)\n",
    "        return out\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        matmul_qk = torch.matmul(q, k.transpose(-2, -1))\n",
    "        d_k = q.size(-1)\n",
    "        scaled_attention_logits = matmul_qk / (d_k**0.5)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += mask * -1e9\n",
    "\n",
    "        attention_weights = F.softmax(scaled_attention_logits, dim=-1)\n",
    "        output = torch.matmul(attention_weights, v)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        # self.initialize_parameters()\n",
    "        self.apply(initialize_parameters)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn_output = self.multi_head_attention(q, k, v, mask)\n",
    "        out1 = self.layernorm1(q + attn_output)\n",
    "\n",
    "        ff_output = self.feed_forward(out1)\n",
    "        out2 = self.layernorm2(out1 + ff_output)\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "# Parameters\n",
    "d_model = 64  # Embedding dimension\n",
    "n_heads = 4  # Number of attention heads\n",
    "seq_len_q = 10  # Sequence length for the query tensor\n",
    "seq_len_k = 20  # Sequence length for the key tensor\n",
    "batch_size = 32  # Batch size\n",
    "\n",
    "# Random data\n",
    "q = torch.rand((batch_size, seq_len_q, d_model))\n",
    "k = torch.rand((batch_size, seq_len_k, d_model))\n",
    "v = k  # Usually, value and key are the same in many applications\n",
    "\n",
    "# Model\n",
    "encoder_layer = TransformerEncoderLayer(d_model, n_heads)\n",
    "\n",
    "# Forward pass\n",
    "output = encoder_layer(q, k, v)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mask_tensor(tensor, model, probability=0.8):\n",
    "    if tensor.dtype == torch.float32:\n",
    "        is_numeric = True\n",
    "    elif tensor.dtype == torch.int64:\n",
    "        is_numeric = False\n",
    "    else:\n",
    "        raise ValueError(f\"Task {tensor.dtype} not supported.\")\n",
    "\n",
    "    tensor = tensor.clone()\n",
    "    bit_mask = torch.rand(tensor.shape) > probability\n",
    "    if is_numeric:\n",
    "        tensor[bit_mask] = torch.tensor(float(\"-Inf\"))\n",
    "    else:\n",
    "        tensor[bit_mask] = model.cat_mask_token\n",
    "    return tensor.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 9, 32]), torch.Size([3, 6]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TabTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokens,\n",
    "        numeric_col_tokens,\n",
    "        cat_col_tokens,\n",
    "        token_dict,\n",
    "        d_model=64,\n",
    "        n_heads=4,\n",
    "        device=device,\n",
    "    ):\n",
    "        super(TabTransformer, self).__init__()\n",
    "        self.device = device\n",
    "        self.d_model = d_model\n",
    "        self.tokens = tokens\n",
    "        self.token_dict = token_dict\n",
    "        self.decoder_dict = {v: k for k, v in token_dict.items()}\n",
    "        # Masks\n",
    "        self.cat_mask_token = torch.tensor(self.token_dict[\"[MASK]\"]).to(device)\n",
    "        self.numeric_mask_token = torch.tensor(self.token_dict[\"[NUMERIC_MASK]\"]).to(\n",
    "            device\n",
    "        )\n",
    "\n",
    "        self.col_tokens = cat_col_tokens + numeric_col_tokens\n",
    "        self.n_tokens = len(tokens)  # TODO Make this\n",
    "        # Embedding layers for categorical features\n",
    "        self.embeddings = nn.Embedding(self.n_tokens, self.d_model).to(device)\n",
    "        self.n_numeric_cols = len(numeric_col_tokens)\n",
    "        self.n_cat_cols = len(cat_col_tokens)\n",
    "        self.n_columns = self.n_numeric_cols + self.n_cat_cols\n",
    "        # self.numeric_embeddings = NumericEmbedding(d_model=self.d_model)\n",
    "        self.col_indices = torch.tensor(\n",
    "            [self.tokens.index(col) for col in self.col_tokens], dtype=torch.long\n",
    "        ).to(device)\n",
    "        self.numeric_indices = torch.tensor(\n",
    "            [self.tokens.index(col) for col in numeric_col_tokens], dtype=torch.long\n",
    "        ).to(device)\n",
    "        self.transformer_encoder = TransformerEncoderLayer(d_model, n_heads=n_heads).to(\n",
    "            device\n",
    "        )\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 2, 1),\n",
    "            nn.ReLU(),\n",
    "        ).to(device)\n",
    "\n",
    "        self.mlm_decoder = nn.Sequential(nn.Linear(d_model, self.n_tokens)).to(\n",
    "            device\n",
    "        )  # TODO try making more complex\n",
    "\n",
    "        self.mnm_decoder = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                self.n_columns * self.d_model, self.d_model * 4\n",
    "            ),  # Try making more complex\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.d_model * 4, self.n_numeric_cols),\n",
    "        ).to(device)\n",
    "\n",
    "        self.flatten_layer = nn.Linear(len(self.col_tokens), 1).to(device)\n",
    "        self.apply(initialize_parameters)\n",
    "\n",
    "    def forward(self, num_inputs, cat_inputs, task=\"regression\"):\n",
    "        # Embed column indices\n",
    "        repeated_col_indices = self.col_indices.unsqueeze(0).repeat(\n",
    "            num_inputs.size(0), 1\n",
    "        )\n",
    "        col_embeddings = self.embeddings(repeated_col_indices)\n",
    "\n",
    "        repeated_numeric_indices = self.numeric_indices.unsqueeze(0).repeat(\n",
    "            num_inputs.size(0), 1\n",
    "        )\n",
    "        numeric_col_embeddings = self.embeddings(repeated_numeric_indices)\n",
    "\n",
    "        cat_embeddings = self.embeddings(cat_inputs)\n",
    "\n",
    "        expanded_num_inputs = num_inputs.unsqueeze(2).repeat(1, 1, self.d_model)\n",
    "        inf_mask = (expanded_num_inputs == float(\"-inf\")).all(dim=2)\n",
    "        base_numeric = torch.zeros_like(expanded_num_inputs)\n",
    "\n",
    "        num_embeddings = (\n",
    "            numeric_col_embeddings[~inf_mask] * expanded_num_inputs[~inf_mask]\n",
    "        )\n",
    "        base_numeric[~inf_mask] = num_embeddings\n",
    "        base_numeric[inf_mask] = self.embeddings(self.numeric_mask_token)\n",
    "\n",
    "        query_embeddings = torch.cat([cat_embeddings, base_numeric], dim=1)\n",
    "        out = self.transformer_encoder(\n",
    "            col_embeddings,\n",
    "            # query_embeddings,\n",
    "            query_embeddings,\n",
    "            query_embeddings\n",
    "            # col_embeddings, query_embeddings, query_embeddings\n",
    "        )\n",
    "        if task == \"regression\":\n",
    "            out = self.regressor(out)\n",
    "            out = self.flatten_layer(out.squeeze(-1))\n",
    "\n",
    "            return out\n",
    "        elif task == \"mlm\":\n",
    "            cat_out = self.mlm_decoder(out)\n",
    "            # print(f\"Out shape: {out.shape}, cat_out shape: {cat_out.shape}\")\n",
    "            numeric_out = out.view(out.size(0), -1)\n",
    "            # print(f\"numeric_out shape: {numeric_out.shape}\")\n",
    "            numeric_out = self.mnm_decoder(numeric_out)\n",
    "            return cat_out, numeric_out\n",
    "        else:\n",
    "            raise ValueError(f\"Task {task} not supported.\")\n",
    "\n",
    "\n",
    "no_price_tokens = tokens.copy()\n",
    "no_price_tokens.remove(\"price\")\n",
    "\n",
    "numeric_col_tokens = (\n",
    "    df.head().drop(\"price\", axis=1).select_dtypes(include=np.number).columns.to_list()\n",
    ")\n",
    "cat_col_tokens = df.head().select_dtypes(exclude=np.number).columns.to_list()\n",
    "\n",
    "model = TabTransformer(\n",
    "    no_price_tokens,\n",
    "    numeric_col_tokens=numeric_col_tokens,\n",
    "    cat_col_tokens=cat_col_tokens,\n",
    "    token_dict=token_dict,\n",
    ").to(device)\n",
    "batch_size = 3\n",
    "test_num = X_train_num_tensor[0:batch_size, :]\n",
    "test_num_mask = mask_tensor(test_num, model)\n",
    "test_cat = X_train_cat_tensor[0:batch_size, :]\n",
    "test_cat_mask = mask_tensor(test_cat, model)\n",
    "with torch.no_grad():\n",
    "    x = model(\n",
    "        test_num_mask,\n",
    "        test_cat_mask,\n",
    "        task=\"mlm\",\n",
    "    )\n",
    "x[0].shape, x[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actual': {'cut': 'Good',\n",
       "  'color': 'F',\n",
       "  'clarity': 'SI2',\n",
       "  'carat': 2.56005597114563,\n",
       "  'depth': -2.5507476329803467,\n",
       "  'table': 2.9338605403900146,\n",
       "  'x': 2.229450225830078,\n",
       "  'y': 2.138209104537964,\n",
       "  'z': 1.7382067441940308},\n",
       " 'masked': {'cut': 'Good',\n",
       "  'color': 'F',\n",
       "  'clarity': 'SI2',\n",
       "  'carat': 2.56005597114563,\n",
       "  'depth': -2.5507476329803467,\n",
       "  'table': -inf,\n",
       "  'x': -inf,\n",
       "  'y': -inf,\n",
       "  'z': -inf},\n",
       " 'pred': {'cut': 'VS1',\n",
       "  'color': 'carat',\n",
       "  'clarity': 'carat',\n",
       "  'carat': 23.145444869995117,\n",
       "  'depth': 10.518811225891113,\n",
       "  'table': 24.12872314453125,\n",
       "  'x': 43.176856994628906,\n",
       "  'y': -1.0302671194076538,\n",
       "  'z': -14.537619590759277}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_mask_pred(i, model, probability):\n",
    "    numeric_values = X_train_num_tensor[i : i + 1, :]\n",
    "    categorical_values = X_train_cat_tensor[i : i + 1, :]\n",
    "    numeric_masked = mask_tensor(numeric_values, model, probability=probability)\n",
    "    categorical_masked = mask_tensor(categorical_values, model, probability=probability)\n",
    "    # Predictions\n",
    "    with torch.no_grad():\n",
    "        cat_preds, numeric_preds = model(numeric_masked, categorical_masked, task=\"mlm\")\n",
    "    # Get the predicted tokens from cat_preds\n",
    "    cat_preds = cat_preds.argmax(dim=2)\n",
    "    # Get the words from the tokens\n",
    "    decoder_dict = model.decoder_dict\n",
    "    cat_preds = [decoder_dict[i.item()] for i in cat_preds[0]]\n",
    "\n",
    "    results_dict = {k: cat_preds[i] for i, k in enumerate(model.col_tokens)}\n",
    "    for i, k in enumerate(model.col_tokens[model.n_cat_cols :]):\n",
    "        results_dict[k] = numeric_preds[0][i].item()\n",
    "    # Get the masked values\n",
    "    categorical_masked = [decoder_dict[i.item()] for i in categorical_masked[0]]\n",
    "    numeric_masked = numeric_masked[0].tolist()\n",
    "    masked_values = categorical_masked + numeric_masked\n",
    "    # zip the masked values with the column names\n",
    "    masked_values = dict(zip(model.col_tokens, masked_values))\n",
    "    # Get the original values\n",
    "    categorical_values = [decoder_dict[i.item()] for i in categorical_values[0]]\n",
    "    numeric_values = numeric_values[0].tolist()\n",
    "    original_values = categorical_values + numeric_values\n",
    "    # zip the original values with the column names\n",
    "    original_values = dict(zip(model.col_tokens, original_values))\n",
    "    # print(numeric_masked)\n",
    "    # print(categorical_masked)\n",
    "    result_dict = {\n",
    "        \"actual\": original_values,\n",
    "        \"masked\": masked_values,\n",
    "        \"pred\": results_dict,\n",
    "    }\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "show_mask_pred(0, model, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked Tabualr Modeling\n",
    "base_model_name = \"OneHundred\"\n",
    "\n",
    "model_time = dt.now()\n",
    "model_time = model_time.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "model_name = f\"{base_model_name}_{model_time}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 Loss: 2.8567\n",
      "Epoch 5/20 Loss: 2.0133\n",
      "Epoch 7/20 Loss: 1.5894\n",
      "Epoch 10/20 Loss: 1.2841\n",
      "Epoch 12/20 Loss: 1.1439\n",
      "Epoch 14/20 Loss: 1.0028\n",
      "Epoch 16/20 Loss: 0.9855\n",
      "Epoch 19/20 Loss: 0.9056\n"
     ]
    }
   ],
   "source": [
    "# Masked Tabualr Modeling\n",
    "epochs = 20\n",
    "batch_size = 1000\n",
    "lr = 0.01\n",
    "mse_loss = nn.MSELoss()\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "summary_writer = SummaryWriter(\"runs/\" + model_name)\n",
    "\n",
    "batch_count = 0\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, X_train_num_tensor.size(0), batch_size):\n",
    "        numeric_values = X_train_num_tensor[i : i + batch_size, :]\n",
    "        categorical_values = X_train_cat_tensor[i : i + batch_size, :]\n",
    "        numeric_masked = mask_tensor(numeric_values, model, probability=0.4)\n",
    "        categorical_masked = mask_tensor(categorical_values, model, probability=0.4)\n",
    "        optimizer.zero_grad()\n",
    "        cat_preds, numeric_preds = model(numeric_masked, categorical_masked, task=\"mlm\")\n",
    "        cat_targets = torch.cat(\n",
    "            (\n",
    "                categorical_values,\n",
    "                model.numeric_indices.expand(categorical_values.size(0), -1),\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        cat_preds = cat_preds.permute(0, 2, 1)\n",
    "        # print(\n",
    "        #     f\"cat_preds.shape: {cat_preds.shape}, cat_targets.shape: {cat_targets.shape}\"\n",
    "        # )\n",
    "        cat_loss = ce_loss(cat_preds, cat_targets)\n",
    "        numeric_loss = mse_loss(numeric_preds, numeric_values)\n",
    "        loss = cat_loss + numeric_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_count += 1\n",
    "        learning_rate = optimizer.param_groups[0][\"lr\"]\n",
    "        summary_writer.add_scalar(\"LossTrain/agg_mask\", loss.item(), batch_count)\n",
    "        summary_writer.add_scalar(\"LossTrain/mlm_loss\", cat_loss.item(), batch_count)\n",
    "        summary_writer.add_scalar(\n",
    "            \"LossTrain/mnm_loss\", numeric_loss.item(), batch_count\n",
    "        )\n",
    "        summary_writer.add_scalar(\"Metrics/mtm_lr\", learning_rate, batch_count)\n",
    "        if batch_count % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} Loss: {loss.item():,.4f}\")\n",
    "            # Test set\n",
    "            with torch.no_grad():\n",
    "                numeric_values = X_test_num_tensor\n",
    "                categorical_values = X_test_cat_tensor\n",
    "                numeric_masked = mask_tensor(numeric_values, model, probability=0.8)\n",
    "                categorical_masked = mask_tensor(\n",
    "                    categorical_values, model, probability=0.8\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                cat_preds, numeric_preds = model(\n",
    "                    numeric_masked, categorical_masked, task=\"mlm\"\n",
    "                )\n",
    "                cat_targets = torch.cat(\n",
    "                    (\n",
    "                        categorical_values,\n",
    "                        model.numeric_indices.expand(categorical_values.size(0), -1),\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "                cat_preds = cat_preds.permute(0, 2, 1)\n",
    "                # print(\n",
    "                #     f\"cat_preds.shape: {cat_preds.shape}, cat_targets.shape: {cat_targets.shape}\"\n",
    "                # )\n",
    "                cat_loss = ce_loss(cat_preds, cat_targets)\n",
    "                numeric_loss = mse_loss(numeric_preds, numeric_values)\n",
    "                loss = cat_loss + numeric_loss\n",
    "                summary_writer.add_scalar(\"LossTest/agg_loss\", loss.item(), batch_count)\n",
    "            summary_writer.add_scalar(\"LossTest/mlm_loss\", cat_loss.item(), batch_count)\n",
    "            summary_writer.add_scalar(\n",
    "                \"LossTest/mnm_loss\", numeric_loss.item(), batch_count\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actual': {'cut': 'Good',\n",
       "  'color': 'F',\n",
       "  'clarity': 'SI2',\n",
       "  'carat': 2.56005597114563,\n",
       "  'depth': -2.5507476329803467,\n",
       "  'table': 2.9338605403900146,\n",
       "  'x': 2.229450225830078,\n",
       "  'y': 2.138209104537964,\n",
       "  'z': 1.7382067441940308},\n",
       " 'masked': {'cut': 'Good',\n",
       "  'color': 'F',\n",
       "  'clarity': 'SI2',\n",
       "  'carat': 2.56005597114563,\n",
       "  'depth': -2.5507476329803467,\n",
       "  'table': 2.9338605403900146,\n",
       "  'x': 2.229450225830078,\n",
       "  'y': 2.138209104537964,\n",
       "  'z': -inf},\n",
       " 'pred': {'cut': 'Premium',\n",
       "  'color': 'I',\n",
       "  'clarity': 'SI2',\n",
       "  'carat': 0.8274498581886292,\n",
       "  'depth': -0.6303593516349792,\n",
       "  'table': 3.284083366394043,\n",
       "  'x': 3.764676332473755,\n",
       "  'y': 1.7112181186676025,\n",
       "  'z': 1.3544758558273315}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_mask_pred(0, model, 0.8)  # Check for learning... XFKAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning the previous model seems to work but when I pre-train, we run into issues. Let's try again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01853911753800519"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000 / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/40 Loss: 1,349,244.12 Test loss: 1,349,244.12\n",
      "Epoch 5/40 Loss: 790,227.94 Test loss: 790,227.94\n",
      "Epoch 7/40 Loss: 599,433.25 Test loss: 599,433.25\n",
      "Epoch 10/40 Loss: 546,980.00 Test loss: 546,980.00\n",
      "Epoch 12/40 Loss: 798,963.94 Test loss: 798,963.94\n",
      "Epoch 14/40 Loss: 503,871.94 Test loss: 503,871.94\n",
      "Epoch 16/40 Loss: 502,906.34 Test loss: 502,906.34\n",
      "Epoch 19/40 Loss: 844,923.19 Test loss: 844,923.19\n",
      "Epoch 21/40 Loss: 1,544,781.12 Test loss: 1,544,781.12\n",
      "Epoch 23/40 Loss: 824,364.50 Test loss: 824,364.50\n",
      "Epoch 25/40 Loss: 646,225.62 Test loss: 646,225.62\n",
      "Epoch 28/40 Loss: 2,756,673.00 Test loss: 2,756,673.00\n",
      "Epoch 30/40 Loss: 2,407,429.75 Test loss: 2,407,429.75\n",
      "Epoch 32/40 Loss: 1,916,535.62 Test loss: 1,916,535.62\n",
      "Epoch 35/40 Loss: 1,555,317.62 Test loss: 1,555,317.62\n",
      "Epoch 37/40 Loss: 1,528,491.75 Test loss: 1,528,491.75\n",
      "Epoch 39/40 Loss: 1,501,334.50 Test loss: 1,501,334.50\n"
     ]
    }
   ],
   "source": [
    "# Regression Model\n",
    "epochs = 40\n",
    "batch_size = 1000\n",
    "lr = 0.1\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# model_time = dt.now()\n",
    "# model_time = model_time.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "# model_name = f\"FullDSNFT{model_time}\"\n",
    "\n",
    "summary_writer = SummaryWriter(\"runs/\" + model_name)\n",
    "\n",
    "small_test = False\n",
    "\n",
    "batch_count = 0\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    # for i in range(0, batch_size, batch_size):\n",
    "    for i in range(0, X_train_num_tensor.size(0), batch_size):\n",
    "        num_inputs = X_train_num_tensor[i : i + batch_size, :]\n",
    "        cat_inputs = X_train_cat_tensor[i : i + batch_size, :]\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(num_inputs, cat_inputs)\n",
    "        loss = loss_fn(y_pred, y_train_tensor[i : i + batch_size, :])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_count += 1\n",
    "        learning_rate = optimizer.param_groups[0][\"lr\"]\n",
    "        summary_writer.add_scalar(\"LossTrain/regression_loss\", loss.item(), batch_count)\n",
    "        summary_writer.add_scalar(\"Metrics/regression_lr\", learning_rate, batch_count)\n",
    "        if (batch_count % 100 == 0) or small_test:\n",
    "            # Test set\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(X_test_num_tensor, X_test_cat_tensor)\n",
    "                loss = loss_fn(y_pred, y_test_tensor)\n",
    "                summary_writer.add_scalar(\n",
    "                    \"LossTest/regression_loss\", loss.item(), batch_count\n",
    "                )\n",
    "                print(\n",
    "                    f\"Epoch {epoch+1}/{epochs} Loss: {loss.item():,.2f} \"\n",
    "                    + f\"Test loss: {loss.item():,.2f}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Epoch 3/20 Loss: 416,668.31\n",
    "Epoch 5/20 Loss: 308,602.41\n",
    "Epoch 7/20 Loss: 375,060.84\n",
    "Epoch 10/20 Loss: 346,249.38\n",
    "Epoch 12/20 Loss: 421,598.84\n",
    "Epoch 14/20 Loss: 362,426.00\n",
    "Epoch 16/20 Loss: 339,481.22\n",
    "Epoch 19/20 Loss: 379,399.06\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1,659,003.25\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_num_tensor[0:10, :], X_test_cat_tensor[0:10, :])\n",
    "    loss = loss_fn(y_pred, y_test_tensor[0:10])\n",
    "    print(f\"Test loss: {loss.item():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 983.38 Actual: 559.00 Diff: 424.38\n",
      "Predicted: 3,007.07 Actual: 2,201.00 Diff: 806.07\n",
      "Predicted: 1,324.15 Actual: 1,238.00 Diff: 86.15\n",
      "Predicted: 1,280.24 Actual: 1,304.00 Diff: -23.76\n",
      "Predicted: 10,587.29 Actual: 6,901.00 Diff: 3,686.29\n",
      "Predicted: 2,165.83 Actual: 3,011.00 Diff: -845.17\n",
      "Predicted: 1,630.87 Actual: 1,765.00 Diff: -134.13\n",
      "Predicted: 2,078.92 Actual: 1,679.00 Diff: 399.92\n",
      "Predicted: 2,313.34 Actual: 2,102.00 Diff: 211.34\n",
      "Predicted: 5,896.50 Actual: 4,789.00 Diff: 1,107.50\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\n",
    "        f\"Predicted: {y_pred[i].item():,.2f} Actual: {y_test_tensor[i].item():,.2f}\",\n",
    "        f\"Diff: {y_pred[i].item() - y_test_tensor[i].item():,.2f}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Predicted: 1,711.44 Actual: 559.00 Diff: 1,152.44\n",
    "Predicted: 1,711.50 Actual: 2,201.00 Diff: -489.50\n",
    "Predicted: 1,711.50 Actual: 1,238.00 Diff: 473.50\n",
    "Predicted: 1,711.61 Actual: 1,304.00 Diff: 407.61\n",
    "Predicted: 1,710.44 Actual: 6,901.00 Diff: -5,190.56\n",
    "Predicted: 1,711.26 Actual: 3,011.00 Diff: -1,299.74\n",
    "Predicted: 1,711.60 Actual: 1,765.00 Diff: -53.40\n",
    "Predicted: 1,711.57 Actual: 1,679.00 Diff: 32.57\n",
    "Predicted: 1,711.03 Actual: 2,102.00 Diff: -390.97\n",
    "Predicted: 1,711.64 Actual: 4,789.00 Diff: -3,077.36\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Predicted: 2,085.14 Actual: 559.00 Diff: 1,526.14\n",
    "Predicted: 3,381.02 Actual: 2,201.00 Diff: 1,180.02\n",
    "Predicted: 1,725.17 Actual: 1,238.00 Diff: 487.17\n",
    "Predicted: 1,914.37 Actual: 1,304.00 Diff: 610.37\n",
    "Predicted: 15,271.41 Actual: 6,901.00 Diff: 8,370.41\n",
    "Predicted: 7,173.91 Actual: 3,011.00 Diff: 4,162.91\n",
    "Predicted: 947.38 Actual: 1,765.00 Diff: -817.62\n",
    "Predicted: 604.91 Actual: 1,679.00 Diff: -1,074.09\n",
    "Predicted: 989.06 Actual: 2,102.00 Diff: -1,112.94\n",
    "Predicted: 9,508.21 Actual: 4,789.00 Diff: 4,719.21\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rs/qflxwtyx6kvfj8jcqx5zm5hr0000gn/T/ipykernel_88739/55412900.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_cat[col] = le.fit_transform(X_train_cat.loc[:, col])\n",
      "/var/folders/rs/qflxwtyx6kvfj8jcqx5zm5hr0000gn/T/ipykernel_88739/55412900.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_cat[col] = le.transform(X_test_cat.loc[:, col])\n",
      "/var/folders/rs/qflxwtyx6kvfj8jcqx5zm5hr0000gn/T/ipykernel_88739/55412900.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_cat[col] = le.fit_transform(X_train_cat.loc[:, col])\n",
      "/var/folders/rs/qflxwtyx6kvfj8jcqx5zm5hr0000gn/T/ipykernel_88739/55412900.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_cat[col] = le.transform(X_test_cat.loc[:, col])\n",
      "/var/folders/rs/qflxwtyx6kvfj8jcqx5zm5hr0000gn/T/ipykernel_88739/55412900.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train_cat[col] = le.fit_transform(X_train_cat.loc[:, col])\n",
      "/var/folders/rs/qflxwtyx6kvfj8jcqx5zm5hr0000gn/T/ipykernel_88739/55412900.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test_cat[col] = le.transform(X_test_cat.loc[:, col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000, Loss: 31,439,204.00\n",
      "Epoch 201/5000, Loss: 516,823.56\n",
      "Epoch 401/5000, Loss: 368,840.97\n",
      "Epoch 601/5000, Loss: 326,413.62\n",
      "Epoch 801/5000, Loss: 302,466.31\n",
      "Epoch 1001/5000, Loss: 279,538.16\n",
      "Epoch 1201/5000, Loss: 275,397.34\n",
      "Epoch 1401/5000, Loss: 267,349.38\n",
      "Epoch 1601/5000, Loss: 258,712.78\n",
      "Epoch 1801/5000, Loss: 252,671.48\n",
      "Epoch 2001/5000, Loss: 249,290.19\n",
      "Epoch 2201/5000, Loss: 253,728.94\n",
      "Epoch 2401/5000, Loss: 253,446.72\n",
      "Epoch 2601/5000, Loss: 249,049.94\n",
      "Epoch 2801/5000, Loss: 243,626.77\n",
      "Epoch 3001/5000, Loss: 242,147.86\n",
      "Epoch 3201/5000, Loss: 253,064.92\n",
      "Epoch 3401/5000, Loss: 233,698.25\n",
      "Epoch 3601/5000, Loss: 235,164.19\n",
      "Epoch 3801/5000, Loss: 234,696.61\n",
      "Epoch 4001/5000, Loss: 231,627.31\n",
      "Epoch 4201/5000, Loss: 229,746.69\n",
      "Epoch 4401/5000, Loss: 228,175.61\n",
      "Epoch 4601/5000, Loss: 234,943.69\n",
      "Epoch 4801/5000, Loss: 225,193.94\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load and preprocess the dataset (assuming you have a CSV file)\n",
    "data = pd.read_csv(\"../data/diamonds.csv\")\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(\"price\", axis=1)\n",
    "y = data[\"price\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Preprocess categorical features\n",
    "cat_columns = [\"cut\", \"color\", \"clarity\"]\n",
    "X_train_cat = X_train[cat_columns]\n",
    "X_test_cat = X_test[cat_columns]\n",
    "\n",
    "label_encoders = {}\n",
    "for col in cat_columns:\n",
    "    le = LabelEncoder()\n",
    "    X_train_cat[col] = le.fit_transform(X_train_cat.loc[:, col])\n",
    "    X_test_cat[col] = le.transform(X_test_cat.loc[:, col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Preprocess numeric features\n",
    "num_columns = [\"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\"]\n",
    "scaler = StandardScaler()\n",
    "X_train_num = scaler.fit_transform(X_train[num_columns])\n",
    "X_test_num = scaler.transform(X_test[num_columns])\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_cat_tensor = torch.tensor(X_train_cat.values, dtype=torch.int64)\n",
    "X_train_num_tensor = torch.tensor(X_train_num, dtype=torch.float32)\n",
    "X_test_cat_tensor = torch.tensor(X_test_cat.values, dtype=torch.int64)\n",
    "X_test_num_tensor = torch.tensor(X_test_num, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "# Define the neural network model\n",
    "class DiamondPricePredictor(nn.Module):\n",
    "    def __init__(self, num_input_dim, cat_embedding_sizes, hidden_dim):\n",
    "        super(DiamondPricePredictor, self).__init__()\n",
    "\n",
    "        # Embedding layers for categorical features\n",
    "        self.embeddings = nn.ModuleList(\n",
    "            [\n",
    "                nn.Embedding(num_classes, emb_size)\n",
    "                for num_classes, emb_size in cat_embedding_sizes\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        total_emb_dim = sum(emb_size for _, emb_size in cat_embedding_sizes)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(num_input_dim + total_emb_dim, hidden_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "        # self.fc1 = nn.Linear(num_input_dim + total_emb_dim, hidden_dim)\n",
    "        # self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, num_inputs, cat_inputs):\n",
    "        embeddings = [\n",
    "            embedding(cat_inputs[:, i]) for i, embedding in enumerate(self.embeddings)\n",
    "        ]\n",
    "        cat_features = torch.cat(embeddings, dim=1)\n",
    "        x = torch.cat([num_inputs, cat_features], dim=1)\n",
    "\n",
    "        x = self.predictor(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "num_input_dim = X_train_num.shape[1]\n",
    "cat_embedding_sizes = [\n",
    "    (len(le.classes_), min(50, (len(le.classes_) + 1) // 2))\n",
    "    for le in label_encoders.values()\n",
    "]\n",
    "hidden_dim = 64\n",
    "simple_model = DiamondPricePredictor(num_input_dim, cat_embedding_sizes, hidden_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(simple_model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 2000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = simple_model(\n",
    "        X_train_num_tensor,  # [0:batch_size],\n",
    "        X_train_cat_tensor,  # [0:batch_size],\n",
    "    )  # Pass numeric and categorical tensors separately\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():,.2f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Data: 4,832,811.50\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_num_tensor, X_test_cat_tensor)\n",
    "    mse = mean_squared_error(y_test_tensor, test_predictions)\n",
    "    print(f\"Mean Squared Error on Test Data: {mse:,.2f}\")  # 735,349.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Params in Tabular Model:211,825 Number of Params in Simple Model:45,900\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Number of Params in Tabular Model:{count_parameters(model):,}\",\n",
    "    f\"Number of Params in Simple Model:{count_parameters(simple_model):,}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 278,657.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the diamonds dataset\n",
    "diamonds_data = pd.read_csv(\"../data/diamonds.csv\")\n",
    "\n",
    "# Encode categorical features using LabelEncoder\n",
    "label_encoders = {}\n",
    "categorical_features = [\"cut\", \"color\", \"clarity\"]\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    diamonds_data[feature] = le.fit_transform(diamonds_data[feature])\n",
    "    label_encoders[feature] = le\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = diamonds_data.drop(\"price\", axis=1)\n",
    "y = diamonds_data[\"price\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train the XGBoost regressor\n",
    "xgb_regressor = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "xgb_regressor.fit(\n",
    "    X_train,  # <[0:batch_size],\n",
    "    y_train,  # [0:batch_size],\n",
    ")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:,.2f}\")\n",
    "\n",
    "# You can also access feature importance scores\n",
    "# feature_importances = xgb_regressor.feature_importances_\n",
    "# print(\"Feature Importance:\")\n",
    "# for feature, importance in zip(X.columns, feature_importances):\n",
    "#     print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas==2.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1,700,000.00'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{1.7e6:,.2f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "lcc_arn": "arn:aws:sagemaker:us-west-2:385115691352:studio-lifecycle-config/base-installs-widgets",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
