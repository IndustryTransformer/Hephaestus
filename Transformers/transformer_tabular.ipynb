{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_At9AOGvu7Wb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mye_8WKbu7We"
   },
   "source": [
    "**bold text**\n",
    "# Language Modeling with ``nn.Transformer`` and torchtext\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "This is a tutorial on training a sequence-to-sequence model that uses the\n",
    "[nn.Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)_ module.\n",
    "\n",
    "The PyTorch 1.2 release includes a standard transformer module based on the\n",
    "paper [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)_.\n",
    "Compared to Recurrent Neural Networks (RNNs), the transformer model has proven\n",
    "to be superior in quality for many sequence-to-sequence tasks while being more\n",
    "parallelizable. The ``nn.Transformer`` module relies entirely on an attention\n",
    "mechanism (implemented as\n",
    "[nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)_)\n",
    "to draw global dependencies between input and output. The ``nn.Transformer``\n",
    "module is highly modularized such that a single component (e.g.,\n",
    "[nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html)_)\n",
    "can be easily adapted/composed.\n",
    "\n",
    "<img src=\"file://../_static/img/transformer_architecture.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJo8MGMwu7Wf"
   },
   "source": [
    "## Define the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3clWEJ31wImv",
    "outputId": "5e8ef4e8-453a-4bae-957e-10b5454244ac",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture installs\n",
    "%pip install polars\n",
    "%pip install torchviz\n",
    "# %conda install -y conda install -c  python-graphviz\n",
    "# %conda install -c fastchan python-graphviz -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0smESe4au7Wh"
   },
   "source": [
    "In this tutorial, we train a ``nn.TransformerEncoder`` model on a\n",
    "language modeling task. The language modeling task is to assign a\n",
    "probability for the likelihood of a given word (or a sequence of words)\n",
    "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
    "layer first, followed by a positional encoding layer to account for the order\n",
    "of the word (see the next paragraph for more details). The\n",
    "``nn.TransformerEncoder`` consists of multiple layers of\n",
    "[nn.TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)_.\n",
    "Along with the input sequence, a square attention mask is required because the\n",
    "self-attention layers in ``nn.TransformerEncoder`` are only allowed to attend\n",
    "the earlier positions in the sequence. For the language modeling task, any\n",
    "tokens on the future positions should be masked. To produce a probability\n",
    "distribution over output words, the output of the ``nn.TransformerEncoder``\n",
    "model is passed through a linear layer followed by a log-softmax function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "o_YarIxhu7Wi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import re\n",
    "from numbers import Number\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import torch\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "yqI8ATppxGcw",
    "outputId": "c8069e1b-7e2b-4767-c505-38450eec9810",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_built():\n",
    "    device_name = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    device_name = 'cuda'\n",
    "else:\n",
    "    device_name = 'cpu'\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "AA0aLnKp7ZUm",
    "outputId": "1dd592ff-9484-49c0-b7a3-4581b458798a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 25)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>x</th><th>y</th><th>station_name</th><th>climate_identifier</th><th>province_code</th><th>local_year</th><th>local_month</th><th>local_day</th><th>local_hour</th><th>temp</th><th>temp_flag</th><th>dew_point_temp</th><th>dew_point_temp_flag</th><th>humidex</th><th>precip_amount</th><th>precip_amount_flag</th><th>relative_humidity</th><th>relative_humidity_flag</th><th>station_pressure</th><th>station_pressure_flag</th><th>wind_chill</th><th>wind_direction</th><th>wind_direction_flag</th><th>wind_speed</th><th>wind_speed_flag</th></tr><tr><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>-114.000297</td><td>51.109447</td><td>&quot;CALGARY INT&#x27;L …</td><td>&quot;3031094&quot;</td><td>&quot;AB&quot;</td><td>&quot;2010&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>&quot;0&quot;</td><td>-21.6</td><td>&quot;missing&quot;</td><td>-23.9</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;missing&quot;</td><td>82.0</td><td>&quot;missing&quot;</td><td>89.38</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;M&quot;</td><td>null</td><td>&quot;M&quot;</td></tr><tr><td>-114.000297</td><td>51.109447</td><td>&quot;CALGARY INT&#x27;L …</td><td>&quot;3031094&quot;</td><td>&quot;AB&quot;</td><td>&quot;2010&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>-21.2</td><td>&quot;missing&quot;</td><td>-23.5</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;missing&quot;</td><td>82.0</td><td>&quot;missing&quot;</td><td>89.25</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;M&quot;</td><td>null</td><td>&quot;M&quot;</td></tr><tr><td>-114.000297</td><td>51.109447</td><td>&quot;CALGARY INT&#x27;L …</td><td>&quot;3031094&quot;</td><td>&quot;AB&quot;</td><td>&quot;2010&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>&quot;2&quot;</td><td>-20.8</td><td>&quot;missing&quot;</td><td>-23.0</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;missing&quot;</td><td>82.0</td><td>&quot;missing&quot;</td><td>89.21</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;M&quot;</td><td>null</td><td>&quot;M&quot;</td></tr><tr><td>-114.000297</td><td>51.109447</td><td>&quot;CALGARY INT&#x27;L …</td><td>&quot;3031094&quot;</td><td>&quot;AB&quot;</td><td>&quot;2010&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>&quot;3&quot;</td><td>-20.4</td><td>&quot;missing&quot;</td><td>-22.6</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;missing&quot;</td><td>83.0</td><td>&quot;missing&quot;</td><td>89.12</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;M&quot;</td><td>null</td><td>&quot;M&quot;</td></tr><tr><td>-114.000297</td><td>51.109447</td><td>&quot;CALGARY INT&#x27;L …</td><td>&quot;3031094&quot;</td><td>&quot;AB&quot;</td><td>&quot;2010&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>&quot;4&quot;</td><td>-20.4</td><td>&quot;missing&quot;</td><td>-22.7</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;missing&quot;</td><td>82.0</td><td>&quot;missing&quot;</td><td>89.04</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;M&quot;</td><td>null</td><td>&quot;M&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 25)\n",
       "┌─────────┬────────┬────────────┬────────────┬───┬────────────┬────────────┬──────────┬────────────┐\n",
       "│ x       ┆ y      ┆ station_na ┆ climate_id ┆ … ┆ wind_direc ┆ wind_direc ┆ wind_spe ┆ wind_speed │\n",
       "│ ---     ┆ ---    ┆ me         ┆ entifier   ┆   ┆ tion       ┆ tion_flag  ┆ ed       ┆ _flag      │\n",
       "│ f64     ┆ f64    ┆ ---        ┆ ---        ┆   ┆ ---        ┆ ---        ┆ ---      ┆ ---        │\n",
       "│         ┆        ┆ str        ┆ str        ┆   ┆ f64        ┆ str        ┆ f64      ┆ str        │\n",
       "╞═════════╪════════╪════════════╪════════════╪═══╪════════════╪════════════╪══════════╪════════════╡\n",
       "│ -114.00 ┆ 51.109 ┆ CALGARY    ┆ 3031094    ┆ … ┆ null       ┆ M          ┆ null     ┆ M          │\n",
       "│ 0297    ┆ 447    ┆ INT'L CS   ┆            ┆   ┆            ┆            ┆          ┆            │\n",
       "│ -114.00 ┆ 51.109 ┆ CALGARY    ┆ 3031094    ┆ … ┆ null       ┆ M          ┆ null     ┆ M          │\n",
       "│ 0297    ┆ 447    ┆ INT'L CS   ┆            ┆   ┆            ┆            ┆          ┆            │\n",
       "│ -114.00 ┆ 51.109 ┆ CALGARY    ┆ 3031094    ┆ … ┆ null       ┆ M          ┆ null     ┆ M          │\n",
       "│ 0297    ┆ 447    ┆ INT'L CS   ┆            ┆   ┆            ┆            ┆          ┆            │\n",
       "│ -114.00 ┆ 51.109 ┆ CALGARY    ┆ 3031094    ┆ … ┆ null       ┆ M          ┆ null     ┆ M          │\n",
       "│ 0297    ┆ 447    ┆ INT'L CS   ┆            ┆   ┆            ┆            ┆          ┆            │\n",
       "│ -114.00 ┆ 51.109 ┆ CALGARY    ┆ 3031094    ┆ … ┆ null       ┆ M          ┆ null     ┆ M          │\n",
       "│ 0297    ┆ 447    ┆ INT'L CS   ┆            ┆   ┆            ┆            ┆          ┆            │\n",
       "└─────────┴────────┴────────────┴────────────┴───┴────────────┴────────────┴──────────┴────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = pl.read_parquet(\"~/Hephaestus/data/weather_clean.parquet\")\n",
    "weather.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzAjejIIu7Wj"
   },
   "source": [
    "``PositionalEncoding`` module injects some information about the\n",
    "relative or absolute position of the tokens in the sequence. The\n",
    "positional encodings have the same dimension as the embeddings so that\n",
    "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
    "different frequencies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VJu-X9OS8m3N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_numeric(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == pl.Float64 or df[col].dtype == pl.Int64:\n",
    "            df = df.with_columns(\n",
    "                ((pl.col(col) - pl.col(col).mean()) / pl.col(col).std()).alias(col)\n",
    "            )  # .select(pl.col([\"dew_point_temp\", \"NewCOL\"]))\n",
    "    return df\n",
    "\n",
    "\n",
    "weather = scale_numeric(weather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JZFqyv5D8o31",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_lower_remove_special_chars(df):\n",
    "    df = df.with_columns(\n",
    "        pl.col(pl.Utf8).str.to_lowercase().str.replace_all(\"[^a-zA-Z0-9]\", \" \")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "weather = make_lower_remove_special_chars(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uzf8dqGa8qyi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_unique_utf8_values(df):\n",
    "    arr = np.array([])\n",
    "    for col in df.select(pl.col(pl.Utf8)).columns:\n",
    "        arr = np.append(arr, df[col].unique().to_numpy())\n",
    "\n",
    "    return np.unique(arr)\n",
    "\n",
    "\n",
    "weather_val_tokens = get_unique_utf8_values(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qx2sWqAA8t64",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_col_tokens(df):\n",
    "    tokens = []\n",
    "    for col_name in df.columns:\n",
    "        sub_strs = re.split(r\"[^a-zA-Z0-9]\", col_name)\n",
    "        tokens.extend(sub_strs)\n",
    "    return np.unique(np.array(tokens))\n",
    "\n",
    "\n",
    "weather_col_tokens = get_col_tokens(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZgAXkAI8wKq",
    "outputId": "da13b04e-67e3-42f3-b563-321df8fa4683",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([',', '0', '1', '10', '11', '12', '13', '14', '15', '16', '17',\n",
       "       '18', '19', '2', '20', '2010', '2011', '2012', '2013', '2014',\n",
       "       '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022',\n",
       "       '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30',\n",
       "       '3012206', '3026knq', '3031094', '3033890', '3035208', '3062696',\n",
       "       '31', '4', '5', '6', '7', '8', '9', ':', '<mask>', '<numeric>',\n",
       "       '<pad>', '<row-end>', '<row-start>', '<unk>', 'ab', 'amount',\n",
       "       'calgary int l cs', 'chill', 'climate', 'code', 'day', 'dew',\n",
       "       'direction', 'edmonton international cs', 'flag',\n",
       "       'fort mcmurray cs', 'hour', 'humidex', 'humidity', 'identifier',\n",
       "       'lethbridge cda', 'local', 'm', 'missing', 'month', 'name',\n",
       "       'pincher creek climate', 'point', 'precip', 'pressure', 'province',\n",
       "       'relative', 'speed', 'station', 'sundre a', 'temp', 'wind', 'x',\n",
       "       'y', 'year'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = np.array(\n",
    "    [\n",
    "        \"missing\",\n",
    "        \"<mask>\",\n",
    "        \"<pad>\",\n",
    "        \"<unk>\",\n",
    "        \"<numeric>\",\n",
    "        \":\",\n",
    "        \",\",\n",
    "        \"<row-start>\",\n",
    "        \"<row-end>\",\n",
    "    ]\n",
    ")\n",
    "tokens = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            weather_val_tokens,\n",
    "            weather_col_tokens,\n",
    "            special_tokens,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 100_000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ot6vmpOS9mbH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# class TransformerModel(nn.Module):\n",
    "\n",
    "#     def __init__(self, n_token: int, d_model: int, n_head: int, d_hid: int,\n",
    "#                 n_layers: int, dropout: float = 0.15):\n",
    "#         super().__init__()\n",
    "#         self.n_token = n_token +1\n",
    "#         self.model_type = 'Transformer'\n",
    "#         self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "#         encoder_layers = TransformerEncoderLayer(d_model, n_head, d_hid, dropout)\n",
    "#         self.transformer_encoder = TransformerEncoder(encoder_layers, n_layers)\n",
    "#         self.encoder = StringNumericEmbedding(n_token, d_model)\n",
    "#         self.d_model = d_model\n",
    "#         self.decoder = nn.Linear(d_model, n_token)\n",
    "#         self.numeric_decoder = nn.Linear(d_model, n_token)\n",
    "\n",
    "#         # self.numeric_decoder = nn.Linear(d_model)\n",
    "\n",
    "\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def init_weights(self) -> None:\n",
    "#         initrange = 0.1\n",
    "#         self.encoder.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "#         self.decoder.bias.data.zero_()\n",
    "#         self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "#         # self.numeric_decoder.data.uniform(-initrange, initrange)\n",
    "\n",
    "#     def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "#         \"\"\"\n",
    "#         Arguments:\n",
    "#             src: Tensor, shape ``[seq_len, batch_size]``\n",
    "#             src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "#         Returns:\n",
    "#             output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "#         \"\"\"\n",
    "#         # src_shape = src.shape\n",
    "#         print(f\"raw src_shape: {len(src)}\")\n",
    "#         src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "#         print(f\"encoded src_shape: {src.shape}\")\n",
    "\n",
    "#         src_shape = src.shape\n",
    "#         src = self.pos_encoder(src)\n",
    "#         output = self.transformer_encoder(src)\n",
    "#         print(f\"output_shape: {output.shape}\")\n",
    "#         numeric_output = self.numeric_decoder(output)  # .flatten()\n",
    "#         numeric_output = torch.mean(numeric_output, [1, 2])\n",
    "#         # numeric_output = nn.flatten(numeric_output)\n",
    "#         output = self.decoder(output)\n",
    "#         print(f\"output_shape decoded: {output.shape}\")\n",
    "#         output = output.view(-1, self.n_token+1)\n",
    "#         # output = output.view(-1, src_shape[0]).T\n",
    "#         print(f\"output_shape view: {output.shape}\")\n",
    "\n",
    "#         return output, numeric_output\n",
    "\n",
    "\n",
    "# def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "#     \"\"\"Generates an upper-triangular matrix of ``-inf``, with zeros on ``diag``.\"\"\"\n",
    "#     return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Y5tWWWk83DM",
    "outputId": "56f7fe30-06e7-4bc6-9df7-783e050baff9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringNumeric(value='climate', is_numeric=False, embedding_idx=None, is_special=False)\n",
      "StringNumeric(value=1.0, is_numeric=True, embedding_idx=0, is_special=False)\n",
      "StringNumeric(value='SomeRandomString', is_numeric=False, embedding_idx=None, is_special=False)\n",
      "StringNumeric(value='climate', is_numeric=False, embedding_idx=64, is_special=False)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class StringNumeric:\n",
    "    value: Union[str, float]\n",
    "    # all_tokens: np.array\n",
    "    is_numeric: bool = field(default=None, repr=True)\n",
    "    embedding_idx: int = field(default=None, repr=True)\n",
    "    is_special: bool = field(default=False, repr=True)\n",
    "    def __post_init__(self):\n",
    "        if isinstance(self.value, str):\n",
    "            self.is_numeric = False\n",
    "        else:\n",
    "            self.is_numeric = True\n",
    "            self.embedding_idx = 0\n",
    "\n",
    "    def gen_embed_idx(self, tokens: np.array, special_tokens: np.array):\n",
    "        if not self.is_numeric:\n",
    "            try:\n",
    "                self.embedding_idx = np.where(tokens == self.value)[0][0] + 1\n",
    "            except IndexError:\n",
    "                self.embedding_idx = np.where(tokens == \"<unk>\")[0][0] + 1\n",
    "            if self.value in special_tokens:\n",
    "                self.is_special = True\n",
    "\n",
    "\n",
    "\n",
    "x = StringNumeric(value=\"climate\")\n",
    "# xx = StringNumeric(value=\"climate\", tokens=tokens)\n",
    "print(x)\n",
    "y = StringNumeric(value=1.0)\n",
    "print(y)\n",
    "z = StringNumeric(value=\"SomeRandomString\")\n",
    "print(z)\n",
    "x.gen_embed_idx(tokens, special_tokens)\n",
    "print(x)\n",
    "# print(StringNumeric(value=1.0, all_tokens=tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETGBMjYt858a",
    "outputId": "b47e0ea3-e60e-4551-c9a8-54168d2e84f9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    }
   ],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    # def __init__(self, df: pl.DataFrame, vocab_dict: Dict, m_dim: int) -> Dataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pl.DataFrame,\n",
    "        vocab,\n",
    "        special_tokens: np.array,\n",
    "        shuffle_cols=False,\n",
    "        max_row_length=512,\n",
    "    ) -> Dataset:\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.special_tokens = special_tokens\n",
    "        self.vocab_len = vocab.shape[0]\n",
    "        self.shuffle_cols = shuffle_cols\n",
    "        self.max_row_length = max_row_length\n",
    "        # self.vocab_dict = vocab_dict\n",
    "        # self.embedding = nn.Embedding(len(self.string_vocab), m_dim)\n",
    "        # Numeric Scale\n",
    "\n",
    "        # self.col_vocab = self.df.columns\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of sequences in the dataset.\"\"\"\n",
    "        length = self.df.shape[0]\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns a tuple of (input, target) at the given index.\"\"\"\n",
    "        row = self.df[idx]\n",
    "        row = self.splitter(row)\n",
    "        return row\n",
    "\n",
    "\n",
    "\n",
    "    def splitter(self, row: pl.DataFrame) -> List[Union[str, float, None]]:\n",
    "        vals = [\"<row-start>\"]\n",
    "        cols = row.columns\n",
    "        if self.shuffle_cols:\n",
    "            np.random.shuffle(cols)\n",
    "\n",
    "        for col in cols:\n",
    "            value = row[col][0]\n",
    "            col = col.split(\"_\")\n",
    "            vals.extend(col)\n",
    "            vals.append(\":\")\n",
    "            if isinstance(value, Number):\n",
    "                vals.append(value)\n",
    "            elif value is None:\n",
    "                vals.append(\"missing\")\n",
    "                # Nones are only for numeric columns, others are \"None\"\n",
    "            elif isinstance(value, str):\n",
    "                vals.extend(value.split(\" \"))\n",
    "            else:\n",
    "                raise ValueError(\"Unknown type\")\n",
    "            vals.append(\",\")\n",
    "        vals.append(\"<row-end>\")\n",
    "\n",
    "        val_len = len(vals)\n",
    "        if val_len < self.max_row_length:\n",
    "            diff = self.max_row_length - val_len\n",
    "            vals.extend([\"<pad>\"] * diff)\n",
    "        elif val_len > self.max_row_length:\n",
    "            vals = vals[:self.max_row_length - 1]\n",
    "            # add warning\n",
    "\n",
    "            vals = np.append(vals, [\"<row-end>\"])\n",
    "            print(\"Row too long, truncating\")\n",
    "            Warning(\"Row too long, truncating\")\n",
    "        vals = [StringNumeric(value=val) for val in vals]\n",
    "        for val in vals:\n",
    "            val.gen_embed_idx(self.vocab, self.special_tokens)\n",
    "\n",
    "        return vals\n",
    "\n",
    "\n",
    "weather_ds = TabularDataset(weather, tokens, special_tokens=special_tokens, shuffle_cols=False, max_row_length=140 )\n",
    "\n",
    "print(len(weather_ds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([',', '0', '1', '10', '11', '12', '13', '14', '15', '16', '17',\n",
       "       '18', '19', '2', '20', '2010', '2011', '2012', '2013', '2014',\n",
       "       '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022',\n",
       "       '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30',\n",
       "       '3012206', '3026knq', '3031094', '3033890', '3035208', '3062696',\n",
       "       '31', '4', '5', '6', '7', '8', '9', ':', '<mask>', '<numeric>',\n",
       "       '<pad>', '<row-end>', '<row-start>', '<unk>', 'ab', 'amount',\n",
       "       'calgary int l cs', 'chill', 'climate', 'code', 'day', 'dew',\n",
       "       'direction', 'edmonton international cs', 'flag',\n",
       "       'fort mcmurray cs', 'hour', 'humidex', 'humidity', 'identifier',\n",
       "       'lethbridge cda', 'local', 'm', 'missing', 'month', 'name',\n",
       "       'pincher creek climate', 'point', 'precip', 'pressure', 'province',\n",
       "       'relative', 'speed', 'station', 'sundre a', 'temp', 'wind', 'x',\n",
       "       'y', 'year'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_ds.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "k4v20RSDE_L-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StringNumericEmbedding(nn.Module):\n",
    "    def __init__(self, n_token: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_token+1, d_model, padding_idx=0).to(device)\n",
    "\n",
    "    def forward(self, input: StringNumeric):\n",
    "        embedding_index = torch.tensor([i.embedding_idx for i in input]).to(device)\n",
    "        embed = self.embedding(embedding_index)\n",
    "        with torch.no_grad():\n",
    "            for idx, value in enumerate(input):\n",
    "                if value.is_numeric:\n",
    "                    embed[idx][0] = value.value\n",
    "        return embed\n",
    "\n",
    "my_embed = StringNumericEmbedding(weather_ds.vocab_len+1, 32)\n",
    "\n",
    "t = my_embed(weather_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([140, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringNumeric(value='<row-start>', is_numeric=False, embedding_idx=58, is_special=True),\n",
       " StringNumeric(value='x', is_numeric=False, embedding_idx=93, is_special=False),\n",
       " StringNumeric(value=':', is_numeric=False, embedding_idx=53, is_special=True)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mask_row(row):\n",
    "    row = row[:]\n",
    "    prob = 0.15\n",
    "    for idx, val in enumerate(row):\n",
    "        if val.is_special:\n",
    "            continue\n",
    "        if np.random.rand() < prob:\n",
    "            val = StringNumeric(value=\"<mask>\")\n",
    "            val.gen_embed_idx(tokens, special_tokens)\n",
    "            row[idx] = val\n",
    "    return row\n",
    "\n",
    "x = mask_row(weather_ds[0])\n",
    "x[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=54, is_special=True) != StringNumeric(value='cs', is_numeric=False, embedding_idx=59, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=54, is_special=True) != StringNumeric(value='local', is_numeric=False, embedding_idx=77, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=54, is_special=True) != StringNumeric(value='day', is_numeric=False, embedding_idx=66, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=54, is_special=True) != StringNumeric(value=-2.034580198428538, is_numeric=True, embedding_idx=0, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=54, is_special=True) != StringNumeric(value='dew', is_numeric=False, embedding_idx=67, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=54, is_special=True) != StringNumeric(value='flag', is_numeric=False, embedding_idx=70, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=54, is_special=True) != StringNumeric(value='wind', is_numeric=False, embedding_idx=92, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=54, is_special=True) != StringNumeric(value='speed', is_numeric=False, embedding_idx=88, is_special=False)\n"
     ]
    }
   ],
   "source": [
    "for idx, val in enumerate(x):\n",
    "    if val == weather_ds[0][idx]:\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"{val} != {weather_ds[0][idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGFJV7L6u7Wl"
   },
   "source": [
    "## Load and batch data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "635664"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weather_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_data(ds, idx: int, n_row=4):\n",
    "    target = []\n",
    "    if len(ds) > n_row+idx:\n",
    "        end_idx = n_row+idx\n",
    "    else:\n",
    "        end_idx = len(ds) - 1\n",
    "    for i in range(idx, end_idx):\n",
    "        target.extend(ds[i])\n",
    "    \n",
    "    batch = mask_row(target)\n",
    "\n",
    "    return batch, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d37K28qhu7Wv"
   },
   "source": [
    "The model hyperparameters are defined below. The ``vocab`` size is\n",
    "equal to the length of the vocab object.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1820 1820\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data, targets = batch_data(weather_ds, idx=635650, n_row=50)\n",
    "print(len(data), len(targets))\n",
    "# data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringNumeric(value='<row-start>', is_numeric=False, embedding_idx=58, is_special=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_embed = StringNumericEmbedding(weather_ds.vocab_len+1, 32)\n",
    "\n",
    "t = my_embed(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1820, 32])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_token: int, d_model: int, n_head: int, d_hid: int,\n",
    "                n_layers: int, dropout: float = 0.15):\n",
    "        super().__init__()\n",
    "        n_token = n_token +1\n",
    "        self.n_token = n_token\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, n_head, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, n_layers)\n",
    "        self.encoder = StringNumericEmbedding(n_token, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, n_token)\n",
    "        self.numeric_decoder = nn.Linear(d_model, n_token)\n",
    "        self.numeric_flattener = nn.Linear(n_token, 1)\n",
    "\n",
    "        # self.numeric_decoder = nn.Linear(d_model)\n",
    "\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # self.numeric_decoder.data.uniform(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        # src_shape = src.shape\n",
    "        # print(f\"raw src_shape: {len(src)}\")\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = torch.unsqueeze(src, dim=1)\n",
    "        # print(f\"encoded src_shape: {src.shape}\")\n",
    "\n",
    "        src_shape = src.shape\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        # print(f\"output_shape: {output.shape}\")\n",
    "        numeric_output = self.numeric_decoder(output)  # .flatten()\n",
    "        numeric_output = torch.squeeze(numeric_output, dim=1)\n",
    "        # numeric_output = torch.mean(numeric_output, [1])\n",
    "        numeric_output = self.numeric_flattener(numeric_output)\n",
    "        # numeric_output = nn.flatten(numeric_output)\n",
    "        output = self.decoder(output)\n",
    "        output = torch.squeeze(output, dim=1)\n",
    "        numeric_output = numeric_output.view(output.shape[0])\n",
    "\n",
    "        # print(f\"output_shape decoded: {output.shape}\")\n",
    "        # output = output.view(-1, self.n_token+1)\n",
    "        # output = output.view(-1, src_shape[0]).T\n",
    "        # print(f\"output_shape view: {output.shape}\")\n",
    "\n",
    "        return output, numeric_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "bKh3bGPMu7Ww",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_token = len(weather_ds.vocab)  # size of vocabulary\n",
    "d_model = 32  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "n_layers = 4  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "n_head = 4  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(n_token, d_model, n_head, d_hid, n_layers, dropout).to(device)\n",
    "# initial_memory_allocated = torch.cuda.memory_allocated()\n",
    "# initial_memory_reserved = torch.cuda.memory_reserved()\n",
    "# c,n = model(data)\n",
    "# final_memory_allocated = torch.cuda.memory_allocated()\n",
    "# final_memory_reserved = torch.cuda.memory_reserved()\n",
    "# print(f'Increase in memory allocated: {final_memory_allocated - initial_memory_allocated}')\n",
    "# print(f'Increase in memory reserved: {final_memory_reserved - initial_memory_reserved}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# c.shape, n.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzPeL_mMu7Ww"
   },
   "source": [
    "## Run the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "\n",
    "# make_dot((c,n), params=dict(list(model.named_parameters())))# .render(\"rnn_torchviz\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHd3tFaWu7Ww"
   },
   "source": [
    "We use [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)_\n",
    "with the [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)_\n",
    "(stochastic gradient descent) optimizer. The learning rate is initially set to\n",
    "5.0 and follows a [StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)_\n",
    "schedule. During training, we use [nn.utils.clip_grad_norm\\_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)_\n",
    "to prevent gradients from exploding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_loss(class_preds, numeric_preds, raw_data):\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "    mse_loss = nn.MSELoss()\n",
    "    raw_data_numeric_class = raw_data[:]\n",
    "    \n",
    "    for idx, val in enumerate(raw_data_numeric_class):\n",
    "        if val.is_numeric:\n",
    "            val = StringNumeric(value=\"<numeric>\")\n",
    "            val.gen_embed_idx(tokens, special_tokens)\n",
    "            raw_data_numeric_class[idx] = val\n",
    "\n",
    "    class_target = torch.tensor([i.embedding_idx for i in raw_data_numeric_class]).to(device)\n",
    "    class_loss = cross_entropy(class_preds, class_target)\n",
    "    \n",
    "    actual_num_idx = torch.tensor([idx for idx, j in enumerate(raw_data) if j.is_numeric]).to(device)\n",
    "    pred_nums = numeric_preds[actual_num_idx]\n",
    "    # print(actual_num_idx.shape)\n",
    "    actual_nums = torch.tensor([i.value for i in raw_data if i.is_numeric]).to(device)\n",
    "    # print(actual_nums.shape)\n",
    "    # print(pred_nums.shape)\n",
    "    reg_loss = mse_loss(pred_nums, actual_nums)\n",
    "    reg_loss_adjuster = 1 # class_loss/reg_loss\n",
    "    \n",
    "    return reg_loss*reg_loss_adjuster+class_loss, {\"reg_loss\": reg_loss,\n",
    "                                                  \"class_loss\": class_loss}\n",
    "\n",
    "# custom_loss_manual(cat, num, targets)\n",
    "\n",
    "# loss, d = custom_loss(c, n, data)\n",
    "# custom_loss(c, n, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "T8PaN3N2u7Wx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "lr = 0.99  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size =100, gamma=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=5, threshold=0.001, threshold_mode='rel', cooldown=0, min_lr=0.01, eps=1e-08, verbose=False)\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 50\n",
    "    n_row=50\n",
    "    start_time = time.time()\n",
    "    # src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    for batch, i in enumerate(trange(0, len(weather_ds) - 1, n_row)):\n",
    "        \n",
    "    # for batch, i in enumerate(range(len(weather_ds))):\n",
    "        \n",
    "        data, targets = batch_data(weather_ds, i, n_row=n_row)\n",
    "        class_output, numeric_output = model(data)\n",
    "        loss, loss_dict = custom_loss(class_output, numeric_output, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            # lr = scheduler.get_last_lr()[0]\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            \n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(#f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | ',\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}', loss_dict)\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "            scheduler.step(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77lXi9Cou7Wx"
   },
   "source": [
    "Loop over epochs. Save the model if the validation loss is the best\n",
    "we've seen so far. Adjust the learning rate after each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 51/12714 [00:21<1:28:52,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 429.52 |  loss  3.64 | ppl    38.13 {'reg_loss': tensor(0.2878, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(1.9063, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 101/12714 [00:41<1:25:28,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 407.79 |  loss  1.98 | ppl     7.23 {'reg_loss': tensor(0.4231, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(1.1885, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 151/12714 [01:02<1:24:02,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 408.00 |  loss  1.47 | ppl     4.37 {'reg_loss': tensor(1.8655, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.7782, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 201/12714 [01:22<1:24:53,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 411.15 |  loss  1.28 | ppl     3.59 {'reg_loss': tensor(0.5743, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.6258, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 251/12714 [01:43<1:28:51,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 410.46 |  loss  0.98 | ppl     2.66 {'reg_loss': tensor(1.1502, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.5130, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 301/12714 [02:03<1:24:20,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 409.47 |  loss  0.97 | ppl     2.64 {'reg_loss': tensor(0.3442, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.4575, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 351/12714 [02:24<1:27:40,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 412.51 |  loss  0.79 | ppl     2.20 {'reg_loss': tensor(0.3452, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.4325, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 401/12714 [02:44<1:23:07,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 409.72 |  loss  0.75 | ppl     2.11 {'reg_loss': tensor(0.5529, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.3997, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 451/12714 [03:05<1:26:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 411.64 |  loss  0.85 | ppl     2.33 {'reg_loss': tensor(0.4885, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.3794, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 501/12714 [03:26<1:22:54,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 411.87 |  loss  0.72 | ppl     2.05 {'reg_loss': tensor(0.2869, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.3886, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 551/12714 [03:46<1:21:55,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 411.23 |  loss  0.65 | ppl     1.92 {'reg_loss': tensor(0.3372, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.3557, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 601/12714 [04:07<1:22:28,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 410.54 |  loss  0.71 | ppl     2.03 {'reg_loss': tensor(0.4134, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.3762, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 651/12714 [04:27<1:20:47,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 409.62 |  loss  0.69 | ppl     2.00 {'reg_loss': tensor(0.2736, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.3338, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 701/12714 [04:48<1:21:39,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 410.03 |  loss  0.67 | ppl     1.95 {'reg_loss': tensor(0.4595, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.3381, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 751/12714 [05:08<1:20:06,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 409.33 |  loss  0.67 | ppl     1.96 {'reg_loss': tensor(0.3113, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.3166, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 801/12714 [05:29<1:21:13,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 410.86 |  loss  0.88 | ppl     2.41 {'reg_loss': tensor(0.3540, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.3415, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 802/12714 [05:29<1:20:57,  2.45it/s]"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_path.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420 420\n"
     ]
    }
   ],
   "source": [
    "data, targets = batch_data(weather_ds, idx=0, n_row=3)\n",
    "print(len(data), len(targets))\n",
    "# data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat, num = model(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9815, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " {'reg_loss': tensor(0.6359, device='cuda:0', grad_fn=<MseLossBackward0>),\n",
       "  'class_loss': tensor(0.3456, device='cuda:0', grad_fn=<NllLossBackward0>)})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def custom_loss_manual(class_preds, numeric_preds, raw_data):\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "    mse_loss = nn.MSELoss()\n",
    "    raw_data_numeric_class = raw_data[:]\n",
    "    \n",
    "    for idx, val in enumerate(raw_data_numeric_class):\n",
    "        if val.is_numeric:\n",
    "            val = StringNumeric(value=\"<numeric>\")\n",
    "            val.gen_embed_idx(tokens, special_tokens)\n",
    "            raw_data_numeric_class[idx] = val\n",
    "\n",
    "    class_target = torch.tensor([i.embedding_idx for i in raw_data_numeric_class]).to(device)\n",
    "    class_loss = cross_entropy(class_preds, class_target)\n",
    "    \n",
    "    actual_num_idx = torch.tensor([idx for idx, j in enumerate(raw_data) if j.is_numeric]).to(device)\n",
    "    pred_nums = numeric_preds[actual_num_idx]\n",
    "    # print(actual_num_idx.shape)\n",
    "    actual_nums = torch.tensor([i.value for i in raw_data if i.is_numeric]).to(device)\n",
    "    # print(actual_nums.shape)\n",
    "    # print(pred_nums.shape)\n",
    "    reg_loss = mse_loss(pred_nums, actual_nums)\n",
    "    reg_loss_adjuster = 1 # class_loss/reg_loss\n",
    "    \n",
    "    return reg_loss*reg_loss_adjuster+class_loss, {\"reg_loss\": reg_loss,\n",
    "                                                  \"class_loss\": class_loss}\n",
    "\n",
    "custom_loss_manual(cat, num, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lsm = nn.Softmax(dim=0)\n",
    "l_cat = lsm(cat.T)\n",
    "l_cat = torch.argmax(l_cat, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<row-start> x : num_-0.8656710386276245 , y : num_-0.6392325162887573 , temp name : <unk> <unk> <unk> <unk> , climate identifier : 3031094 , province num_-0.39019083976745605 : ab , local year : name , local month : dew , local day : 1 , local hour : 9 , temp : num_-1.5012503862380981 , temp flag : missing , dew point temp : num_-0.38628333806991577 , dew point temp flag : missing , humidex : missing , precip amount : missing , precip flag flag : missing , relative humidity : num_-0.5297474265098572 , relative humidity flag : missing , station pressure : num_-1.1136837005615234 , station pressure flag : missing , wind chill : missing , wind direction : missing , flag direction flag : m , wind speed : missing , wind speed flag : m , '"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tokens = []\n",
    "for idx, pred in enumerate(l_cat):\n",
    "    token = tokens[pred-1]\n",
    "    if token == \"<numeric>\":\n",
    "        gen_tokens.append(\"num_\" + str(num[idx].item()))\n",
    "    else:\n",
    "        gen_tokens.append(token)\n",
    "preds = \" \".join(gen_tokens)\n",
    "preds.split(\"<row-end>\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<row-start> x : -0.551099305737714 , y : -0.37817406811183396 , station name : calgary int l cs , climate identifier : 3031094 , province code : ab , local year : 2010 , local month : 1 , local day : 1 , local hour : 0 , temp : -2.034580198428538 , temp flag : missing , dew point temp : -2.0356676557539686 , dew point temp flag : missing , humidex : missing , precip amount : missing , precip amount flag : missing , relative humidity : 0.6997231826522904 , relative humidity flag : missing , station pressure : -0.39734550502172294 , station pressure flag : missing , wind chill : missing , wind direction : missing , wind direction flag : m , wind speed : missing , wind speed flag : m , '"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actuals = [str(i.value) for i in targets]\n",
    "actuals_ = \" \".join(actuals)\n",
    "actuals_.split(\"<row-end>\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data, i):\n",
    "    model.eval\n",
    "    with torch.no_grad():\n",
    "        data, targets = batch_data(weather_ds, i, n_row=4)\n",
    "        class_output, numeric_output = model(data)\n",
    "        loss, loss_dict = custom_loss(class_output, numeric_output, targets)\n",
    "        \n",
    "        return data, targets, class_output, numeric_output, loss, loss_dict\n",
    "data, targets, class_output, numeric_output, loss, loss_dict = evaluate(model, weather_ds, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.softmax(class_output, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "input = torch.randn(2, 3)\n",
    "test_out = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_out.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input, test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            if seq_len != bptt:\n",
    "                src_mask = src_mask[:seq_len, :seq_len]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, n_token)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "8hksJGvRu7Wy",
    "outputId": "a4115cda-835e-46aa-e4d5-4c24741b24a1"
   },
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zc8hciQ0u7Wy"
   },
   "source": [
    "## Evaluate the best model on the test dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMB7A-pgu7Wy"
   },
   "outputs": [],
   "source": [
    "test_loss = evaluate(model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pVQ0yeGvL9O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0eFqpWpaAbKZ"
   },
   "outputs": [],
   "source": [
    "class StringNumericEmbedding(nn.Embedding):\n",
    "    def __init__(self, string_numeric:StringNumeric, embed_dim: int=28):\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
