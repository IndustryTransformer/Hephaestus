{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_At9AOGvu7Wb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mye_8WKbu7We"
   },
   "source": [
    "**bold text**\n",
    "# Language Modeling with ``nn.Transformer`` and torchtext\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "This is a tutorial on training a sequence-to-sequence model that uses the\n",
    "[nn.Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)_ module.\n",
    "\n",
    "The PyTorch 1.2 release includes a standard transformer module based on the\n",
    "paper [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)_.\n",
    "Compared to Recurrent Neural Networks (RNNs), the transformer model has proven\n",
    "to be superior in quality for many sequence-to-sequence tasks while being more\n",
    "parallelizable. The ``nn.Transformer`` module relies entirely on an attention\n",
    "mechanism (implemented as\n",
    "[nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)_)\n",
    "to draw global dependencies between input and output. The ``nn.Transformer``\n",
    "module is highly modularized such that a single component (e.g.,\n",
    "[nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html)_)\n",
    "can be easily adapted/composed.\n",
    "\n",
    "<img src=\"file://../_static/img/transformer_architecture.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJo8MGMwu7Wf"
   },
   "source": [
    "## Define the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3clWEJ31wImv",
    "outputId": "5e8ef4e8-453a-4bae-957e-10b5454244ac",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture installs\n",
    "%pip install polars\n",
    "%pip install torchviz\n",
    "# %conda install -y conda install -c  python-graphviz\n",
    "# %conda install -c fastchan python-graphviz -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0smESe4au7Wh"
   },
   "source": [
    "In this tutorial, we train a ``nn.TransformerEncoder`` model on a\n",
    "language modeling task. The language modeling task is to assign a\n",
    "probability for the likelihood of a given word (or a sequence of words)\n",
    "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
    "layer first, followed by a positional encoding layer to account for the order\n",
    "of the word (see the next paragraph for more details). The\n",
    "``nn.TransformerEncoder`` consists of multiple layers of\n",
    "[nn.TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)_.\n",
    "Along with the input sequence, a square attention mask is required because the\n",
    "self-attention layers in ``nn.TransformerEncoder`` are only allowed to attend\n",
    "the earlier positions in the sequence. For the language modeling task, any\n",
    "tokens on the future positions should be masked. To produce a probability\n",
    "distribution over output words, the output of the ``nn.TransformerEncoder``\n",
    "model is passed through a linear layer followed by a log-softmax function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "o_YarIxhu7Wi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import re\n",
    "from numbers import Number\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import torch\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "yqI8ATppxGcw",
    "outputId": "c8069e1b-7e2b-4767-c505-38450eec9810",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_built():\n",
    "    device_name = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    device_name = 'cuda'\n",
    "else:\n",
    "    device_name = 'cpu'\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "AA0aLnKp7ZUm",
    "outputId": "1dd592ff-9484-49c0-b7a3-4581b458798a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 25)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>x</th><th>y</th><th>station_name</th><th>climate_identifier</th><th>province_code</th><th>local_year</th><th>local_month</th><th>local_day</th><th>local_hour</th><th>temp</th><th>temp_flag</th><th>dew_point_temp</th><th>dew_point_temp_flag</th><th>humidex</th><th>precip_amount</th><th>precip_amount_flag</th><th>relative_humidity</th><th>relative_humidity_flag</th><th>station_pressure</th><th>station_pressure_flag</th><th>wind_chill</th><th>wind_direction</th><th>wind_direction_flag</th><th>wind_speed</th><th>wind_speed_flag</th></tr><tr><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>-114.000297</td><td>51.109447</td><td>&quot;CALGARY INT&#x27;L …</td><td>&quot;3031094&quot;</td><td>&quot;AB&quot;</td><td>&quot;2010&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>&quot;0&quot;</td><td>-21.6</td><td>&quot;missing&quot;</td><td>-23.9</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;missing&quot;</td><td>82.0</td><td>&quot;missing&quot;</td><td>89.38</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;M&quot;</td><td>null</td><td>&quot;M&quot;</td></tr><tr><td>-114.000297</td><td>51.109447</td><td>&quot;CALGARY INT&#x27;L …</td><td>&quot;3031094&quot;</td><td>&quot;AB&quot;</td><td>&quot;2010&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>-21.2</td><td>&quot;missing&quot;</td><td>-23.5</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;missing&quot;</td><td>82.0</td><td>&quot;missing&quot;</td><td>89.25</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;M&quot;</td><td>null</td><td>&quot;M&quot;</td></tr><tr><td>-114.000297</td><td>51.109447</td><td>&quot;CALGARY INT&#x27;L …</td><td>&quot;3031094&quot;</td><td>&quot;AB&quot;</td><td>&quot;2010&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>&quot;2&quot;</td><td>-20.8</td><td>&quot;missing&quot;</td><td>-23.0</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;missing&quot;</td><td>82.0</td><td>&quot;missing&quot;</td><td>89.21</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;M&quot;</td><td>null</td><td>&quot;M&quot;</td></tr><tr><td>-114.000297</td><td>51.109447</td><td>&quot;CALGARY INT&#x27;L …</td><td>&quot;3031094&quot;</td><td>&quot;AB&quot;</td><td>&quot;2010&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>&quot;3&quot;</td><td>-20.4</td><td>&quot;missing&quot;</td><td>-22.6</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;missing&quot;</td><td>83.0</td><td>&quot;missing&quot;</td><td>89.12</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;M&quot;</td><td>null</td><td>&quot;M&quot;</td></tr><tr><td>-114.000297</td><td>51.109447</td><td>&quot;CALGARY INT&#x27;L …</td><td>&quot;3031094&quot;</td><td>&quot;AB&quot;</td><td>&quot;2010&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>&quot;4&quot;</td><td>-20.4</td><td>&quot;missing&quot;</td><td>-22.7</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;missing&quot;</td><td>82.0</td><td>&quot;missing&quot;</td><td>89.04</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;M&quot;</td><td>null</td><td>&quot;M&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 25)\n",
       "┌─────────┬────────┬────────────┬────────────┬───┬────────────┬────────────┬──────────┬────────────┐\n",
       "│ x       ┆ y      ┆ station_na ┆ climate_id ┆ … ┆ wind_direc ┆ wind_direc ┆ wind_spe ┆ wind_speed │\n",
       "│ ---     ┆ ---    ┆ me         ┆ entifier   ┆   ┆ tion       ┆ tion_flag  ┆ ed       ┆ _flag      │\n",
       "│ f64     ┆ f64    ┆ ---        ┆ ---        ┆   ┆ ---        ┆ ---        ┆ ---      ┆ ---        │\n",
       "│         ┆        ┆ str        ┆ str        ┆   ┆ f64        ┆ str        ┆ f64      ┆ str        │\n",
       "╞═════════╪════════╪════════════╪════════════╪═══╪════════════╪════════════╪══════════╪════════════╡\n",
       "│ -114.00 ┆ 51.109 ┆ CALGARY    ┆ 3031094    ┆ … ┆ null       ┆ M          ┆ null     ┆ M          │\n",
       "│ 0297    ┆ 447    ┆ INT'L CS   ┆            ┆   ┆            ┆            ┆          ┆            │\n",
       "│ -114.00 ┆ 51.109 ┆ CALGARY    ┆ 3031094    ┆ … ┆ null       ┆ M          ┆ null     ┆ M          │\n",
       "│ 0297    ┆ 447    ┆ INT'L CS   ┆            ┆   ┆            ┆            ┆          ┆            │\n",
       "│ -114.00 ┆ 51.109 ┆ CALGARY    ┆ 3031094    ┆ … ┆ null       ┆ M          ┆ null     ┆ M          │\n",
       "│ 0297    ┆ 447    ┆ INT'L CS   ┆            ┆   ┆            ┆            ┆          ┆            │\n",
       "│ -114.00 ┆ 51.109 ┆ CALGARY    ┆ 3031094    ┆ … ┆ null       ┆ M          ┆ null     ┆ M          │\n",
       "│ 0297    ┆ 447    ┆ INT'L CS   ┆            ┆   ┆            ┆            ┆          ┆            │\n",
       "│ -114.00 ┆ 51.109 ┆ CALGARY    ┆ 3031094    ┆ … ┆ null       ┆ M          ┆ null     ┆ M          │\n",
       "│ 0297    ┆ 447    ┆ INT'L CS   ┆            ┆   ┆            ┆            ┆          ┆            │\n",
       "└─────────┴────────┴────────────┴────────────┴───┴────────────┴────────────┴──────────┴────────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = pl.read_parquet(\"~/Hephaestus/data/weather_clean.parquet\")\n",
    "weather.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzAjejIIu7Wj"
   },
   "source": [
    "``PositionalEncoding`` module injects some information about the\n",
    "relative or absolute position of the tokens in the sequence. The\n",
    "positional encodings have the same dimension as the embeddings so that\n",
    "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
    "different frequencies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VJu-X9OS8m3N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_numeric(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == pl.Float64 or df[col].dtype == pl.Int64:\n",
    "            df = df.with_columns(\n",
    "                ((pl.col(col) - pl.col(col).mean()) / pl.col(col).std()).alias(col)\n",
    "            )  # .select(pl.col([\"dew_point_temp\", \"NewCOL\"]))\n",
    "    return df\n",
    "\n",
    "\n",
    "weather = scale_numeric(weather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JZFqyv5D8o31",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_lower_remove_special_chars(df):\n",
    "    df = df.with_columns(\n",
    "        pl.col(pl.Utf8).str.to_lowercase().str.replace_all(\"[^a-zA-Z0-9]\", \" \")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "weather = make_lower_remove_special_chars(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uzf8dqGa8qyi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_unique_utf8_values(df):\n",
    "    arr = np.array([])\n",
    "    for col in df.select(pl.col(pl.Utf8)).columns:\n",
    "        arr = np.append(arr, df[col].unique().to_numpy())\n",
    "\n",
    "    return np.unique(arr)\n",
    "\n",
    "\n",
    "weather_val_tokens = get_unique_utf8_values(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qx2sWqAA8t64",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_col_tokens(df):\n",
    "    tokens = []\n",
    "    for col_name in df.columns:\n",
    "        sub_strs = re.split(r\"[^a-zA-Z0-9]\", col_name)\n",
    "        tokens.extend(sub_strs)\n",
    "    return np.unique(np.array(tokens))\n",
    "\n",
    "\n",
    "weather_col_tokens = get_col_tokens(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZgAXkAI8wKq",
    "outputId": "da13b04e-67e3-42f3-b563-321df8fa4683",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([',', '0', '1', '10', '11', '12', '13', '14', '15', '16', '17',\n",
       "       '18', '19', '2', '20', '2010', '2011', '2012', '2013', '2014',\n",
       "       '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022',\n",
       "       '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30',\n",
       "       '3012206', '3026knq', '3031094', '3033890', '3035208', '3062696',\n",
       "       '31', '4', '5', '6', '7', '8', '9', ':', '<mask><pad>',\n",
       "       '<row-end>', '<row-start>', '<unk>', 'ab', 'amount',\n",
       "       'calgary int l cs', 'chill', 'climate', 'code', 'day', 'dew',\n",
       "       'direction', 'edmonton international cs', 'flag',\n",
       "       'fort mcmurray cs', 'hour', 'humidex', 'humidity', 'identifier',\n",
       "       'lethbridge cda', 'local', 'm', 'missing', 'month', 'name',\n",
       "       'pincher creek climate', 'point', 'precip', 'pressure', 'province',\n",
       "       'relative', 'speed', 'station', 'sundre a', 'temp', 'wind', 'x',\n",
       "       'y', 'year'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = np.array(\n",
    "    [\n",
    "        \"missing\",\n",
    "        \"<mask>\"\n",
    "        \"<pad>\",\n",
    "        \"<unk>\",\n",
    "        \":\",\n",
    "        \",\",\n",
    "        \"<row-start>\",\n",
    "        \"<row-end>\",\n",
    "    ]\n",
    ")\n",
    "tokens = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            weather_val_tokens,\n",
    "            weather_col_tokens,\n",
    "            special_tokens,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 100_000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ot6vmpOS9mbH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# class TransformerModel(nn.Module):\n",
    "\n",
    "#     def __init__(self, n_token: int, d_model: int, n_head: int, d_hid: int,\n",
    "#                 n_layers: int, dropout: float = 0.15):\n",
    "#         super().__init__()\n",
    "#         self.n_token = n_token +1\n",
    "#         self.model_type = 'Transformer'\n",
    "#         self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "#         encoder_layers = TransformerEncoderLayer(d_model, n_head, d_hid, dropout)\n",
    "#         self.transformer_encoder = TransformerEncoder(encoder_layers, n_layers)\n",
    "#         self.encoder = StringNumericEmbedding(n_token, d_model)\n",
    "#         self.d_model = d_model\n",
    "#         self.decoder = nn.Linear(d_model, n_token)\n",
    "#         self.numeric_decoder = nn.Linear(d_model, n_token)\n",
    "\n",
    "#         # self.numeric_decoder = nn.Linear(d_model)\n",
    "\n",
    "\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def init_weights(self) -> None:\n",
    "#         initrange = 0.1\n",
    "#         self.encoder.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "#         self.decoder.bias.data.zero_()\n",
    "#         self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "#         # self.numeric_decoder.data.uniform(-initrange, initrange)\n",
    "\n",
    "#     def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "#         \"\"\"\n",
    "#         Arguments:\n",
    "#             src: Tensor, shape ``[seq_len, batch_size]``\n",
    "#             src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "#         Returns:\n",
    "#             output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "#         \"\"\"\n",
    "#         # src_shape = src.shape\n",
    "#         print(f\"raw src_shape: {len(src)}\")\n",
    "#         src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "#         print(f\"encoded src_shape: {src.shape}\")\n",
    "\n",
    "#         src_shape = src.shape\n",
    "#         src = self.pos_encoder(src)\n",
    "#         output = self.transformer_encoder(src)\n",
    "#         print(f\"output_shape: {output.shape}\")\n",
    "#         numeric_output = self.numeric_decoder(output)  # .flatten()\n",
    "#         numeric_output = torch.mean(numeric_output, [1, 2])\n",
    "#         # numeric_output = nn.flatten(numeric_output)\n",
    "#         output = self.decoder(output)\n",
    "#         print(f\"output_shape decoded: {output.shape}\")\n",
    "#         output = output.view(-1, self.n_token+1)\n",
    "#         # output = output.view(-1, src_shape[0]).T\n",
    "#         print(f\"output_shape view: {output.shape}\")\n",
    "\n",
    "#         return output, numeric_output\n",
    "\n",
    "\n",
    "# def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "#     \"\"\"Generates an upper-triangular matrix of ``-inf``, with zeros on ``diag``.\"\"\"\n",
    "#     return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Y5tWWWk83DM",
    "outputId": "56f7fe30-06e7-4bc6-9df7-783e050baff9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringNumeric(value='climate', is_numeric=False, embedding_idx=None, is_special=False)\n",
      "StringNumeric(value=1.0, is_numeric=True, embedding_idx=0, is_special=False)\n",
      "StringNumeric(value='SomeRandomString', is_numeric=False, embedding_idx=None, is_special=False)\n",
      "StringNumeric(value='climate', is_numeric=False, embedding_idx=62, is_special=False)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class StringNumeric:\n",
    "    value: Union[str, float]\n",
    "    # all_tokens: np.array\n",
    "    is_numeric: bool = field(default=None, repr=True)\n",
    "    embedding_idx: int = field(default=None, repr=True)\n",
    "    is_special: bool = field(default=False, repr=True)\n",
    "    def __post_init__(self):\n",
    "        if isinstance(self.value, str):\n",
    "            self.is_numeric = False\n",
    "        else:\n",
    "            self.is_numeric = True\n",
    "            self.embedding_idx = 0\n",
    "\n",
    "    def gen_embed_idx(self, tokens: np.array, special_tokens: np.array):\n",
    "        if not self.is_numeric:\n",
    "            try:\n",
    "                self.embedding_idx = np.where(tokens == self.value)[0][0] + 1\n",
    "            except IndexError:\n",
    "                self.embedding_idx = np.where(tokens == \"<unk>\")[0][0] + 1\n",
    "            if self.value in special_tokens:\n",
    "                self.is_special = True\n",
    "\n",
    "\n",
    "\n",
    "x = StringNumeric(value=\"climate\")\n",
    "# xx = StringNumeric(value=\"climate\", tokens=tokens)\n",
    "print(x)\n",
    "y = StringNumeric(value=1.0)\n",
    "print(y)\n",
    "z = StringNumeric(value=\"SomeRandomString\")\n",
    "print(z)\n",
    "x.gen_embed_idx(tokens, special_tokens)\n",
    "print(x)\n",
    "# print(StringNumeric(value=1.0, all_tokens=tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETGBMjYt858a",
    "outputId": "b47e0ea3-e60e-4551-c9a8-54168d2e84f9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    }
   ],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    # def __init__(self, df: pl.DataFrame, vocab_dict: Dict, m_dim: int) -> Dataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pl.DataFrame,\n",
    "        vocab,\n",
    "        special_tokens: np.array,\n",
    "        shuffle_cols=False,\n",
    "        max_row_length=512,\n",
    "    ) -> Dataset:\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.special_tokens = special_tokens\n",
    "        self.vocab_len = vocab.shape[0]\n",
    "        self.shuffle_cols = shuffle_cols\n",
    "        self.max_row_length = max_row_length\n",
    "        # self.vocab_dict = vocab_dict\n",
    "        # self.embedding = nn.Embedding(len(self.string_vocab), m_dim)\n",
    "        # Numeric Scale\n",
    "\n",
    "        # self.col_vocab = self.df.columns\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of sequences in the dataset.\"\"\"\n",
    "        length = self.df.shape[0]\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns a tuple of (input, target) at the given index.\"\"\"\n",
    "        row = self.df[idx]\n",
    "        row = self.splitter(row)\n",
    "        return row\n",
    "\n",
    "\n",
    "\n",
    "    def splitter(self, row: pl.DataFrame) -> List[Union[str, float, None]]:\n",
    "        vals = [\"<row-start>\"]\n",
    "        cols = row.columns\n",
    "        if self.shuffle_cols:\n",
    "            np.random.shuffle(cols)\n",
    "\n",
    "        for col in cols:\n",
    "            value = row[col][0]\n",
    "            col = col.split(\"_\")\n",
    "            vals.extend(col)\n",
    "            vals.append(\":\")\n",
    "            if isinstance(value, Number):\n",
    "                vals.append(value)\n",
    "            elif value is None:\n",
    "                vals.append(\"missing\")\n",
    "                # Nones are only for numeric columns, others are \"None\"\n",
    "            elif isinstance(value, str):\n",
    "                vals.extend(value.split(\" \"))\n",
    "            else:\n",
    "                raise ValueError(\"Unknown type\")\n",
    "            vals.append(\",\")\n",
    "        vals.append(\"<row-end>\")\n",
    "\n",
    "        val_len = len(vals)\n",
    "        if val_len < self.max_row_length:\n",
    "            diff = self.max_row_length - val_len\n",
    "            vals.extend([\"<pad>\"] * diff)\n",
    "        elif val_len > self.max_row_length:\n",
    "            vals = vals[:self.max_row_length - 1]\n",
    "            # add warning\n",
    "\n",
    "            vals = np.append(vals, [\"<row-end>\"])\n",
    "            print(\"Row too long, truncating\")\n",
    "            Warning(\"Row too long, truncating\")\n",
    "        vals = [StringNumeric(value=val) for val in vals]\n",
    "        for val in vals:\n",
    "            val.gen_embed_idx(self.vocab, self.special_tokens)\n",
    "\n",
    "        return vals\n",
    "\n",
    "\n",
    "weather_ds = TabularDataset(weather, tokens, special_tokens=special_tokens, shuffle_cols=False, max_row_length=140 )\n",
    "\n",
    "print(len(weather_ds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([',', '0', '1', '10', '11', '12', '13', '14', '15', '16', '17',\n",
       "       '18', '19', '2', '20', '2010', '2011', '2012', '2013', '2014',\n",
       "       '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022',\n",
       "       '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30',\n",
       "       '3012206', '3026knq', '3031094', '3033890', '3035208', '3062696',\n",
       "       '31', '4', '5', '6', '7', '8', '9', ':', '<mask><pad>',\n",
       "       '<row-end>', '<row-start>', '<unk>', 'ab', 'amount',\n",
       "       'calgary int l cs', 'chill', 'climate', 'code', 'day', 'dew',\n",
       "       'direction', 'edmonton international cs', 'flag',\n",
       "       'fort mcmurray cs', 'hour', 'humidex', 'humidity', 'identifier',\n",
       "       'lethbridge cda', 'local', 'm', 'missing', 'month', 'name',\n",
       "       'pincher creek climate', 'point', 'precip', 'pressure', 'province',\n",
       "       'relative', 'speed', 'station', 'sundre a', 'temp', 'wind', 'x',\n",
       "       'y', 'year'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_ds.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "k4v20RSDE_L-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StringNumericEmbedding(nn.Module):\n",
    "    def __init__(self, n_token: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_token+1, d_model, padding_idx=0).to(device)\n",
    "\n",
    "    def forward(self, input: StringNumeric):\n",
    "        embedding_index = torch.tensor([i.embedding_idx for i in input]).to(device)\n",
    "        embed = self.embedding(embedding_index)\n",
    "        with torch.no_grad():\n",
    "            for idx, value in enumerate(input):\n",
    "                if value.is_numeric:\n",
    "                    embed[idx][0] = value.value\n",
    "        return embed\n",
    "\n",
    "my_embed = StringNumericEmbedding(weather_ds.vocab_len+1, 32)\n",
    "\n",
    "t = my_embed(weather_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([140, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringNumeric(value='<row-start>', is_numeric=False, embedding_idx=56, is_special=True),\n",
       " StringNumeric(value='x', is_numeric=False, embedding_idx=91, is_special=False),\n",
       " StringNumeric(value=':', is_numeric=False, embedding_idx=53, is_special=True)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mask_row(row):\n",
    "    prob = 0.15\n",
    "    for idx, val in enumerate(row):\n",
    "        if val.is_special:\n",
    "            continue\n",
    "        if np.random.rand() < prob:\n",
    "            val = StringNumeric(value=\"<mask>\")\n",
    "            val.gen_embed_idx(tokens, special_tokens)\n",
    "            row[idx] = val\n",
    "    return row\n",
    "\n",
    "x = mask_row(weather_ds[0])\n",
    "x[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='int', is_numeric=False, embedding_idx=57, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='l', is_numeric=False, embedding_idx=57, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='year', is_numeric=False, embedding_idx=93, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='local', is_numeric=False, embedding_idx=75, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='day', is_numeric=False, embedding_idx=64, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='dew', is_numeric=False, embedding_idx=65, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='precip', is_numeric=False, embedding_idx=82, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='humidity', is_numeric=False, embedding_idx=72, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='wind', is_numeric=False, embedding_idx=90, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='wind', is_numeric=False, embedding_idx=90, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='wind', is_numeric=False, embedding_idx=90, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='<pad>', is_numeric=False, embedding_idx=57, is_special=False)\n"
     ]
    }
   ],
   "source": [
    "for idx, val in enumerate(x):\n",
    "    if val == weather_ds[0][idx]:\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"{val} != {weather_ds[0][idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGFJV7L6u7Wl"
   },
   "source": [
    "## Load and batch data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_data(ds, idx: int, n_row=4):\n",
    "    target = []\n",
    "    for i in range(idx, idx+n_row):\n",
    "        target.extend(ds[i])\n",
    "    batch = mask_row(target)\n",
    "\n",
    "    return batch, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d37K28qhu7Wv"
   },
   "source": [
    "The model hyperparameters are defined below. The ``vocab`` size is\n",
    "equal to the length of the vocab object.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 7000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data, targets = batch_data(weather_ds, idx=0, n_row=50)\n",
    "print(len(data), len(targets))\n",
    "# data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringNumeric(value='<row-start>', is_numeric=False, embedding_idx=56, is_special=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_embed = StringNumericEmbedding(weather_ds.vocab_len+1, 32)\n",
    "\n",
    "t = my_embed(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7000, 32])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_token: int, d_model: int, n_head: int, d_hid: int,\n",
    "                n_layers: int, dropout: float = 0.15):\n",
    "        super().__init__()\n",
    "        n_token = n_token +1\n",
    "        self.n_token = n_token\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, n_head, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, n_layers)\n",
    "        self.encoder = StringNumericEmbedding(n_token, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, n_token)\n",
    "        self.numeric_decoder = nn.Linear(d_model, n_token)\n",
    "        self.numeric_flattener = nn.Linear(n_token, 1)\n",
    "\n",
    "        # self.numeric_decoder = nn.Linear(d_model)\n",
    "\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # self.numeric_decoder.data.uniform(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        # src_shape = src.shape\n",
    "        # print(f\"raw src_shape: {len(src)}\")\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = torch.unsqueeze(src, dim=1)\n",
    "        # print(f\"encoded src_shape: {src.shape}\")\n",
    "\n",
    "        src_shape = src.shape\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        # print(f\"output_shape: {output.shape}\")\n",
    "        numeric_output = self.numeric_decoder(output)  # .flatten()\n",
    "        numeric_output = torch.squeeze(numeric_output, dim=1)\n",
    "        # numeric_output = torch.mean(numeric_output, [1])\n",
    "        numeric_output = self.numeric_flattener(numeric_output)\n",
    "        # numeric_output = nn.flatten(numeric_output)\n",
    "        output = self.decoder(output)\n",
    "        output = torch.squeeze(output, dim=1)\n",
    "        numeric_output = numeric_output.view(output.shape[0])\n",
    "\n",
    "        # print(f\"output_shape decoded: {output.shape}\")\n",
    "        # output = output.view(-1, self.n_token+1)\n",
    "        # output = output.view(-1, src_shape[0]).T\n",
    "        # print(f\"output_shape view: {output.shape}\")\n",
    "\n",
    "        return output, numeric_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([140, 94])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "bKh3bGPMu7Ww",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increase in memory allocated: 4736512\n",
      "Increase in memory reserved: 0\n"
     ]
    }
   ],
   "source": [
    "n_token = len(weather_ds.vocab)  # size of vocabulary\n",
    "d_model = 32  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "n_layers = 4  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "n_head = 4  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(n_token, d_model, n_head, d_hid, n_layers, dropout).to(device)\n",
    "initial_memory_allocated = torch.cuda.memory_allocated()\n",
    "initial_memory_reserved = torch.cuda.memory_reserved()\n",
    "c,n = model(data)\n",
    "final_memory_allocated = torch.cuda.memory_allocated()\n",
    "final_memory_reserved = torch.cuda.memory_reserved()\n",
    "print(f'Increase in memory allocated: {final_memory_allocated - initial_memory_allocated}')\n",
    "print(f'Increase in memory reserved: {final_memory_reserved - initial_memory_reserved}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([140, 94]), torch.Size([140]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape, n.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzPeL_mMu7Ww"
   },
   "source": [
    "## Run the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "\n",
    "# make_dot((c,n), params=dict(list(model.named_parameters())))# .render(\"rnn_torchviz\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHd3tFaWu7Ww"
   },
   "source": [
    "We use [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)_\n",
    "with the [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)_\n",
    "(stochastic gradient descent) optimizer. The learning rate is initially set to\n",
    "5.0 and follows a [StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)_\n",
    "schedule. During training, we use [nn.utils.clip_grad_norm\\_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)_\n",
    "to prevent gradients from exploding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_loss(class_preds, numeric_preds, raw_data):\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "    mse_loss = nn.MSELoss()\n",
    "    \n",
    "    class_target = torch.tensor([i.embedding_idx for i in raw_data]).to(device)\n",
    "    class_loss = cross_entropy(class_preds, class_target)\n",
    "    \n",
    "    actual_num_idx = torch.tensor([idx for idx, j in enumerate(raw_data) if j.is_numeric]).to(device)\n",
    "    pred_nums = numeric_preds[actual_num_idx]\n",
    "    # print(actual_num_idx.shape)\n",
    "    actual_nums = torch.tensor([i.value for i in raw_data if i.is_numeric]).to(device)\n",
    "    # print(actual_nums.shape)\n",
    "    # print(pred_nums.shape)\n",
    "    reg_loss = mse_loss(pred_nums, actual_nums)\n",
    "    reg_loss_adjuster = 1 # class_loss/reg_loss\n",
    "    \n",
    "    return reg_loss*reg_loss_adjuster+class_loss, {\"reg_loss\": reg_loss,\n",
    "                                                  \"class_loss\": class_loss}\n",
    "\n",
    "# loss, d = custom_loss(c, n, data)\n",
    "# custom_loss(c, n, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "T8PaN3N2u7Wx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "lr = 0.99  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size =100, gamma=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.99, patience=10, threshold=0.001, threshold_mode='rel', cooldown=0, min_lr=0.01, eps=1e-08, verbose=False)\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 50\n",
    "    n_row=50\n",
    "    start_time = time.time()\n",
    "    # src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    for batch, i in enumerate(trange(0, len(weather_ds) - 1, n_row)):\n",
    "        \n",
    "    # for batch, i in enumerate(range(len(weather_ds))):\n",
    "        \n",
    "        data, targets = batch_data(weather_ds, i, n_row=n_row)\n",
    "        class_output, numeric_output = model(data)\n",
    "        loss, loss_dict = custom_loss(class_output, numeric_output, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            # lr = scheduler.get_last_lr()[0]\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            \n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(#f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | ',\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}', loss_dict)\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "            scheduler.step(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77lXi9Cou7Wx"
   },
   "source": [
    "Loop over epochs. Save the model if the validation loss is the best\n",
    "we've seen so far. Adjust the learning rate after each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 51/12714 [00:21<1:27:50,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 432.40 |  loss  1.73 | ppl     5.65 {'reg_loss': tensor(0.2929, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0171, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 101/12714 [00:43<1:35:40,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 430.10 |  loss  0.89 | ppl     2.42 {'reg_loss': tensor(1.4536, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0149, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 151/12714 [01:04<1:28:53,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 426.11 |  loss  0.68 | ppl     1.98 {'reg_loss': tensor(0.7153, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 201/12714 [01:25<1:26:43,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 426.29 |  loss  0.65 | ppl     1.92 {'reg_loss': tensor(0.3370, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0285, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 251/12714 [01:47<1:26:41,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 426.17 |  loss  0.42 | ppl     1.53 {'reg_loss': tensor(0.4447, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0282, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 301/12714 [02:08<1:26:10,  2.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 430.17 |  loss  0.49 | ppl     1.64 {'reg_loss': tensor(0.2231, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 351/12714 [02:30<1:39:42,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 436.01 |  loss  0.34 | ppl     1.41 {'reg_loss': tensor(0.1577, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 401/12714 [02:51<1:27:56,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 429.36 |  loss  0.28 | ppl     1.32 {'reg_loss': tensor(0.4126, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 451/12714 [03:13<1:26:06,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 432.54 |  loss  0.45 | ppl     1.57 {'reg_loss': tensor(0.3846, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0224, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 501/12714 [03:34<1:25:09,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 429.58 |  loss  0.29 | ppl     1.33 {'reg_loss': tensor(0.2243, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 551/12714 [03:56<1:25:13,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 430.19 |  loss  0.28 | ppl     1.32 {'reg_loss': tensor(0.2360, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0341, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 601/12714 [04:18<1:37:16,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 439.92 |  loss  0.36 | ppl     1.43 {'reg_loss': tensor(0.2785, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0192, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 651/12714 [04:39<1:27:13,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 430.22 |  loss  0.30 | ppl     1.36 {'reg_loss': tensor(0.3683, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0107, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 701/12714 [05:01<1:24:22,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 430.23 |  loss  0.31 | ppl     1.36 {'reg_loss': tensor(0.4491, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0391, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 751/12714 [05:23<1:23:38,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 431.19 |  loss  0.28 | ppl     1.33 {'reg_loss': tensor(0.1891, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 801/12714 [05:44<1:23:00,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 430.54 |  loss  0.48 | ppl     1.62 {'reg_loss': tensor(0.2590, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 851/12714 [06:06<1:43:17,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 437.42 |  loss  0.26 | ppl     1.29 {'reg_loss': tensor(0.1374, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0486, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 901/12714 [06:27<1:25:12,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 430.47 |  loss  0.24 | ppl     1.28 {'reg_loss': tensor(0.1626, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 951/12714 [06:49<1:22:48,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 431.13 |  loss  0.67 | ppl     1.95 {'reg_loss': tensor(0.1431, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0154, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1001/12714 [07:11<1:22:18,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 431.32 |  loss  4.80 | ppl   121.66 {'reg_loss': tensor(0.0819, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1051/12714 [07:32<1:21:31,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 430.85 |  loss  0.30 | ppl     1.34 {'reg_loss': tensor(0.2435, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0356, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 1101/12714 [07:54<1:39:08,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 437.27 |  loss  0.42 | ppl     1.52 {'reg_loss': tensor(0.4585, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.1212, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1151/12714 [08:16<1:25:19,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 432.38 |  loss  0.41 | ppl     1.51 {'reg_loss': tensor(0.3673, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0141, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1201/12714 [08:37<1:21:14,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 430.70 |  loss  0.35 | ppl     1.42 {'reg_loss': tensor(0.1703, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0440, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 1251/12714 [08:59<1:20:06,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 0.99 | ms/batch 430.89 |  loss  0.33 | ppl     1.39 {'reg_loss': tensor(0.2437, device='cuda:0', grad_fn=<MseLossBackward0>), 'class_loss': tensor(0.0190, device='cuda:0', grad_fn=<NllLossBackward0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 1256/12714 [09:01<1:20:09,  2.38it/s]"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 140\n"
     ]
    }
   ],
   "source": [
    "data, targets = batch_data(weather_ds, idx=0, n_row=1)\n",
    "print(len(data), len(targets))\n",
    "# data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat, num = model(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([140, 94]), torch.Size([140]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.shape, num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0077, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " {'reg_loss': tensor(2.0211, device='cuda:0', grad_fn=<MseLossBackward0>),\n",
       "  'class_loss': tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward0>)})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_loss(cat, num, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lsm = nn.LogSoftmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([140, 94])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsm(cat).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data, i):\n",
    "    model.eval\n",
    "    with torch.no_grad():\n",
    "        data, targets = batch_data(weather_ds, i, n_row=4)\n",
    "        class_output, numeric_output = model(data)\n",
    "        loss, loss_dict = custom_loss(class_output, numeric_output, targets)\n",
    "        \n",
    "        return data, targets, class_output, numeric_output, loss, loss_dict\n",
    "data, targets, class_output, numeric_output, loss, loss_dict = evaluate(model, weather_ds, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.softmax(class_output, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = nn.LogSoftmax(dim=1)\n",
    "input = torch.randn(2, 3)\n",
    "test_out = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.0576, -4.8689])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_out.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.9935,  0.5413,  0.8352],\n",
       "         [-0.5195, -1.0162,  1.3274]]),\n",
       " tensor([[-2.4737, -0.9389, -0.6450],\n",
       "         [-2.0730, -2.5697, -0.2261]]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, test_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            if seq_len != bptt:\n",
    "                src_mask = src_mask[:seq_len, :seq_len]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, n_token)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "8hksJGvRu7Wy",
    "outputId": "a4115cda-835e-46aa-e4d5-4c24741b24a1"
   },
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zc8hciQ0u7Wy"
   },
   "source": [
    "## Evaluate the best model on the test dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMB7A-pgu7Wy"
   },
   "outputs": [],
   "source": [
    "test_loss = evaluate(model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pVQ0yeGvL9O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0eFqpWpaAbKZ"
   },
   "outputs": [],
   "source": [
    "class StringNumericEmbedding(nn.Embedding):\n",
    "    def __init__(self, string_numeric:StringNumeric, embed_dim: int=28):\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
