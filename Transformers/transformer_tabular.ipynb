{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_At9AOGvu7Wb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mye_8WKbu7We"
   },
   "source": [
    "**bold text**\n",
    "# Language Modeling with ``nn.Transformer`` and torchtext\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "This is a tutorial on training a sequence-to-sequence model that uses the\n",
    "[nn.Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)_ module.\n",
    "\n",
    "The PyTorch 1.2 release includes a standard transformer module based on the\n",
    "paper [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf)_.\n",
    "Compared to Recurrent Neural Networks (RNNs), the transformer model has proven\n",
    "to be superior in quality for many sequence-to-sequence tasks while being more\n",
    "parallelizable. The ``nn.Transformer`` module relies entirely on an attention\n",
    "mechanism (implemented as\n",
    "[nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)_)\n",
    "to draw global dependencies between input and output. The ``nn.Transformer``\n",
    "module is highly modularized such that a single component (e.g.,\n",
    "[nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html)_)\n",
    "can be easily adapted/composed.\n",
    "\n",
    "<img src=\"file://../_static/img/transformer_architecture.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJo8MGMwu7Wf"
   },
   "source": [
    "## Define the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3clWEJ31wImv",
    "outputId": "5e8ef4e8-453a-4bae-957e-10b5454244ac",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture installs\n",
    "%pip install polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0smESe4au7Wh"
   },
   "source": [
    "In this tutorial, we train a ``nn.TransformerEncoder`` model on a\n",
    "language modeling task. The language modeling task is to assign a\n",
    "probability for the likelihood of a given word (or a sequence of words)\n",
    "to follow a sequence of words. A sequence of tokens are passed to the embedding\n",
    "layer first, followed by a positional encoding layer to account for the order\n",
    "of the word (see the next paragraph for more details). The\n",
    "``nn.TransformerEncoder`` consists of multiple layers of\n",
    "[nn.TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)_.\n",
    "Along with the input sequence, a square attention mask is required because the\n",
    "self-attention layers in ``nn.TransformerEncoder`` are only allowed to attend\n",
    "the earlier positions in the sequence. For the language modeling task, any\n",
    "tokens on the future positions should be masked. To produce a probability\n",
    "distribution over output words, the output of the ``nn.TransformerEncoder``\n",
    "model is passed through a linear layer followed by a log-softmax function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o_YarIxhu7Wi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "\n",
    "import re\n",
    "from numbers import Number\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "yqI8ATppxGcw",
    "outputId": "c8069e1b-7e2b-4767-c505-38450eec9810",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "AA0aLnKp7ZUm",
    "outputId": "1dd592ff-9484-49c0-b7a3-4581b458798a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 25)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>x</th><th>y</th><th>station_name</th><th>climate_identifier</th><th>province_code</th><th>local_year</th><th>local_month</th><th>local_day</th><th>local_hour</th><th>temp</th><th>temp_flag</th><th>dew_point_temp</th><th>dew_point_temp_flag</th><th>humidex</th><th>precip_amount</th><th>precip_amount_flag</th><th>relative_humidity</th><th>relative_humidity_flag</th><th>station_pressure</th><th>station_pressure_flag</th><th>wind_chill</th><th>wind_direction</th><th>wind_direction_flag</th><th>wind_speed</th><th>wind_speed_flag</th></tr><tr><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>str</td><td>f64</td><td>str</td></tr></thead><tbody><tr><td>-114.000297</td><td>51.109447</td><td>&quot;CALGARY INT&#x27;L …</td><td>&quot;3031094&quot;</td><td>&quot;AB&quot;</td><td>&quot;2010&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>&quot;0&quot;</td><td>-21.6</td><td>&quot;missing&quot;</td><td>-23.9</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;missing&quot;</td><td>82.0</td><td>&quot;missing&quot;</td><td>89.38</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;M&quot;</td><td>null</td><td>&quot;M&quot;</td></tr><tr><td>-114.000297</td><td>51.109447</td><td>&quot;CALGARY INT&#x27;L …</td><td>&quot;3031094&quot;</td><td>&quot;AB&quot;</td><td>&quot;2010&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>-21.2</td><td>&quot;missing&quot;</td><td>-23.5</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;missing&quot;</td><td>82.0</td><td>&quot;missing&quot;</td><td>89.25</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;M&quot;</td><td>null</td><td>&quot;M&quot;</td></tr><tr><td>-114.000297</td><td>51.109447</td><td>&quot;CALGARY INT&#x27;L …</td><td>&quot;3031094&quot;</td><td>&quot;AB&quot;</td><td>&quot;2010&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>&quot;2&quot;</td><td>-20.8</td><td>&quot;missing&quot;</td><td>-23.0</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;missing&quot;</td><td>82.0</td><td>&quot;missing&quot;</td><td>89.21</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;M&quot;</td><td>null</td><td>&quot;M&quot;</td></tr><tr><td>-114.000297</td><td>51.109447</td><td>&quot;CALGARY INT&#x27;L …</td><td>&quot;3031094&quot;</td><td>&quot;AB&quot;</td><td>&quot;2010&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>&quot;3&quot;</td><td>-20.4</td><td>&quot;missing&quot;</td><td>-22.6</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;missing&quot;</td><td>83.0</td><td>&quot;missing&quot;</td><td>89.12</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;M&quot;</td><td>null</td><td>&quot;M&quot;</td></tr><tr><td>-114.000297</td><td>51.109447</td><td>&quot;CALGARY INT&#x27;L …</td><td>&quot;3031094&quot;</td><td>&quot;AB&quot;</td><td>&quot;2010&quot;</td><td>&quot;1&quot;</td><td>&quot;1&quot;</td><td>&quot;4&quot;</td><td>-20.4</td><td>&quot;missing&quot;</td><td>-22.7</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;missing&quot;</td><td>82.0</td><td>&quot;missing&quot;</td><td>89.04</td><td>&quot;missing&quot;</td><td>null</td><td>null</td><td>&quot;M&quot;</td><td>null</td><td>&quot;M&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 25)\n",
       "┌─────────┬────────┬────────────┬────────────┬───┬────────────┬────────────┬──────────┬────────────┐\n",
       "│ x       ┆ y      ┆ station_na ┆ climate_id ┆ … ┆ wind_direc ┆ wind_direc ┆ wind_spe ┆ wind_speed │\n",
       "│ ---     ┆ ---    ┆ me         ┆ entifier   ┆   ┆ tion       ┆ tion_flag  ┆ ed       ┆ _flag      │\n",
       "│ f64     ┆ f64    ┆ ---        ┆ ---        ┆   ┆ ---        ┆ ---        ┆ ---      ┆ ---        │\n",
       "│         ┆        ┆ str        ┆ str        ┆   ┆ f64        ┆ str        ┆ f64      ┆ str        │\n",
       "╞═════════╪════════╪════════════╪════════════╪═══╪════════════╪════════════╪══════════╪════════════╡\n",
       "│ -114.00 ┆ 51.109 ┆ CALGARY    ┆ 3031094    ┆ … ┆ null       ┆ M          ┆ null     ┆ M          │\n",
       "│ 0297    ┆ 447    ┆ INT'L CS   ┆            ┆   ┆            ┆            ┆          ┆            │\n",
       "│ -114.00 ┆ 51.109 ┆ CALGARY    ┆ 3031094    ┆ … ┆ null       ┆ M          ┆ null     ┆ M          │\n",
       "│ 0297    ┆ 447    ┆ INT'L CS   ┆            ┆   ┆            ┆            ┆          ┆            │\n",
       "│ -114.00 ┆ 51.109 ┆ CALGARY    ┆ 3031094    ┆ … ┆ null       ┆ M          ┆ null     ┆ M          │\n",
       "│ 0297    ┆ 447    ┆ INT'L CS   ┆            ┆   ┆            ┆            ┆          ┆            │\n",
       "│ -114.00 ┆ 51.109 ┆ CALGARY    ┆ 3031094    ┆ … ┆ null       ┆ M          ┆ null     ┆ M          │\n",
       "│ 0297    ┆ 447    ┆ INT'L CS   ┆            ┆   ┆            ┆            ┆          ┆            │\n",
       "│ -114.00 ┆ 51.109 ┆ CALGARY    ┆ 3031094    ┆ … ┆ null       ┆ M          ┆ null     ┆ M          │\n",
       "│ 0297    ┆ 447    ┆ INT'L CS   ┆            ┆   ┆            ┆            ┆          ┆            │\n",
       "└─────────┴────────┴────────────┴────────────┴───┴────────────┴────────────┴──────────┴────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = pl.read_parquet(\"~/Hephaestus/data/weather_clean.parquet\")\n",
    "weather.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzAjejIIu7Wj"
   },
   "source": [
    "``PositionalEncoding`` module injects some information about the\n",
    "relative or absolute position of the tokens in the sequence. The\n",
    "positional encodings have the same dimension as the embeddings so that\n",
    "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
    "different frequencies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VJu-X9OS8m3N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_numeric(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == pl.Float64 or df[col].dtype == pl.Int64:\n",
    "            df = df.with_columns(\n",
    "                ((pl.col(col) - pl.col(col).mean()) / pl.col(col).std()).alias(col)\n",
    "            )  # .select(pl.col([\"dew_point_temp\", \"NewCOL\"]))\n",
    "    return df\n",
    "\n",
    "\n",
    "weather = scale_numeric(weather)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JZFqyv5D8o31",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_lower_remove_special_chars(df):\n",
    "    df = df.with_columns(\n",
    "        pl.col(pl.Utf8).str.to_lowercase().str.replace_all(\"[^a-zA-Z0-9]\", \" \")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "weather = make_lower_remove_special_chars(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uzf8dqGa8qyi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_unique_utf8_values(df):\n",
    "    arr = np.array([])\n",
    "    for col in df.select(pl.col(pl.Utf8)).columns:\n",
    "        arr = np.append(arr, df[col].unique().to_numpy())\n",
    "\n",
    "    return np.unique(arr)\n",
    "\n",
    "\n",
    "weather_val_tokens = get_unique_utf8_values(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "qx2sWqAA8t64",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_col_tokens(df):\n",
    "    tokens = []\n",
    "    for col_name in df.columns:\n",
    "        sub_strs = re.split(r\"[^a-zA-Z0-9]\", col_name)\n",
    "        tokens.extend(sub_strs)\n",
    "    return np.unique(np.array(tokens))\n",
    "\n",
    "\n",
    "weather_col_tokens = get_col_tokens(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZgAXkAI8wKq",
    "outputId": "da13b04e-67e3-42f3-b563-321df8fa4683",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([',', '0', '1', '10', '11', '12', '13', '14', '15', '16', '17',\n",
       "       '18', '19', '2', '20', '2010', '2011', '2012', '2013', '2014',\n",
       "       '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022',\n",
       "       '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30',\n",
       "       '3012206', '3026knq', '3031094', '3033890', '3035208', '3062696',\n",
       "       '31', '4', '5', '6', '7', '8', '9', ':', '<mask><pad>',\n",
       "       '<row-end>', '<row-start>', '<unk>', 'ab', 'amount',\n",
       "       'calgary int l cs', 'chill', 'climate', 'code', 'day', 'dew',\n",
       "       'direction', 'edmonton international cs', 'flag',\n",
       "       'fort mcmurray cs', 'hour', 'humidex', 'humidity', 'identifier',\n",
       "       'lethbridge cda', 'local', 'm', 'missing', 'month', 'name',\n",
       "       'pincher creek climate', 'point', 'precip', 'pressure', 'province',\n",
       "       'relative', 'speed', 'station', 'sundre a', 'temp', 'wind', 'x',\n",
       "       'y', 'year'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = np.array(\n",
    "    [\n",
    "        \"missing\",\n",
    "        \"<mask>\"\n",
    "        \"<pad>\",\n",
    "        \"<unk>\",\n",
    "        \":\",\n",
    "        \",\",\n",
    "        \"<row-start>\",\n",
    "        \"<row-end>\",\n",
    "    ]\n",
    ")\n",
    "tokens = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            weather_val_tokens,\n",
    "            weather_col_tokens,\n",
    "            special_tokens,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ot6vmpOS9mbH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_token: int, d_model: int, n_head: int, d_hid: int,\n",
    "                n_layers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, n_head, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, n_layers)\n",
    "        self.encoder = StringNumericEmbedding(n_token, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.decoder = nn.Linear(d_model, n_token)\n",
    "        self.numeric_decoder = nn.Linear(d_model, n_token)\n",
    "        # self.numeric_decoder = nn.Linear(d_model)\n",
    "\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        # self.numeric_decoder.data.uniform(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]``\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        \n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        print(src.shape)\n",
    "        print(\"entering pos encoder\")\n",
    "        src = self.pos_encoder(src)\n",
    "        print(\"Entering transformer encoder\")\n",
    "        output = self.transformer_encoder(src)\n",
    "        print(\"Entering numeric decoder\")\n",
    "        numeric_output = self.numeric_decoder(output).flatten()\n",
    "        # numeric_output = nn.flatten(numeric_output)\n",
    "        print(\"Entering decoder\")\n",
    "        output = self.decoder(output)\n",
    "\n",
    "        return output, numeric_output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of ``-inf``, with zeros on ``diag``.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Y5tWWWk83DM",
    "outputId": "56f7fe30-06e7-4bc6-9df7-783e050baff9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringNumeric(value='climate', is_numeric=False, embedding_idx=None, is_special=False)\n",
      "StringNumeric(value=1.0, is_numeric=True, embedding_idx=0, is_special=False)\n",
      "StringNumeric(value='SomeRandomString', is_numeric=False, embedding_idx=None, is_special=False)\n",
      "StringNumeric(value='climate', is_numeric=False, embedding_idx=62, is_special=False)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class StringNumeric:\n",
    "    value: Union[str, float]\n",
    "    # all_tokens: np.array\n",
    "    is_numeric: bool = field(default=None, repr=True)\n",
    "    embedding_idx: int = field(default=None, repr=True)\n",
    "    is_special: bool = field(default=False, repr=True)\n",
    "    def __post_init__(self):\n",
    "        if isinstance(self.value, str):\n",
    "            self.is_numeric = False\n",
    "        else:\n",
    "            self.is_numeric = True\n",
    "            self.embedding_idx = 0\n",
    "\n",
    "    def gen_embed_idx(self, tokens: np.array, special_tokens: np.array):\n",
    "        if not self.is_numeric:\n",
    "            try:\n",
    "                self.embedding_idx = np.where(tokens == self.value)[0][0] + 1\n",
    "            except IndexError:\n",
    "                self.embedding_idx = np.where(tokens == \"<unk>\")[0][0] + 1\n",
    "            if self.value in special_tokens:\n",
    "                self.is_special = True\n",
    "\n",
    "\n",
    "\n",
    "x = StringNumeric(value=\"climate\")\n",
    "# xx = StringNumeric(value=\"climate\", tokens=tokens)\n",
    "print(x)\n",
    "y = StringNumeric(value=1.0)\n",
    "print(y)\n",
    "z = StringNumeric(value=\"SomeRandomString\")\n",
    "print(z)\n",
    "x.gen_embed_idx(tokens, special_tokens)\n",
    "print(x)\n",
    "# print(StringNumeric(value=1.0, all_tokens=tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ETGBMjYt858a",
    "outputId": "b47e0ea3-e60e-4551-c9a8-54168d2e84f9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    }
   ],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    # def __init__(self, df: pl.DataFrame, vocab_dict: Dict, m_dim: int) -> Dataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pl.DataFrame,\n",
    "        vocab,\n",
    "        special_tokens: np.array,\n",
    "        shuffle_cols=False,\n",
    "        max_row_length=512,\n",
    "    ) -> Dataset:\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.special_tokens = special_tokens\n",
    "        self.vocab_len = vocab.shape[0]\n",
    "        self.shuffle_cols = shuffle_cols\n",
    "        self.max_row_length = max_row_length\n",
    "        # self.vocab_dict = vocab_dict\n",
    "        # self.embedding = nn.Embedding(len(self.string_vocab), m_dim)\n",
    "        # Numeric Scale\n",
    "\n",
    "        # self.col_vocab = self.df.columns\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of sequences in the dataset.\"\"\"\n",
    "        length = self.df.shape[0]\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns a tuple of (input, target) at the given index.\"\"\"\n",
    "        row = self.df[idx]\n",
    "        row = self.splitter(row)\n",
    "        return row\n",
    "\n",
    "\n",
    "\n",
    "    def splitter(self, row: pl.DataFrame) -> List[Union[str, float, None]]:\n",
    "        vals = [\"<row-start>\"]\n",
    "        cols = row.columns\n",
    "        if self.shuffle_cols:\n",
    "            np.random.shuffle(cols)\n",
    "\n",
    "        for col in cols:\n",
    "            value = row[col][0]\n",
    "            col = col.split(\"_\")\n",
    "            vals.extend(col)\n",
    "            vals.append(\":\")\n",
    "            if isinstance(value, Number):\n",
    "                vals.append(value)\n",
    "            elif value is None:\n",
    "                vals.append(\"missing\")\n",
    "                # Nones are only for numeric columns, others are \"None\"\n",
    "            elif isinstance(value, str):\n",
    "                vals.extend(value.split(\" \"))\n",
    "            else:\n",
    "                raise ValueError(\"Unknown type\")\n",
    "            vals.append(\",\")\n",
    "        vals.append(\"<row-end>\")\n",
    "\n",
    "        val_len = len(vals)\n",
    "        if val_len < self.max_row_length:\n",
    "            diff = self.max_row_length - val_len\n",
    "            vals.extend([\"<pad>\"] * diff)\n",
    "        elif val_len > self.max_row_length:\n",
    "            vals = vals[:self.max_row_length - 1]\n",
    "            # add warning\n",
    "\n",
    "            vals = np.append(vals, [\"<row-end>\"])\n",
    "            print(\"Row too long, truncating\")\n",
    "            Warning(\"Row too long, truncating\")\n",
    "        vals = [StringNumeric(value=val) for val in vals]\n",
    "        for val in vals:\n",
    "            val.gen_embed_idx(self.vocab, self.special_tokens)\n",
    "\n",
    "        return vals\n",
    "\n",
    "\n",
    "weather_ds = TabularDataset(weather, tokens, special_tokens=special_tokens, shuffle_cols=False, max_row_length=140 )\n",
    "\n",
    "print(len(weather_ds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "k4v20RSDE_L-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StringNumericEmbedding(nn.Module):\n",
    "    def __init__(self, n_token: int, d_model: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(n_token+1, d_model, padding_idx=0).to(device)\n",
    "\n",
    "    def forward(self, input: StringNumeric):\n",
    "        embedding_index = torch.tensor([i.embedding_idx for i in input]).to(device)\n",
    "        embed = self.embedding(embedding_index)\n",
    "        with torch.no_grad():\n",
    "            for idx, value in enumerate(input):\n",
    "                if value.is_numeric:\n",
    "                    embed[idx][0] = value.value\n",
    "        return embed\n",
    "\n",
    "# my_embed = StringNumericEmbedding(200, weather_ds.vocab_len+1)\n",
    "\n",
    "# t = my_embed(weather_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringNumeric(value='<row-start>', is_numeric=False, embedding_idx=56, is_special=True),\n",
       " StringNumeric(value='x', is_numeric=False, embedding_idx=91, is_special=False),\n",
       " StringNumeric(value=':', is_numeric=False, embedding_idx=53, is_special=True),\n",
       " StringNumeric(value=-0.551099305737714, is_numeric=True, embedding_idx=0, is_special=False),\n",
       " StringNumeric(value=',', is_numeric=False, embedding_idx=1, is_special=True),\n",
       " StringNumeric(value='y', is_numeric=False, embedding_idx=92, is_special=False),\n",
       " StringNumeric(value=':', is_numeric=False, embedding_idx=53, is_special=True),\n",
       " StringNumeric(value=-0.37817406811183396, is_numeric=True, embedding_idx=0, is_special=False),\n",
       " StringNumeric(value=',', is_numeric=False, embedding_idx=1, is_special=True),\n",
       " StringNumeric(value='station', is_numeric=False, embedding_idx=87, is_special=False)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mask_row(row):\n",
    "    prob = 0.15\n",
    "    for idx, val in enumerate(row):\n",
    "        if val.is_special:\n",
    "            continue\n",
    "        if np.random.rand() < prob:\n",
    "            val = StringNumeric(value=\"<mask>\")\n",
    "            val.gen_embed_idx(tokens, special_tokens)\n",
    "            row[idx] = val\n",
    "    return row\n",
    "\n",
    "x = mask_row(weather_ds[0])\n",
    "x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='calgary', is_numeric=False, embedding_idx=57, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='temp', is_numeric=False, embedding_idx=89, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='flag', is_numeric=False, embedding_idx=68, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='wind', is_numeric=False, embedding_idx=90, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='m', is_numeric=False, embedding_idx=76, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='speed', is_numeric=False, embedding_idx=86, is_special=False)\n",
      "StringNumeric(value='<mask>', is_numeric=False, embedding_idx=57, is_special=False) != StringNumeric(value='<pad>', is_numeric=False, embedding_idx=57, is_special=False)\n"
     ]
    }
   ],
   "source": [
    "for idx, val in enumerate(x):\n",
    "    if val == weather_ds[0][idx]:\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"{val} != {weather_ds[0][idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGFJV7L6u7Wl"
   },
   "source": [
    "## Load and batch data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_data(ds, idx: int):\n",
    "    n_rows = 3\n",
    "    target = []\n",
    "    for i in range(idx, idx+n_rows):\n",
    "        target.extend(ds[i])\n",
    "    print(type(target))\n",
    "    print(len(target))\n",
    "    batch = mask_row(target)\n",
    "\n",
    "    return batch, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "G0Omb9XMeRkZ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_built():\n",
    "    device_name = 'mps'\n",
    "elif torch.cuda.is_available():\n",
    "    device_name = 'cuda'\n",
    "else:\n",
    "    device_name = 'cpu'\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d37K28qhu7Wv"
   },
   "source": [
    "The model hyperparameters are defined below. The ``vocab`` size is\n",
    "equal to the length of the vocab object.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "420\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data, targets = batch_data(weather_ds, 0)\n",
    "# data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "bKh3bGPMu7Ww",
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_token = len(weather_ds.vocab)  # size of vocabulary\n",
    "d_model = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "n_layers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "n_head = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(n_token, d_model, n_head, d_hid, n_layers, dropout).to(device)\n",
    "# c,n = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# n.shape, c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzPeL_mMu7Ww"
   },
   "source": [
    "## Run the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHd3tFaWu7Ww"
   },
   "source": [
    "We use [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)_\n",
    "with the [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)_\n",
    "(stochastic gradient descent) optimizer. The learning rate is initially set to\n",
    "5.0 and follows a [StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html)_\n",
    "schedule. During training, we use [nn.utils.clip_grad_norm\\_](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)_\n",
    "to prevent gradients from exploding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i.embedding_idx for i in targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "T8PaN3N2u7Wx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "def custom_loss(class_preds, numeric_preds, raw_data):\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "    mse_loss = nn.MSELoss()\n",
    "    class_target = torch.tensor([i.embedding_idx for i in raw_data])\n",
    "    class_loss = cross_entropy(class_preds, class_target)\n",
    "    \n",
    "    actual_num_idx = nn.Tensor([idx for idx, j in enumerate(raw_data) if j.is_numeric])\n",
    "    pred_nums = numeric_preds[actual_num_idx]\n",
    "    actual_nums = torch.tensor([i.value for i in raw_data if i.is_numeric])\n",
    "    reg_loss = mse_loss(pred_nums, actual_nums)\n",
    "\n",
    "    return reg_loss + class_loss\n",
    "\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    # src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    for batch, i in enumerate(range(0, len(weather_ds) - 1, 2)):\n",
    "    # for batch, i in enumerate(range(len(weather_ds))):\n",
    "        \n",
    "        data, targets = batch_data(weather_ds, i)\n",
    "        class_output, numeric_output = model(data)\n",
    "        flat_class_output = class_output.view(-1, n_token)\n",
    "        flat_numeric_output = numeric_output.view(-1, n_token)\n",
    "        loss = custom_loss(flat_class_output, flat_numeric_output, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            seq_len = data.size(0)\n",
    "            if seq_len != bptt:\n",
    "                src_mask = src_mask[:seq_len, :seq_len]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, n_token)\n",
    "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77lXi9Cou7Wx"
   },
   "source": [
    "Loop over epochs. Save the model if the validation loss is the best\n",
    "we've seen so far. Adjust the learning rate after each epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "420\n",
      "torch.Size([420, 200])\n",
      "entering pos encoder\n",
      "Entering transformer encoder\n",
      "Entering numeric decoder\n",
      "Entering decoder\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (176400) to match target batch_size (420).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     33\u001b[0m flat_class_output \u001b[38;5;241m=\u001b[39m class_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_token)\n\u001b[1;32m     34\u001b[0m flat_numeric_output \u001b[38;5;241m=\u001b[39m numeric_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_token)\n\u001b[0;32m---> 35\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_class_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_numeric_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     38\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[36], line 9\u001b[0m, in \u001b[0;36mcustom_loss\u001b[0;34m(class_preds, numeric_preds, raw_data)\u001b[0m\n\u001b[1;32m      7\u001b[0m mse_loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m      8\u001b[0m class_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([i\u001b[38;5;241m.\u001b[39membedding_idx \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m raw_data])\n\u001b[0;32m----> 9\u001b[0m class_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m actual_num_idx \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mTensor([idx \u001b[38;5;28;01mfor\u001b[39;00m idx, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(raw_data) \u001b[38;5;28;01mif\u001b[39;00m j\u001b[38;5;241m.\u001b[39mis_numeric])\n\u001b[1;32m     12\u001b[0m pred_nums \u001b[38;5;241m=\u001b[39m numeric_preds[actual_num_idx]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (176400) to match target batch_size (420)."
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "8hksJGvRu7Wy",
    "outputId": "a4115cda-835e-46aa-e4d5-4c24741b24a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 824.53 | loss  8.22 | ppl  3723.21\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 800.76 | loss  6.93 | ppl  1020.40\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 850.47 | loss  6.47 | ppl   642.27\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 823.83 | loss  6.31 | ppl   552.42\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 820.16 | loss  6.20 | ppl   495.21\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 823.57 | loss  6.17 | ppl   476.97\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 879.57 | loss  6.12 | ppl   455.25\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 831.05 | loss  6.11 | ppl   450.69\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 838.54 | loss  6.03 | ppl   415.73\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d715ea9cf758>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mval_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-7311c15e56c1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model)\n",
    "        val_loss = evaluate(model, val_data)\n",
    "        val_ppl = math.exp(val_loss)\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path)) # load best model states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zc8hciQ0u7Wy"
   },
   "source": [
    "## Evaluate the best model on the test dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMB7A-pgu7Wy"
   },
   "outputs": [],
   "source": [
    "test_loss = evaluate(model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pVQ0yeGvL9O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0eFqpWpaAbKZ"
   },
   "outputs": [],
   "source": [
    "class StringNumericEmbedding(nn.Embedding):\n",
    "    def __init__(self, string_numeric:StringNumeric, embed_dim: int=28):\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
