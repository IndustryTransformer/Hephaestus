{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diamond Transformer\n",
    "\n",
    "This notebook investigates the use of a transformer to predict the price of diamonds. The dataset is from Kaggle and can be found [here](https://www.kaggle.com/shivam2503/diamonds).\n",
    "\n",
    "While many traditional ML and DL techniques work on the dataset, our approach uses far less labeled data while achieving similar results. This is done by using a transformer.\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "## Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import math\n",
    "from datetime import datetime as dt\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import hephaestus as hp\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diamonds dataset: df = pl.read_csv(\"../data/diamonds.csv\") df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>num1</th><th>num2</th><th>operation</th><th>result</th></tr><tr><td>f64</td><td>f64</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>10.0</td><td>2.402908</td><td>&quot;multiply&quot;</td><td>24.029081</td></tr><tr><td>3.990972</td><td>7.0</td><td>&quot;add&quot;</td><td>10.990972</td></tr><tr><td>10.0</td><td>1.951676</td><td>&quot;divide&quot;</td><td>5.123802</td></tr><tr><td>10.0</td><td>9.0</td><td>&quot;subtract&quot;</td><td>1.0</td></tr><tr><td>1.30749</td><td>9.023088</td><td>&quot;divide&quot;</td><td>0.144905</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌──────────┬──────────┬───────────┬───────────┐\n",
       "│ num1     ┆ num2     ┆ operation ┆ result    │\n",
       "│ ---      ┆ ---      ┆ ---       ┆ ---       │\n",
       "│ f64      ┆ f64      ┆ str       ┆ f64       │\n",
       "╞══════════╪══════════╪═══════════╪═══════════╡\n",
       "│ 10.0     ┆ 2.402908 ┆ multiply  ┆ 24.029081 │\n",
       "│ 3.990972 ┆ 7.0      ┆ add       ┆ 10.990972 │\n",
       "│ 10.0     ┆ 1.951676 ┆ divide    ┆ 5.123802  │\n",
       "│ 10.0     ┆ 9.0      ┆ subtract  ┆ 1.0       │\n",
       "│ 1.30749  ┆ 9.023088 ┆ divide    ┆ 0.144905  │\n",
       "└──────────┴──────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function to generate a random number, either integer or float\n",
    "def generate_random_number():\n",
    "    return random.randint(1, 10) if random.random() < 0.5 else random.uniform(1, 10)\n",
    "\n",
    "\n",
    "# Number of rows in the DataFrame\n",
    "n_rows = 100_000\n",
    "\n",
    "# Generate random values for num1 and num2 (both integers and floats)\n",
    "num1 = [generate_random_number() for _ in range(n_rows)]\n",
    "num2 = [generate_random_number() for _ in range(n_rows)]\n",
    "\n",
    "# Randomly choose an operation for each row\n",
    "operations = [\n",
    "    random.choice([\"multiply\", \"divide\", \"add\", \"subtract\"]) for _ in range(n_rows)\n",
    "]\n",
    "\n",
    "# Create a DataFrame with the columns num1, num2, and operation\n",
    "df = pl.DataFrame({\"num1\": num1, \"num2\": num2, \"operation\": operations})\n",
    "\n",
    "# Apply the operation to each row to create the result column\n",
    "df = df.with_columns(\n",
    "    pl.when(pl.col(\"operation\") == \"multiply\")\n",
    "    .then(pl.col(\"num1\") * pl.col(\"num2\"))\n",
    "    .when(pl.col(\"operation\") == \"divide\")\n",
    "    .then(pl.col(\"num1\") / pl.col(\"num2\"))\n",
    "    .when(pl.col(\"operation\") == \"add\")\n",
    "    .then(pl.col(\"num1\") + pl.col(\"num2\"))\n",
    "    .when(pl.col(\"operation\") == \"subtract\")\n",
    "    .then(pl.col(\"num1\") - pl.col(\"num2\"))\n",
    "    .otherwise(None)\n",
    "    .alias(\"result\")\n",
    ")\n",
    "\n",
    "\n",
    "# Print the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>describe</th><th>num1</th><th>num2</th><th>operation</th><th>result</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>100000.0</td><td>100000.0</td><td>&quot;100000&quot;</td><td>100000.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>0.0</td><td>&quot;0&quot;</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>5.494172</td><td>5.507518</td><td>null</td><td>10.706492</td></tr><tr><td>&quot;std&quot;</td><td>2.731547</td><td>2.738743</td><td>null</td><td>16.78496</td></tr><tr><td>&quot;min&quot;</td><td>1.0</td><td>1.0</td><td>&quot;add&quot;</td><td>-9.0</td></tr><tr><td>&quot;max&quot;</td><td>10.0</td><td>10.0</td><td>&quot;subtract&quot;</td><td>100.0</td></tr><tr><td>&quot;median&quot;</td><td>5.462981</td><td>5.541014</td><td>null</td><td>5.025067</td></tr><tr><td>&quot;25%&quot;</td><td>3.0</td><td>3.0</td><td>null</td><td>0.864027</td></tr><tr><td>&quot;75%&quot;</td><td>8.0</td><td>8.0</td><td>null</td><td>13.308779</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 5)\n",
       "┌────────────┬──────────┬──────────┬───────────┬───────────┐\n",
       "│ describe   ┆ num1     ┆ num2     ┆ operation ┆ result    │\n",
       "│ ---        ┆ ---      ┆ ---      ┆ ---       ┆ ---       │\n",
       "│ str        ┆ f64      ┆ f64      ┆ str       ┆ f64       │\n",
       "╞════════════╪══════════╪══════════╪═══════════╪═══════════╡\n",
       "│ count      ┆ 100000.0 ┆ 100000.0 ┆ 100000    ┆ 100000.0  │\n",
       "│ null_count ┆ 0.0      ┆ 0.0      ┆ 0         ┆ 0.0       │\n",
       "│ mean       ┆ 5.494172 ┆ 5.507518 ┆ null      ┆ 10.706492 │\n",
       "│ std        ┆ 2.731547 ┆ 2.738743 ┆ null      ┆ 16.78496  │\n",
       "│ min        ┆ 1.0      ┆ 1.0      ┆ add       ┆ -9.0      │\n",
       "│ max        ┆ 10.0     ┆ 10.0     ┆ subtract  ┆ 100.0     │\n",
       "│ median     ┆ 5.462981 ┆ 5.541014 ┆ null      ┆ 5.025067  │\n",
       "│ 25%        ┆ 3.0      ┆ 3.0      ┆ null      ┆ 0.864027  │\n",
       "│ 75%        ┆ 8.0      ┆ 8.0      ┆ null      ┆ 13.308779 │\n",
       "└────────────┴──────────┴──────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>num1</th><th>num2</th><th>operation</th><th>result</th></tr><tr><td>f64</td><td>f64</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>1.649551</td><td>-1.133589</td><td>&quot;multiply&quot;</td><td>0.793722</td></tr><tr><td>-0.550311</td><td>0.544952</td><td>&quot;add&quot;</td><td>0.016948</td></tr><tr><td>1.649551</td><td>-1.298348</td><td>&quot;divide&quot;</td><td>-0.332601</td></tr><tr><td>1.649551</td><td>1.275214</td><td>&quot;subtract&quot;</td><td>-0.578285</td></tr><tr><td>-1.532715</td><td>1.283644</td><td>&quot;divide&quot;</td><td>-0.629229</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌───────────┬───────────┬───────────┬───────────┐\n",
       "│ num1      ┆ num2      ┆ operation ┆ result    │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│ f64       ┆ f64       ┆ str       ┆ f64       │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 1.649551  ┆ -1.133589 ┆ multiply  ┆ 0.793722  │\n",
       "│ -0.550311 ┆ 0.544952  ┆ add       ┆ 0.016948  │\n",
       "│ 1.649551  ┆ -1.298348 ┆ divide    ┆ -0.332601 │\n",
       "│ 1.649551  ┆ 1.275214  ┆ subtract  ┆ -0.578285 │\n",
       "│ -1.532715 ┆ 1.283644  ┆ divide    ┆ -0.629229 │\n",
       "└───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = hp.scale_numeric(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = hp.make_lower_remove_special_chars(df)\n",
    "val_tokens = hp.get_unique_utf8_values(df)\n",
    "col_tokens = hp.get_col_tokens(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>num1</th><th>num2</th><th>operation</th><th>result</th></tr><tr><td>f64</td><td>f64</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>1.649551</td><td>-1.133589</td><td>&quot;multiply&quot;</td><td>0.793722</td></tr><tr><td>-0.550311</td><td>0.544952</td><td>&quot;add&quot;</td><td>0.016948</td></tr><tr><td>1.649551</td><td>-1.298348</td><td>&quot;divide&quot;</td><td>-0.332601</td></tr><tr><td>1.649551</td><td>1.275214</td><td>&quot;subtract&quot;</td><td>-0.578285</td></tr><tr><td>-1.532715</td><td>1.283644</td><td>&quot;divide&quot;</td><td>-0.629229</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌───────────┬───────────┬───────────┬───────────┐\n",
       "│ num1      ┆ num2      ┆ operation ┆ result    │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│ f64       ┆ f64       ┆ str       ┆ f64       │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 1.649551  ┆ -1.133589 ┆ multiply  ┆ 0.793722  │\n",
       "│ -0.550311 ┆ 0.544952  ┆ add       ┆ 0.016948  │\n",
       "│ 1.649551  ┆ -1.298348 ┆ divide    ┆ -0.332601 │\n",
       "│ 1.649551  ┆ 1.275214  ┆ subtract  ┆ -0.578285 │\n",
       "│ -1.532715 ┆ 1.283644  ┆ divide    ┆ -0.629229 │\n",
       "└───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = np.array(\n",
    "    [\n",
    "        \"missing\",\n",
    "        \"<mask>\",\n",
    "        \"<numeric_mask>\",\n",
    "        \"<pad>\",\n",
    "        \"<unk>\",\n",
    "        \":\",\n",
    "        \",\",\n",
    "        \"<row-start>\",\n",
    "        \"<row-end>\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([',', ':', '<mask>', '<numeric>', '<numeric_mask>', '<pad>',\n",
       "       '<row-end>', '<row-start>', '<unk>', 'add', 'divide', 'missing',\n",
       "       'multiply', 'num1', 'num2', 'operation', 'result', 'subtract'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = np.unique(\n",
    "    np.concatenate(\n",
    "        (\n",
    "            val_tokens,\n",
    "            col_tokens,\n",
    "            special_tokens,\n",
    "            [\n",
    "                \"<numeric>\",\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    ")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split\n",
    "\n",
    "To show the actual model performance out of sample we split the data into a training and test set. The training set will be used to train the model and the test set will be used to evaluate the model performance. We will use 80% of the data for training and 20% for testing.\n",
    "\n",
    "We also remove the price column from the training and test sets and will only use a tiny subset of the data to simulate an industrial process with lots of input data but expensive and limited labeled data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>num1</th><th>num2</th><th>operation</th><th>result</th></tr><tr><td>f64</td><td>f64</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>1.649551</td><td>-1.133589</td><td>&quot;multiply&quot;</td><td>0.793722</td></tr><tr><td>-0.550311</td><td>0.544952</td><td>&quot;add&quot;</td><td>0.016948</td></tr><tr><td>1.649551</td><td>-1.298348</td><td>&quot;divide&quot;</td><td>-0.332601</td></tr><tr><td>1.649551</td><td>1.275214</td><td>&quot;subtract&quot;</td><td>-0.578285</td></tr><tr><td>-1.532715</td><td>1.283644</td><td>&quot;divide&quot;</td><td>-0.629229</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌───────────┬───────────┬───────────┬───────────┐\n",
       "│ num1      ┆ num2      ┆ operation ┆ result    │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│ f64       ┆ f64       ┆ str       ┆ f64       │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 1.649551  ┆ -1.133589 ┆ multiply  ┆ 0.793722  │\n",
       "│ -0.550311 ┆ 0.544952  ┆ add       ┆ 0.016948  │\n",
       "│ 1.649551  ┆ -1.298348 ┆ divide    ┆ -0.332601 │\n",
       "│ 1.649551  ┆ 1.275214  ┆ subtract  ┆ -0.578285 │\n",
       "│ -1.532715 ┆ 1.283644  ┆ divide    ┆ -0.629229 │\n",
       "└───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle for randomness\n",
    "df = df.sample(fraction=1.0, seed=42)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fraction = 0.8\n",
    "n_train = int(train_fraction * len(df))\n",
    "train_test_df = df.select(pl.all())\n",
    "\n",
    "train, test = train_test_df.head(n_train), train_test_df.tail(\n",
    "    len(train_test_df) - n_train\n",
    ")\n",
    "\n",
    "labeled_train, labeled_test = df.head(n_train), df.tail(len(df) - n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(shape: (5, 4)\n",
       " ┌───────────┬───────────┬───────────┬───────────┐\n",
       " │ num1      ┆ num2      ┆ operation ┆ result    │\n",
       " │ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       " │ f64       ┆ f64       ┆ str       ┆ f64       │\n",
       " ╞═══════════╪═══════════╪═══════════╪═══════════╡\n",
       " │ 1.649551  ┆ -1.133589 ┆ multiply  ┆ 0.793722  │\n",
       " │ -0.550311 ┆ 0.544952  ┆ add       ┆ 0.016948  │\n",
       " │ 1.649551  ┆ -1.298348 ┆ divide    ┆ -0.332601 │\n",
       " │ 1.649551  ┆ 1.275214  ┆ subtract  ┆ -0.578285 │\n",
       " │ -1.532715 ┆ 1.283644  ┆ divide    ┆ -0.629229 │\n",
       " └───────────┴───────────┴───────────┴───────────┘,\n",
       " (80000, 4))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(), train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>num1</th><th>num2</th><th>operation</th><th>result</th></tr><tr><td>f64</td><td>f64</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>1.649551</td><td>-1.133589</td><td>&quot;multiply&quot;</td><td>0.793722</td></tr><tr><td>-0.550311</td><td>0.544952</td><td>&quot;add&quot;</td><td>0.016948</td></tr><tr><td>1.649551</td><td>-1.298348</td><td>&quot;divide&quot;</td><td>-0.332601</td></tr><tr><td>1.649551</td><td>1.275214</td><td>&quot;subtract&quot;</td><td>-0.578285</td></tr><tr><td>-1.532715</td><td>1.283644</td><td>&quot;divide&quot;</td><td>-0.629229</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 4)\n",
       "┌───────────┬───────────┬───────────┬───────────┐\n",
       "│ num1      ┆ num2      ┆ operation ┆ result    │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│ f64       ┆ f64       ┆ str       ┆ f64       │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 1.649551  ┆ -1.133589 ┆ multiply  ┆ 0.793722  │\n",
       "│ -0.550311 ┆ 0.544952  ┆ add       ┆ 0.016948  │\n",
       "│ 1.649551  ┆ -1.298348 ┆ divide    ┆ -0.332601 │\n",
       "│ 1.649551  ┆ 1.275214  ┆ subtract  ┆ -0.578285 │\n",
       "│ -1.532715 ┆ 1.283644  ┆ divide    ┆ -0.629229 │\n",
       "└───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "ds = hp.TabularDataset(\n",
    "    train,\n",
    "    tokens,\n",
    "    special_tokens=special_tokens,\n",
    "    shuffle_cols=False,\n",
    "    max_row_length=19,\n",
    ")\n",
    "\n",
    "print(len(ds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<row-start>', 'num1', ':', 1.649551366877896, ',', 'num2', ':', -1.1335891330004717, ',', 'operation', ':', 'multiply', ',', 'result', ':', 0.7937218076665503, ',', '<row-end>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print([i.value for i in ds[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_built():\n",
    "    device_name = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device_name = \"cuda\"\n",
    "else:\n",
    "    device_name = \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/transformer.py:218: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because  self.layers[0].self_attn.batch_first was not True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "n_token = len(ds.vocab)  # size of vocabulary\n",
    "d_model = 96  # embedding dimension\n",
    "d_hid = 1_000  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "n_layers = 12  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "n_head = 12  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "model = hp.TransformerModel(\n",
    "    n_token, d_model, n_head, d_hid, n_layers, device, dropout\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2122e-01, -1.4427e+00,  1.1115e+00,  5.9512e-01, -3.4239e-01,\n",
       "         -2.5932e-01, -1.1413e+00, -1.1750e+00,  1.1038e+00, -3.6903e-01,\n",
       "         -1.1014e-01,  1.9545e-01,  5.1087e-02, -9.7750e-01, -1.2443e-01,\n",
       "         -1.0850e+00, -7.2038e-01,  5.1180e-01,  1.4882e+00, -5.7293e-01,\n",
       "          1.8395e+00, -5.2486e-01, -1.4442e+00, -2.1654e+00, -1.0036e+00,\n",
       "          2.5070e-01,  7.3173e-01, -2.1000e+00,  4.9463e-01, -7.9781e-01,\n",
       "          2.6385e-01, -1.6529e+00],\n",
       "        [-7.8153e-01, -6.3538e-01,  7.2132e-01, -1.1425e+00,  5.3475e-02,\n",
       "          1.5438e-01,  1.1437e+00,  7.0751e-01,  9.5417e-01,  4.6317e-01,\n",
       "         -4.2034e-01,  1.4357e+00,  1.6850e+00,  2.5449e-01,  5.8208e-01,\n",
       "          3.2469e-02,  2.1502e-01,  1.0419e+00, -1.0950e+00,  1.3500e+00,\n",
       "         -5.7551e-02, -1.2167e-01, -2.3170e-01,  4.0796e-01, -1.4895e-01,\n",
       "          6.2435e-01,  9.3019e-01,  1.1817e-01, -1.7125e+00, -3.5525e-02,\n",
       "          4.0968e-01,  1.2545e+00],\n",
       "        [-4.0215e-01, -3.0042e-02,  1.3880e+00,  1.5431e-01, -1.9079e+00,\n",
       "         -7.4855e-02, -5.9615e-01, -1.8618e-01, -9.9841e-01, -3.1373e-01,\n",
       "         -2.1722e-01, -1.7122e+00,  1.3636e+00, -8.8043e-03,  4.1247e-01,\n",
       "          8.5388e-01, -7.1144e-01,  7.7240e-01,  6.0148e-01,  2.7059e+00,\n",
       "          3.7917e-01, -6.1505e-01,  1.7971e+00, -7.3969e-01,  1.6938e+00,\n",
       "         -3.8398e-01,  8.0636e-01,  6.0623e-01, -6.9738e-02,  7.4422e-01,\n",
       "         -8.6151e-01, -6.7268e-01],\n",
       "        [ 3.2969e-01, -9.0062e-01, -4.1820e-01,  6.2556e-01,  9.2156e-01,\n",
       "         -6.2520e-02,  1.0265e+00,  4.7464e-01, -7.7424e-01,  4.8972e-01,\n",
       "         -3.0522e-01, -9.8683e-01, -9.1206e-01,  4.4561e-01, -1.1711e+00,\n",
       "         -9.7590e-02, -3.9586e-01, -5.4837e-01,  2.3717e-01, -7.3670e-01,\n",
       "          1.1891e+00, -3.7431e-01, -9.1790e-01, -7.6296e-01,  7.3191e-01,\n",
       "         -1.3701e+00,  4.0294e-01,  1.7012e-01,  7.0528e-01, -2.4496e-01,\n",
       "         -2.7431e-01,  1.0025e-01],\n",
       "        [ 1.4438e+00,  1.0811e+00,  2.1872e+00, -8.2680e-01, -1.0072e-01,\n",
       "         -3.6868e-01, -9.0479e-02, -1.5177e+00, -1.4455e+00, -1.2853e+00,\n",
       "          1.0690e+00, -2.1772e-01, -4.0119e-02, -8.5877e-01,  2.2350e-01,\n",
       "         -8.6754e-01,  6.0285e-01, -7.8735e-01, -1.0031e+00, -1.5067e-01,\n",
       "          8.2226e-02, -3.1545e-01, -1.2342e-01,  8.5801e-01, -6.5587e-01,\n",
       "          6.7534e-01, -8.5712e-01,  6.2405e-01, -1.5668e+00,  2.3707e-01,\n",
       "         -5.3333e-02,  8.8425e-02],\n",
       "        [ 8.7179e-01, -1.9958e-01, -4.6375e-01,  6.5722e-01, -5.7117e-01,\n",
       "         -6.9799e-01,  2.4949e-01,  1.8534e+00,  1.3625e-01, -1.3261e+00,\n",
       "         -1.2361e+00,  1.7362e-01, -6.9098e-01,  2.4885e+00,  7.6535e-01,\n",
       "          2.1625e-01, -1.6000e-01, -3.6218e-01,  9.0123e-01, -1.3641e+00,\n",
       "          6.5208e-01, -8.4575e-01, -1.3254e+00, -1.1680e-01, -2.1645e-01,\n",
       "          9.5983e-01, -7.2688e-01, -1.4710e+00, -3.0958e-01, -2.6985e+00,\n",
       "          2.1569e-01,  5.7544e-01],\n",
       "        [-4.0215e-01, -3.0042e-02,  1.3880e+00,  1.5431e-01, -1.9079e+00,\n",
       "         -7.4855e-02, -5.9615e-01, -1.8618e-01, -9.9841e-01, -3.1373e-01,\n",
       "         -2.1722e-01, -1.7122e+00,  1.3636e+00, -8.8043e-03,  4.1247e-01,\n",
       "          8.5388e-01, -7.1144e-01,  7.7240e-01,  6.0148e-01,  2.7059e+00,\n",
       "          3.7917e-01, -6.1505e-01,  1.7971e+00, -7.3969e-01,  1.6938e+00,\n",
       "         -3.8398e-01,  8.0636e-01,  6.0623e-01, -6.9738e-02,  7.4422e-01,\n",
       "         -8.6151e-01, -6.7268e-01],\n",
       "        [ 4.1867e-01, -3.2559e-02,  2.2356e-01,  4.6050e-01,  9.0282e-02,\n",
       "         -6.5284e-01,  2.6777e-02,  7.0833e-01, -1.0108e+00, -1.2310e-01,\n",
       "         -9.3270e-01, -9.2917e-01, -1.0546e+00,  6.4863e-01, -6.4587e-01,\n",
       "         -7.5130e-01, -5.3089e-02, -9.2061e-01,  1.2043e+00, -6.7829e-01,\n",
       "          2.2182e-01, -4.1141e-02, -6.3681e-01,  2.8997e-03,  1.3532e-01,\n",
       "         -2.9184e-01, -6.1742e-01, -6.4906e-01,  8.6454e-01,  8.0186e-01,\n",
       "         -5.5226e-01,  1.1277e-01],\n",
       "        [ 1.4438e+00,  1.0811e+00,  2.1872e+00, -8.2680e-01, -1.0072e-01,\n",
       "         -3.6868e-01, -9.0479e-02, -1.5177e+00, -1.4455e+00, -1.2853e+00,\n",
       "          1.0690e+00, -2.1772e-01, -4.0119e-02, -8.5877e-01,  2.2350e-01,\n",
       "         -8.6754e-01,  6.0285e-01, -7.8735e-01, -1.0031e+00, -1.5067e-01,\n",
       "          8.2226e-02, -3.1545e-01, -1.2342e-01,  8.5801e-01, -6.5587e-01,\n",
       "          6.7534e-01, -8.5712e-01,  6.2405e-01, -1.5668e+00,  2.3707e-01,\n",
       "         -5.3333e-02,  8.8425e-02],\n",
       "        [ 5.0083e-01,  4.2847e-01, -7.9605e-01, -9.1627e-01,  9.8338e-01,\n",
       "         -2.3164e+00,  1.4984e+00, -1.4580e+00,  1.9978e-01,  7.8667e-01,\n",
       "          2.6904e-02,  1.5700e-01, -5.8855e-01,  2.6457e+00, -2.3712e-01,\n",
       "          1.6906e+00, -1.9836e-01,  1.0898e+00, -8.6785e-01, -1.3141e+00,\n",
       "         -4.8933e-01, -2.0235e+00,  8.2565e-02, -1.0637e+00, -7.2053e-01,\n",
       "         -3.7698e-01,  9.6414e-01, -1.3479e-01, -1.5394e+00,  1.2656e+00,\n",
       "          1.9869e-01,  1.4319e+00],\n",
       "        [-4.0215e-01, -3.0042e-02,  1.3880e+00,  1.5431e-01, -1.9079e+00,\n",
       "         -7.4855e-02, -5.9615e-01, -1.8618e-01, -9.9841e-01, -3.1373e-01,\n",
       "         -2.1722e-01, -1.7122e+00,  1.3636e+00, -8.8043e-03,  4.1247e-01,\n",
       "          8.5388e-01, -7.1144e-01,  7.7240e-01,  6.0148e-01,  2.7059e+00,\n",
       "          3.7917e-01, -6.1505e-01,  1.7971e+00, -7.3969e-01,  1.6938e+00,\n",
       "         -3.8398e-01,  8.0636e-01,  6.0623e-01, -6.9738e-02,  7.4422e-01,\n",
       "         -8.6151e-01, -6.7268e-01],\n",
       "        [-4.4450e-01, -3.9966e-01,  1.4983e+00,  2.4586e-02, -8.5366e-01,\n",
       "         -2.7244e-01,  1.2812e-01,  1.6448e+00, -6.4289e-04, -4.2612e-01,\n",
       "          1.6237e+00, -1.3072e+00,  3.0386e-01,  1.3055e+00,  2.2955e-01,\n",
       "         -1.0233e+00, -6.6244e-01,  1.2576e-01, -1.2701e-01,  1.7108e+00,\n",
       "         -4.6347e-01, -3.2975e-01, -9.5661e-02,  2.5871e-01, -5.7442e-01,\n",
       "          1.5245e+00, -5.8571e-01,  3.0455e-02,  7.2429e-01, -1.9985e-01,\n",
       "          1.6066e+00, -8.5236e-02],\n",
       "        [ 1.4438e+00,  1.0811e+00,  2.1872e+00, -8.2680e-01, -1.0072e-01,\n",
       "         -3.6868e-01, -9.0479e-02, -1.5177e+00, -1.4455e+00, -1.2853e+00,\n",
       "          1.0690e+00, -2.1772e-01, -4.0119e-02, -8.5877e-01,  2.2350e-01,\n",
       "         -8.6754e-01,  6.0285e-01, -7.8735e-01, -1.0031e+00, -1.5067e-01,\n",
       "          8.2226e-02, -3.1545e-01, -1.2342e-01,  8.5801e-01, -6.5587e-01,\n",
       "          6.7534e-01, -8.5712e-01,  6.2405e-01, -1.5668e+00,  2.3707e-01,\n",
       "         -5.3333e-02,  8.8425e-02],\n",
       "        [-5.9008e-01, -3.2565e-02, -1.8558e+00, -3.8788e-01, -8.8447e-01,\n",
       "          2.3201e+00,  3.1247e-01, -1.2980e-01,  5.3884e-01, -4.4690e-01,\n",
       "          4.6697e-02,  2.4640e-01,  3.0236e-01,  2.3823e+00,  3.6140e-01,\n",
       "          1.0646e+00,  7.1382e-01,  1.0557e+00, -6.1774e-01, -1.6079e+00,\n",
       "          4.0411e-01,  3.5378e-01, -1.4371e+00,  2.4771e-02,  4.1337e-01,\n",
       "          6.7558e-01, -7.8989e-01, -1.6947e+00,  1.1306e+00, -4.8115e-01,\n",
       "         -2.9422e-02,  1.8987e+00],\n",
       "        [-4.0215e-01, -3.0042e-02,  1.3880e+00,  1.5431e-01, -1.9079e+00,\n",
       "         -7.4855e-02, -5.9615e-01, -1.8618e-01, -9.9841e-01, -3.1373e-01,\n",
       "         -2.1722e-01, -1.7122e+00,  1.3636e+00, -8.8043e-03,  4.1247e-01,\n",
       "          8.5388e-01, -7.1144e-01,  7.7240e-01,  6.0148e-01,  2.7059e+00,\n",
       "          3.7917e-01, -6.1505e-01,  1.7971e+00, -7.3969e-01,  1.6938e+00,\n",
       "         -3.8398e-01,  8.0636e-01,  6.0623e-01, -6.9738e-02,  7.4422e-01,\n",
       "         -8.6151e-01, -6.7268e-01],\n",
       "        [ 3.7577e-01, -4.5103e-01, -8.5818e-02,  5.4007e-01,  4.9102e-01,\n",
       "         -3.6826e-01,  5.0871e-01,  5.9567e-01, -8.9675e-01,  1.7233e-01,\n",
       "         -6.3020e-01, -9.5697e-01, -9.8589e-01,  5.5076e-01, -8.9908e-01,\n",
       "         -4.3616e-01, -2.1833e-01, -7.4116e-01,  7.3806e-01, -7.0645e-01,\n",
       "          6.8814e-01, -2.0176e-01, -7.7232e-01, -3.6631e-01,  4.2293e-01,\n",
       "         -8.1166e-01, -1.2553e-01, -2.5415e-01,  7.8776e-01,  2.9721e-01,\n",
       "         -4.1826e-01,  1.0673e-01],\n",
       "        [ 1.4438e+00,  1.0811e+00,  2.1872e+00, -8.2680e-01, -1.0072e-01,\n",
       "         -3.6868e-01, -9.0479e-02, -1.5177e+00, -1.4455e+00, -1.2853e+00,\n",
       "          1.0690e+00, -2.1772e-01, -4.0119e-02, -8.5877e-01,  2.2350e-01,\n",
       "         -8.6754e-01,  6.0285e-01, -7.8735e-01, -1.0031e+00, -1.5067e-01,\n",
       "          8.2226e-02, -3.1545e-01, -1.2342e-01,  8.5801e-01, -6.5587e-01,\n",
       "          6.7534e-01, -8.5712e-01,  6.2405e-01, -1.5668e+00,  2.3707e-01,\n",
       "         -5.3333e-02,  8.8425e-02],\n",
       "        [-1.5901e+00, -1.6053e+00, -6.6140e-01, -5.2282e-02,  1.7414e+00,\n",
       "         -1.8928e+00, -1.4566e+00,  1.8792e+00,  1.5860e+00, -2.6071e-01,\n",
       "         -6.2541e-01,  5.7961e-01,  8.2241e-01,  1.5904e+00, -1.2200e+00,\n",
       "          2.6996e+00, -7.4591e-03,  2.1757e+00, -7.0624e-01,  4.1122e-01,\n",
       "         -1.2294e+00,  6.6243e-01,  2.1961e+00,  1.7105e+00,  4.0160e-02,\n",
       "          1.2657e+00,  3.2230e-01,  4.6042e-01, -6.4530e-02,  2.5749e-01,\n",
       "          2.2331e+00, -1.0982e+00],\n",
       "        [-6.7987e-01, -6.4839e-01, -2.7151e-01,  1.3469e+00, -4.7174e-01,\n",
       "          2.3264e-01,  8.8092e-01,  8.1172e-01, -1.7899e+00,  3.8727e-01,\n",
       "         -7.3771e-01,  9.1892e-01,  7.4005e-01,  1.1822e+00, -1.2311e-01,\n",
       "          1.9642e+00, -7.1720e-01,  8.7773e-01,  2.1674e-01, -3.4185e-02,\n",
       "         -1.4977e+00,  8.6255e-01, -8.6417e-01, -1.2161e-01, -1.6943e+00,\n",
       "          4.4470e-01,  2.0200e-01, -1.1372e+00,  7.9009e-01, -1.0068e+00,\n",
       "         -1.5001e+00,  3.3115e-01]], device='mps:0', grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class StringNumericEmbedding(nn.Module):\n",
    "#     def __init__(self, n_token: int, d_model: int, device: torch.device):\n",
    "#         super().__init__()\n",
    "#         self.device = device\n",
    "#         self.embedding = nn.Embedding(n_token + 1, d_model).to(device)  # padding_idx=0\n",
    "#         self.numeric_embedding = nn.Linear(1, d_model).to(device)\n",
    "\n",
    "#     def forward(self, input: hp.StringNumeric):\n",
    "#         embedding_tensor = torch.zeros(\n",
    "#             (len(input), self.embedding.embedding_dim), dtype=torch.float32\n",
    "#         ).to(self.device)\n",
    "#         for idx, val in enumerate(input):\n",
    "#             if val.is_numeric:\n",
    "#                 print(\"numeric\")\n",
    "#                 val = torch.Tensor([val.value]).float().to(self.device)\n",
    "#                 embedding_tensor[idx] = self.numeric_embedding(val)\n",
    "#             else:\n",
    "#                 print(\"non-numeric\")\n",
    "#                 embed_idx = torch.Tensor([val.embedding_idx]).long().to(self.device)\n",
    "#                 embedding_tensor[idx] = self.embedding(embed_idx)\n",
    "\n",
    "#         embedding = torch.Tensor(embedding_list).to(self.device)\n",
    "\n",
    "\n",
    "x = hp.StringNumericEmbedding(d_model=32, n_token=tokens.shape[0], device=device)\n",
    "data, targets = hp.batch_data(ds, 1, n_row=1)\n",
    "x(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw src_shape: 19\n"
     ]
    }
   ],
   "source": [
    "# Test the model out:\n",
    "with torch.no_grad():\n",
    "    data, targets = hp.batch_data(ds, 1, n_row=1)\n",
    "    class_out, numeric_out = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "lr = 0.1  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size =100, gamma=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.8,\n",
    "    patience=5,\n",
    "    threshold=0.001,\n",
    "    threshold_mode=\"rel\",\n",
    "    cooldown=0,\n",
    "    min_lr=0.01,\n",
    "    eps=1e-08,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "\n",
    "def train(model: nn.Module, epochs=1, model_name=\"\") -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.0\n",
    "    log_interval = 1000\n",
    "    lr_eval_interval = 25\n",
    "    n_row = 100  # one because it's not time series\n",
    "    start_time = time.time()\n",
    "    for epoch in trange(1, epochs + 1, leave=True, desc=\"Epoch\"):\n",
    "        pbar = trange(0, len(ds) - 1, n_row, desc=f\"Batch for {epoch}/{epochs}\")\n",
    "        writer = SummaryWriter(\"runs/\" + model_name + \"_run_\" + str(epoch))\n",
    "        for batch, i in enumerate(pbar):\n",
    "            data, targets = hp.batch_data(ds, i, n_row=n_row)\n",
    "            class_output, numeric_output = model(data)\n",
    "            loss, loss_dict = hp.hephaestus_loss(\n",
    "                class_output, numeric_output, targets, tokens, special_tokens, device\n",
    "            )\n",
    "            num_loss = loss_dict[\"reg_loss\"].item()\n",
    "            class_loss = loss_dict[\"class_loss\"].item()\n",
    "            writer.add_scalar(\"Loss/total_loss\", loss, batch)\n",
    "            writer.add_scalar(\"Loss/numeric_loss\", num_loss, batch)\n",
    "            writer.add_scalar(\"Loss/class_loss\", class_loss, batch)\n",
    "            writer.add_scalar(\n",
    "                \"Metrics/learning_rate\", optimizer.param_groups[0][\"lr\"], batch\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            pbar.set_postfix(\n",
    "                {\n",
    "                    \"tl\": f\"{loss:.2f}\",\n",
    "                    \"cl\": f\"{class_loss:.2f}\",\n",
    "                    \"nl\": f\"{num_loss:.2f}\",\n",
    "                    \"tr\": f\"{optimizer.param_groups[0]['lr']:.2f}\",\n",
    "                },\n",
    "                refresh=True,\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "            if batch % lr_eval_interval == 0:\n",
    "                # pbar.set_p(\n",
    "                #     f\"tl: {}, nl: {num_loss:.2f}, cl: {class_loss:.2f}, tr: {optimizer.param_groups[0]['lr']:.2f}\"\n",
    "                # )\n",
    "\n",
    "                scheduler.step(loss)\n",
    "\n",
    "                # if batch % log_interval == 0:\n",
    "                #     # lr = scheduler.get_last_lr()[0]\n",
    "                #     lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "                #     ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "                #     cur_loss = total_loss / log_interval\n",
    "                #     ppl = math.exp(cur_loss)\n",
    "                #     print(  # f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                #         f\"lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | \",\n",
    "                #         f\"loss {cur_loss:5.2f} | ppl {ppl:8.2f}\",\n",
    "                #         f\"num_loss {num_loss:5.2f} | class_loss {class_loss:5.2f}\",\n",
    "                #     )\n",
    "                #     total_loss = 0\n",
    "                start_time = time.time()\n",
    "                # scheduler.step(loss)\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_time = dt.now()\n",
    "model_time = model_time.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "exp_name = \"numeric_scaling6\"\n",
    "\n",
    "model_name = model_time + \"_\" + exp_name\n",
    "epochs = 3\n",
    "train(model=model, epochs=epochs, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 2,776,946\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# model = CustomNumericAttention(d_model=512, n_head=8) # Replace with your model\n",
    "param_count = count_parameters(model)\n",
    "print(f\"Total trainable parameters: {param_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "save_model = True\n",
    "if save_model:\n",
    "    MODEL_PATH = \"models/\" + model_name + \".pth\"\n",
    "    torch.save(model.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = hp.TabularDataset(\n",
    "    test,\n",
    "    tokens,\n",
    "    special_tokens=special_tokens,\n",
    "    shuffle_cols=False,\n",
    "    max_row_length=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, ds, idx) -> None:\n",
    "    model.eval()  # turn on train mode\n",
    "    n_row = 1  # one because it's not time series\n",
    "    with torch.no_grad():\n",
    "        data, targets = hp.batch_data(ds, idx, n_row=n_row)\n",
    "        class_output, numeric_output = model(data)\n",
    "        loss, loss_dict = hp.hephaestus_loss(\n",
    "            class_output, numeric_output, targets, tokens, special_tokens, device\n",
    "        )\n",
    "        return {\n",
    "            \"loss\": loss.item(),\n",
    "            \"loss_dict\": loss_dict,\n",
    "            \"data\": data,\n",
    "            \"targets\": targets,\n",
    "            \"class_output\": class_output,\n",
    "            \"numeric_output\": numeric_output,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2_000, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>num1</th><th>num2</th><th>operation</th><th>result</th></tr><tr><td>f64</td><td>f64</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>-0.222069</td><td>-0.093071</td><td>&quot;divide&quot;</td><td>-0.410356</td></tr><tr><td>-0.760505</td><td>1.052537</td><td>&quot;multiply&quot;</td><td>1.074778</td></tr><tr><td>-0.596589</td><td>0.714712</td><td>&quot;add&quot;</td><td>-0.403601</td></tr><tr><td>1.45287</td><td>0.309759</td><td>&quot;divide&quot;</td><td>-0.410351</td></tr><tr><td>0.846681</td><td>-0.99011</td><td>&quot;subtract&quot;</td><td>-0.406913</td></tr><tr><td>0.336244</td><td>0.043056</td><td>&quot;divide&quot;</td><td>-0.410354</td></tr><tr><td>1.622331</td><td>-0.777194</td><td>&quot;multiply&quot;</td><td>1.35483</td></tr><tr><td>-1.0474</td><td>-0.145427</td><td>&quot;multiply&quot;</td><td>0.192173</td></tr><tr><td>0.55095</td><td>-0.853984</td><td>&quot;subtract&quot;</td><td>-0.407723</td></tr><tr><td>1.709486</td><td>0.4846</td><td>&quot;multiply&quot;</td><td>3.7469</td></tr><tr><td>0.714232</td><td>-1.164632</td><td>&quot;divide&quot;</td><td>-0.410334</td></tr><tr><td>-0.093761</td><td>-1.0669</td><td>&quot;multiply&quot;</td><td>0.197621</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>1.366175</td><td>1.505544</td><td>&quot;divide&quot;</td><td>-0.410355</td></tr><tr><td>1.600914</td><td>-0.882463</td><td>&quot;subtract&quot;</td><td>-0.405697</td></tr><tr><td>-0.959116</td><td>1.317061</td><td>&quot;divide&quot;</td><td>-0.41036</td></tr><tr><td>-0.59361</td><td>-1.65782</td><td>&quot;subtract&quot;</td><td>-0.408373</td></tr><tr><td>-0.502959</td><td>1.456678</td><td>&quot;add&quot;</td><td>-0.40204</td></tr><tr><td>1.210125</td><td>0.130316</td><td>&quot;add&quot;</td><td>-0.401297</td></tr><tr><td>0.398664</td><td>-0.034867</td><td>&quot;add&quot;</td><td>-0.40313</td></tr><tr><td>0.710765</td><td>1.529977</td><td>&quot;subtract&quot;</td><td>-0.411873</td></tr><tr><td>0.013741</td><td>1.486612</td><td>&quot;subtract&quot;</td><td>-0.413102</td></tr><tr><td>-1.539825</td><td>0.591053</td><td>&quot;divide&quot;</td><td>-0.410361</td></tr><tr><td>-0.721429</td><td>-1.011053</td><td>&quot;add&quot;</td><td>-0.407058</td></tr><tr><td>-1.403492</td><td>-0.707386</td><td>&quot;subtract&quot;</td><td>-0.411669</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2_000, 4)\n",
       "┌───────────┬───────────┬───────────┬───────────┐\n",
       "│ num1      ┆ num2      ┆ operation ┆ result    │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│ f64       ┆ f64       ┆ str       ┆ f64       │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ -0.222069 ┆ -0.093071 ┆ divide    ┆ -0.410356 │\n",
       "│ -0.760505 ┆ 1.052537  ┆ multiply  ┆ 1.074778  │\n",
       "│ -0.596589 ┆ 0.714712  ┆ add       ┆ -0.403601 │\n",
       "│ 1.45287   ┆ 0.309759  ┆ divide    ┆ -0.410351 │\n",
       "│ …         ┆ …         ┆ …         ┆ …         │\n",
       "│ 0.013741  ┆ 1.486612  ┆ subtract  ┆ -0.413102 │\n",
       "│ -1.539825 ┆ 0.591053  ┆ divide    ┆ -0.410361 │\n",
       "│ -0.721429 ┆ -1.011053 ┆ add       ┆ -0.407058 │\n",
       "│ -1.403492 ┆ -0.707386 ┆ subtract  ┆ -0.411669 │\n",
       "└───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = hp.TabularDataset(\n",
    "    test,\n",
    "    tokens,\n",
    "    special_tokens=special_tokens,\n",
    "    shuffle_cols=False,\n",
    "    max_row_length=18,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(res):\n",
    "    def replacer(x):\n",
    "        x = x.replace(\"  \", \":\")\n",
    "        x = x.replace(\" ,\", \",\")\n",
    "        return x\n",
    "\n",
    "    actuals = [str(i.value) for i in res[\"targets\"]]\n",
    "    actuals_ = \" \".join(actuals)\n",
    "    actual_str = actuals_.split(\"<row-end>\")[0]\n",
    "    actual_str = replacer(actual_str)\n",
    "    masked_str = [str(i.value) for i in res[\"data\"]]\n",
    "    masked_str = \" \".join(masked_str)\n",
    "    masked_str = masked_str.split(\"<row-end>\")[0]\n",
    "    masked_str = replacer(masked_str)\n",
    "    lsm = nn.Softmax(dim=0)\n",
    "    softmax_cats = lsm(res[\"class_output\"])\n",
    "    softmax_cats = torch.argmax(softmax_cats, dim=1)\n",
    "    gen_tokens = []\n",
    "    for idx, pred in enumerate(softmax_cats):\n",
    "        token = tokens[pred - 1]\n",
    "        if token == \"<numeric>\":\n",
    "            gen_tokens.append(str(res[\"numeric_output\"][idx].item()))\n",
    "        else:\n",
    "            gen_tokens.append(token)\n",
    "    preds = \" \".join(gen_tokens)\n",
    "    preds = replacer(preds)\n",
    "\n",
    "    s = (\n",
    "        f\"Targets   : {actual_str}\\n\"\n",
    "        + f\"Masked    : {masked_str}\\n\"\n",
    "        + f\"Predicted : {preds.split('<row-end>')[0]}\"\n",
    "    )\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0\n",
      "Targets   : <row-start> num1 : 0.627537880086947, num2 : -1.4249174175476262, operation : multiply, result : 0.006089391901095394, \n",
      "Masked    : <row-start> num1 : 0.627537880086947, num2 : -1.4249174175476262, operation : multiply, result : 0.006089391901095394, \n",
      "Predicted : <row-start> num1 : 0.746845006942749, num2 : subtract, operation : multiply, result : -0.0796755775809288, \n",
      "\n",
      "Row 1\n",
      "Targets   : <row-start> num1 : -0.21727413399197865, num2 : -1.6463103806205621, operation : multiply, result : -0.3242663783358256, \n",
      "Masked    : <row-start> num1 : -0.21727413399197865, num2 : -1.6463103806205621, operation : multiply, result : -0.3242663783358256, \n",
      "Predicted : <row-start> num1 : -0.2761440575122833, num2 : subtract, operation : multiply, result : -0.31431835889816284, \n",
      "\n",
      "Row 2\n",
      "Targets   : <row-start> num1 : 0.4922944119488227, num2 : -1.1597225240082345, operation : add, result : -0.4050543816063307, \n",
      "Masked    : <row-start> num1 : 0.4922944119488227, num2 : -1.1597225240082345, operation : add, <mask> : -0.4050543816063307, \n",
      "Predicted : <row-start> num1 : 0.597592830657959, num2 : subtract, operation : add, result : -0.4656722843647003, \n",
      "\n",
      "Row 3\n",
      "Targets   : <row-start> num1 : 0.2980986628274134, num2 : 0.3281232982046361, operation : add, result : -0.4026414763754813, \n",
      "Masked    : <row-start> <mask> : 0.2980986628274134, num2 : <numeric_mask>, operation : add, <mask> : -0.4026414763754813, \n",
      "Predicted : <row-start> result : 0.4876459836959839, num2 : -0.18969981372356415, operation : <pad>, result : -0.554686427116394, \n",
      "\n",
      "Row 4\n",
      "Targets   : <row-start> num1 : 1.5326287822420868, num2 : 0.1418872688472103, operation : subtract, result : -0.4077374472967699, \n",
      "Masked    : <row-start> num1 : 1.5326287822420868, num2 : 0.1418872688472103, operation : subtract, result : -0.4077374472967699, \n",
      "Predicted : <row-start> num1 : subtract, num2 : 0.2131994068622589, operation : subtract, result : -0.5440848469734192, \n",
      "\n",
      "Row 5\n",
      "Targets   : <row-start> num1 : -0.719697155884671, num2 : -0.3862665799710027, operation : add, result : -0.40588791345890773, \n",
      "Masked    : <row-start> num1 : <numeric_mask>, num2 : -0.3862665799710027, operation : add, result : -0.40588791345890773, \n",
      "Predicted : <row-start> num1 : -0.10331123322248459, num2 : -0.5987114906311035, operation : add, result : -0.5253350734710693, \n",
      "\n",
      "Row 6\n",
      "Targets   : <row-start> num1 : 1.6768448410186372, num2 : -1.126236900574068, operation : add, result : -0.40276584005801164, \n",
      "Masked    : <row-start> num1 : <numeric_mask>, num2 : -1.126236900574068, <mask> : add, result : -0.40276584005801164, \n",
      "Predicted : <row-start> num1 : -0.06223365291953087, num2 : -1.0088807344436646, operation : add, result : -0.43729862570762634, \n",
      "\n",
      "Row 7\n",
      "Targets   : <row-start> num1 : 1.0679460968444288, num2 : 0.32578033230741865, operation : divide, result : -0.4103527426792386, \n",
      "Masked    : <row-start> <mask> : 1.0679460968444288, num2 : 0.32578033230741865, operation : divide, <mask> : -0.4103527426792386, \n",
      "Predicted : <row-start> result : 1.504229187965393, num2 : 0.5459406971931458, operation : divide, result : -0.5973004698753357, \n",
      "\n",
      "Row 8\n",
      "Targets   : <row-start> num1 : 0.8344886380314569, num2 : -0.16986016545501192, operation : subtract, result : -0.40846736244358517, \n",
      "Masked    : <row-start> <mask> : 0.8344886380314569, num2 : -0.16986016545501192, operation : subtract, result : -0.40846736244358517, \n",
      "Predicted : <row-start> result : num1, num2 : -0.31731298565864563, operation : subtract, result : -0.5374314188957214, \n",
      "\n",
      "Row 9\n",
      "Targets   : <row-start> num1 : 0.48972924482543834, num2 : 1.0901836351945475, operation : multiply, result : 3.0071781849695562, \n",
      "Masked    : <row-start> <mask> : <numeric_mask>, num2 : 1.0901836351945475, operation : multiply, result : 3.0071781849695562, \n",
      "Predicted : <row-start> result : -0.15376323461532593, num2 : 1.4277478456497192, operation : divide, result : num1, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    res = evaluate(model, ds, i)\n",
    "    print(f\"Row {i}\")\n",
    "    print(show_results(res))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, ds, idx) -> None:\n",
    "    model.eval()  # turn on train mode\n",
    "    n_row = 1  # one because it's not time series\n",
    "    with torch.no_grad():\n",
    "        data, targets = hp.batch_data(ds, idx, n_row=n_row)\n",
    "        class_output, numeric_output = model(data)\n",
    "        loss, loss_dict = hp.hephaestus_loss(\n",
    "            class_output, numeric_output, targets, tokens, special_tokens, device\n",
    "        )\n",
    "        return {\n",
    "            \"loss\": loss.item(),\n",
    "            \"loss_dict\": loss_dict,\n",
    "            \"data\": data,\n",
    "            \"targets\": targets,\n",
    "            \"class_output\": class_output,\n",
    "            \"numeric_output\": numeric_output,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = evaluate(model, ds_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StringNumeric' object has no attribute 'numeric_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 34\u001b[0m\n\u001b[1;32m     30\u001b[0m     pred_str \u001b[39m=\u001b[39m prediction_str(d[\u001b[39m\"\u001b[39m\u001b[39mclass_output\u001b[39m\u001b[39m\"\u001b[39m], d[\u001b[39m\"\u001b[39m\u001b[39mnumeric_output\u001b[39m\u001b[39m\"\u001b[39m], tokens)\n\u001b[1;32m     31\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mData  : \u001b[39m\u001b[39m{\u001b[39;00mdata_str\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mTarget: \u001b[39m\u001b[39m{\u001b[39;00mtarget_str\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mPredict: \u001b[39m\u001b[39m{\u001b[39;00mpred_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m print_results(res)\n",
      "Cell \u001b[0;32mIn[34], line 28\u001b[0m, in \u001b[0;36mprint_results\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_results\u001b[39m(d: \u001b[39mdict\u001b[39m):\n\u001b[0;32m---> 28\u001b[0m     data_str \u001b[39m=\u001b[39m make_str(d[\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     29\u001b[0m     target_str \u001b[39m=\u001b[39m make_str(d[\u001b[39m\"\u001b[39m\u001b[39mtargets\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     30\u001b[0m     pred_str \u001b[39m=\u001b[39m prediction_str(d[\u001b[39m\"\u001b[39m\u001b[39mclass_output\u001b[39m\u001b[39m\"\u001b[39m], d[\u001b[39m\"\u001b[39m\u001b[39mnumeric_output\u001b[39m\u001b[39m\"\u001b[39m], tokens)\n",
      "Cell \u001b[0;32mIn[34], line 5\u001b[0m, in \u001b[0;36mmake_str\u001b[0;34m(l)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m l:\n\u001b[1;32m      4\u001b[0m     \u001b[39mif\u001b[39;00m i\u001b[39m.\u001b[39mis_numeric:\n\u001b[0;32m----> 5\u001b[0m         result_list\u001b[39m.\u001b[39mappend(i\u001b[39m.\u001b[39;49mnumeric_value)\n\u001b[1;32m      6\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m         result_list\u001b[39m.\u001b[39mappend(i\u001b[39m.\u001b[39mvalue)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StringNumeric' object has no attribute 'numeric_value'"
     ]
    }
   ],
   "source": [
    "def make_str(l: list):\n",
    "    result_list = []\n",
    "    for i in l:\n",
    "        if i.is_numeric:\n",
    "            result_list.append(i.numeric_value)\n",
    "        else:\n",
    "            result_list.append(i.value)\n",
    "    result_list = [str(i) for i in result_list]\n",
    "    return \" \".join(result_list).split(\"<row-end>\")[0]\n",
    "\n",
    "\n",
    "def prediction_str(class_output, numeric_output, tokens):\n",
    "    lsm = nn.Softmax(dim=0)\n",
    "    softmax_cats = lsm(class_output)\n",
    "    softmax_cats = torch.argmax(softmax_cats, dim=1)\n",
    "    gen_tokens = []\n",
    "    for idx, pred in enumerate(softmax_cats):\n",
    "        token = tokens[pred]\n",
    "        if token == \"<numeric>\":\n",
    "            gen_tokens.append(str(numeric_output[idx].item()))\n",
    "        else:\n",
    "            gen_tokens.append(token)\n",
    "    preds = \" \".join(gen_tokens).split(\"<row-end>\")[0]\n",
    "    return preds\n",
    "\n",
    "\n",
    "def print_results(d: dict):\n",
    "    data_str = make_str(d[\"data\"])\n",
    "    target_str = make_str(d[\"targets\"])\n",
    "    pred_str = prediction_str(d[\"class_output\"], d[\"numeric_output\"], tokens)\n",
    "    print(f\"Data  : {data_str}\\nTarget: {target_str}\\nPredict: {pred_str}\")\n",
    "\n",
    "\n",
    "print_results(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 41])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"class_output\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"numeric_output\"][:, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 50])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((res[\"numeric_output\"][None, :], res[\"class_output\"].T)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_str(class_output, numeric_output, tokens):\n",
    "    lsm = nn.Softmax(dim=0)\n",
    "    softmax_cats = lsm(class_output)\n",
    "    softmax_cats = torch.argmax(softmax_cats, dim=1)\n",
    "    gen_tokens = []\n",
    "    for idx, pred in enumerate(softmax_cats):\n",
    "        token = tokens[pred]\n",
    "        if token == \"<numeric>\":\n",
    "            gen_tokens.append(str(numeric_output[idx].item()))\n",
    "        else:\n",
    "            gen_tokens.append(token)\n",
    "    preds = \" \".join(gen_tokens).split(\"<row-end>\")[0]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_data(N):\n",
    "    x = torch.randint(1, 100, (N, 2), dtype=torch.float32)\n",
    "    y_mul = x[:, 0] * x[:, 1]\n",
    "    y_div = x[:, 0] / x[:, 1]\n",
    "    y = torch.stack((y_mul, y_div), dim=1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MathNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MathNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5478397.5\n",
      "Epoch 100, Loss: 1675928.875\n",
      "Epoch 200, Loss: 784210.25\n",
      "Epoch 300, Loss: 779894.25\n",
      "Epoch 400, Loss: 775376.625\n",
      "Epoch 500, Loss: 770643.8125\n",
      "Epoch 600, Loss: 765713.625\n",
      "Epoch 700, Loss: 760585.8125\n",
      "Epoch 800, Loss: 755247.5\n",
      "Epoch 900, Loss: 749672.125\n"
     ]
    }
   ],
   "source": [
    "N = 1000000\n",
    "data_x, data_y = generate_data(N)\n",
    "\n",
    "# Split into training and testing\n",
    "train_x, test_x = data_x[:8000], data_x[8000:]\n",
    "train_y, test_y = data_y[:8000], data_y[8000:]\n",
    "\n",
    "model = MathNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    outputs = model(train_x)\n",
    "    loss = criterion(outputs, train_y)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 737482.3125\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_outputs = model(test_x)\n",
    "    test_loss = criterion(test_outputs, test_y)\n",
    "    print(f\"Test Loss: {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8000, 2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([992000, 2])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3350e+03, 5.9333e+00],\n",
       "        [7.4000e+01, 1.8500e+01],\n",
       "        [2.7600e+02, 1.7250e+01],\n",
       "        ...,\n",
       "        [9.8600e+02, 8.5294e-01],\n",
       "        [9.0000e+02, 1.1111e-01],\n",
       "        [2.5550e+03, 2.0857e+00]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.5403e+03, -6.8926e-01],\n",
       "        [ 5.4591e+03, -7.7647e-01],\n",
       "        [ 1.7531e+03,  9.1369e+00],\n",
       "        ...,\n",
       "        [ 8.5710e+02,  8.9137e-01],\n",
       "        [ 6.9791e+03, -1.7934e+00],\n",
       "        [ 5.2343e+01,  1.2579e+00]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N):\n",
    "    x = torch.randint(1, 1000, (N, 2), dtype=torch.float32)\n",
    "    y_mul = x[:, 0] * x[:, 1]\n",
    "    y_div = x[:, 0] / x[:, 1]\n",
    "    y = torch.stack((y_mul, y_div), dim=1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        N = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.nn.Softmax(dim=3)(energy)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 256), nn.ReLU(), nn.Linear(256, embed_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Add skip connection, run through normalization and finally feed forward\n",
    "        x = self.norm1(attention + query)\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.norm2(forward + x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MathNet(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(MathNet, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        self.fc1 = nn.Linear(2, embed_size)\n",
    "        self.transformer_block = TransformerBlock(embed_size, heads)\n",
    "        self.fc2 = nn.Linear(embed_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = x.repeat(1, 3, 1)  # Repeat x three times to form keys, values, queries\n",
    "        x = self.transformer_block(x, x, x, mask=None)\n",
    "        x = self.fc2(x[:, 0, :])  # We only need the first result from the sequence\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/pytorch/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([8000, 2])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5377728.5\n",
      "Epoch 100, Loss: 4712922.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/pytorch/lib/python3.11/site-packages/torch/_tensor.py:491\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    482\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    483\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    484\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    490\u001b[0m     )\n\u001b[0;32m--> 491\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    492\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    493\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/pytorch/lib/python3.11/site-packages/torch/autograd/__init__.py:204\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    201\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    206\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MathNet(embed_size=64, heads=4)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    outputs = model(train_x)\n",
    "    loss = criterion(outputs, train_y)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NumberEmbedding(nn.Module):\n",
    "    def __init__(self, embed_size=512):\n",
    "        super(NumberEmbedding, self).__init__()\n",
    "        self.linear = nn.Linear(1, embed_size)\n",
    "\n",
    "    def forward(self, numbers):\n",
    "        # Assuming numbers is a tensor containing the numerical values\n",
    "        return self.linear(numbers.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne = NumberEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4]) torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "n = torch.Tensor([1.01, 1, 2, 3])\n",
    "nn = n.unsqueeze(-1)\n",
    "\n",
    "print(n.shape, nn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0100],\n",
       "        [1.0000],\n",
       "        [2.0000],\n",
       "        [3.0000]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0800, -0.2637,  0.9156,  ...,  0.5839, -0.1389,  0.9257],\n",
       "        [ 0.0831, -0.2657,  0.9104,  ...,  0.5853, -0.1418,  0.9182],\n",
       "        [-0.2212, -0.0711,  1.4304,  ...,  0.4433,  0.1406,  1.6661],\n",
       "        [-0.5256,  0.1235,  1.9504,  ...,  0.3013,  0.4230,  2.4140]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ne(n)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
