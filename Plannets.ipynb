{"cells":[{"cell_type":"markdown","metadata":{"id":"trbcfMV6vked"},"source":["<https://github.com/PolymathicAI/xVal>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"id":"yKsx1R26dLGC"},"outputs":[],"source":["import os\n","\n","os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.95\""]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2102,"status":"ok","timestamp":1708903394514,"user":{"displayName":"Kai Lukowiak","userId":"12340107642472090190"},"user_tz":420},"id":"RYm4WOeS1B8x","outputId":"a5c71de2-9597-4fcb-a9a7-b743f1a29feb"},"outputs":[{"data":{"text/plain":["{cuda(id=0)}"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import jax.numpy as jnp  # Oddly works in colab to set gpu\n","\n","arr = jnp.array([1, 2, 3])\n","arr.devices()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Jqmm_5s9KXkI"},"outputs":[],"source":["import icecream\n","from icecream import ic\n","\n","icecream.install()\n","ic_disable = True\n","if ic_disable:\n","    ic.disable()\n","ic.configureOutput(includeContext=True, contextAbsPath=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18045,"status":"ok","timestamp":1708903436762,"user":{"displayName":"Kai Lukowiak","userId":"12340107642472090190"},"user_tz":420},"id":"oSKniUdxtiTd","outputId":"49871258-6ef7-4956-ad3b-90af827f7253"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-20 04:02:00.917371: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["import os\n","import ast\n","\n","from datetime import datetime as dt\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","import hephaestus as hp\n","import jax\n","import jax.numpy as jnp\n","import numpy as np\n","import optax\n","import pandas as pd\n","from flax.training import train_state\n","from icecream import ic\n","from jax import random\n","from flax import struct\n","from jax.tree_util import tree_flatten\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm.notebook import tqdm, trange\n","\n","pd.options.mode.copy_on_write = True"]},{"cell_type":"markdown","metadata":{},"source":["\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def line2df(line, idx):\n","    data_rows = []\n","    line = ast.literal_eval(line)\n","    for i, time_step in enumerate(line[\"data\"]):\n","        row = {\"time_step\": i}\n","        # Add position data for each planet\n","        for j, position in enumerate(time_step):\n","            row[f\"planet{j}_x\"] = position[0]\n","            row[f\"planet{j}_y\"] = position[1]\n","        data_rows.append(row)\n","\n","    df = pd.DataFrame(data_rows)\n","    description = line.pop(\"description\")\n","    step_size = description.pop(\"stepsize\")\n","    for k, v in description.items():\n","        for k_prop, v_prop in v.items():\n","            df[f\"{k}_{k_prop}\"] = v_prop\n","    df[\"time_step\"] = df[\"time_step\"] * step_size\n","    df.insert(0, \"idx\", idx)\n","\n","    return df"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["files = os.listdir(\"data\")\n","if \"planets.parquet\" not in files:\n","    with open(\"data/planets.data\") as f:\n","        data = f.read().splitlines()\n","\n","        dfs = []\n","        for idx, line in enumerate(tqdm(data)):\n","            dfs.append(line2df(line, idx))\n","        print(\"Concatenating dfs...\")\n","        df = pd.concat(dfs)\n","    df.to_parquet(\"data/planets.parquet\")\n","else:\n","    df = pd.read_parquet(\"data/planets.parquet\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["min     30.000000\n","mean    44.511656\n","max     59.000000\n","Name: time_step, dtype: float64"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Get min, mean, and max number of time steps\n","df.groupby(\"idx\").count().time_step.agg([\"min\", \"mean\", \"max\"])"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class SimpleDS(Dataset):\n","    def __init__(self, df, custom_attention: bool = True):\n","        # Add nan padding to make sure all sequences are the same length\n","        # use the idx column to group by\n","        self.max_seq_len = df.groupby(\"idx\").count().time_step.max()\n","        self.custom_attention = custom_attention\n","        self.df = df\n","        self.batch_size = self.max_seq_len\n","\n","        self.special_tokens = [\"[PAD]\", \"[NUMERIC_MASK]\", \"[MASK]\"]\n","        self.cat_mask = \"[MASK]\"\n","        self.numeric_mask = \"[NUMERIC_MASK]\"\n","\n","        self.col_tokens = [col_name for col_name in df.columns if col_name != \"idx\"]\n","\n","        self.tokens = self.special_tokens + self.col_tokens\n","\n","        self.token_dict = {token: i for i, token in enumerate(self.tokens)}\n","        self.token_decoder_dict = {i: token for i, token in enumerate(self.tokens)}\n","        self.n_tokens = len(self.tokens)\n","        self.numeric_indices = jnp.array(\n","            [self.tokens.index(i) for i in self.col_tokens]\n","        )\n","\n","        self.numeric_mask_token = self.tokens.index(self.numeric_mask)\n","\n","    def __len__(self):\n","        return self.df.idx.max() + 1  # probably should be max idx + 1 thanks\n","\n","    def __getitem__(self, set_idx):\n","        batch = self.df.loc[\n","            self.df.idx == set_idx, [col for col in self.df.columns if col != \"idx\"]\n","        ]\n","        batch = np.array(batch.values)\n","        # Add padding\n","        batch_len, n_cols = batch.shape\n","        pad_len = self.max_seq_len - batch_len\n","        padding = np.full((pad_len, n_cols), jnp.nan)\n","        batch = np.concatenate([batch, padding], axis=0)\n","        return batch"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"4RZof2SNKXkK"},"outputs":[],"source":["# Get train test split at 80/20\n","train_idx = int(df.idx.max() * 0.8)\n","train_df = df.loc[df.idx < train_idx].copy()\n","test_df = df.loc[df.idx >= train_idx].copy()\n","del df\n","train_ds = SimpleDS(train_df)\n","test_ds = SimpleDS(test_df)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def make_batch(ds: SimpleDS, start: int, length: int):\n","    data = []\n","    for i in range(start, length + start):\n","        data.append(ds[i])\n","\n","    return jnp.array(data)\n","\n","\n","batch = make_batch(train_ds, 0, 4)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Y4b7IkMWKXkK"},"outputs":[],"source":["time_series_regressor = hp.simple_time_series.SimplePred(train_ds, d_model=64 * 4)"]},{"cell_type":"markdown","metadata":{},"source":["\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["key = random.PRNGKey(0)\n","init_key, dropout_key = random.split(key)\n","vars = time_series_regressor.init(\n","    {\"params\": init_key, \"dropout\": dropout_key}, batch, deterministic=False\n",")\n","dropout_key, original_dropout_key = random.split(dropout_key)\n","x = time_series_regressor.apply(\n","    vars, batch, deterministic=False, rngs={\"dropout\": dropout_key}\n",")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                   SimplePred Summary                                                   </span>\n","┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> path                  </span>┃<span style=\"font-weight: bold\"> module                </span>┃<span style=\"font-weight: bold\"> inputs                </span>┃<span style=\"font-weight: bold\"> outputs               </span>┃<span style=\"font-weight: bold\"> params               </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n","│                       │ SimplePred            │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26]      │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26]      │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ TimeSeriesTransformer │ deterministic: False  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                      │\n","│                       │                       │ numeric_inputs:       │                       │                      │\n","│                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26]      │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Embed                 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">int32</span>[26]             │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[26,256]       │ embedding:           │\n","│                       │                       │                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[29,256]      │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">7,424 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(29.7 KB)</span>      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ PositionalEncoding    │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,256]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ TransformerBlock      │ deterministic: False  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                      │\n","│                       │                       │ x:                    │                       │                      │\n","│                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ MultiHeadDotProductA… │ -                     │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                      │\n","│                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                       │                      │\n","│                       │                       │ - deterministic:      │                       │                      │\n","│                       │                       │ False                 │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ DenseGeneral          │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,4,68] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,68]  │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272,4,68]    │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">74,256 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(297.0 KB)</span>    │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ DenseGeneral          │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,4,68] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,68]  │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272,4,68]    │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">74,256 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(297.0 KB)</span>    │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ DenseGeneral          │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,4,68] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,68]  │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272,4,68]    │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">74,256 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(297.0 KB)</span>    │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ DenseGeneral          │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,4,68] │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272]   │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,68,272]    │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">74,256 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(297.0 KB)</span>    │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ LayerNorm             │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272]   │\n","│                       │                       │                       │                       │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272]  │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">544 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(2.2 KB)</span>         │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ FeedForwardNetwork    │ -                     │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                      │\n","│                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                       │                      │\n","│                       │                       │ - deterministic:      │                       │                      │\n","│                       │                       │ False                 │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Dense                 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,64]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]    │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272,64]      │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">17,472 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(69.9 KB)</span>     │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Dropout               │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,64] │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,64]   │                      │\n","│                       │                       │ - deterministic:      │                       │                      │\n","│                       │                       │ False                 │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Dense                 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,64]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272]   │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64,272]      │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">17,680 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(70.7 KB)</span>     │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Dropout               │ -                     │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                      │\n","│                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                       │                      │\n","│                       │                       │ - deterministic:      │                       │                      │\n","│                       │                       │ False                 │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ LayerNorm             │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272]   │\n","│                       │                       │                       │                       │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272]  │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">544 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(2.2 KB)</span>         │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ TransformerBlock      │ deterministic: False  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                      │\n","│                       │                       │ x:                    │                       │                      │\n","│                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ MultiHeadDotProductA… │ -                     │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                      │\n","│                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                       │                      │\n","│                       │                       │ - deterministic:      │                       │                      │\n","│                       │                       │ False                 │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ DenseGeneral          │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,4,68] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,68]  │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272,4,68]    │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">74,256 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(297.0 KB)</span>    │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ DenseGeneral          │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,4,68] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,68]  │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272,4,68]    │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">74,256 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(297.0 KB)</span>    │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ DenseGeneral          │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,4,68] │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,68]  │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272,4,68]    │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">74,256 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(297.0 KB)</span>    │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ DenseGeneral          │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,4,68] │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272]   │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,68,272]    │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">74,256 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(297.0 KB)</span>    │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ LayerNorm             │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272]   │\n","│                       │                       │                       │                       │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272]  │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">544 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(2.2 KB)</span>         │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ FeedForwardNetwork    │ -                     │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                      │\n","│                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                       │                      │\n","│                       │                       │ - deterministic:      │                       │                      │\n","│                       │                       │ False                 │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Dense                 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,64]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64]    │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272,64]      │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">17,472 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(69.9 KB)</span>     │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Dropout               │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,64] │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,64]   │                      │\n","│                       │                       │ - deterministic:      │                       │                      │\n","│                       │                       │ False                 │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Dense                 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,64]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272]   │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[64,272]      │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">17,680 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(70.7 KB)</span>     │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Dropout               │ -                     │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                      │\n","│                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │                       │                      │\n","│                       │                       │ - deterministic:      │                       │                      │\n","│                       │                       │ False                 │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ LayerNorm             │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272]   │\n","│                       │                       │                       │                       │ scale: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272]  │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">544 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(2.2 KB)</span>         │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ RegressionOutputChain │ Sequential            │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,1]    │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ RegressionDense1      │ Dense                 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,272]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,512]  │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512]   │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[272,512]     │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">139,776 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(559.1 KB)</span>   │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ RegressionDense2      │ Dense                 │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,512]  │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,59,26,1]    │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[1]     │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[512,1]       │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ <span style=\"font-weight: bold\">513 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(2.1 KB)</span>         │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│<span style=\"font-weight: bold\">                       </span>│<span style=\"font-weight: bold\">                       </span>│<span style=\"font-weight: bold\">                       </span>│<span style=\"font-weight: bold\">                 Total </span>│<span style=\"font-weight: bold\"> 814,241 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(3.3 MB)</span><span style=\"font-weight: bold\">     </span>│\n","└───────────────────────┴───────────────────────┴───────────────────────┴───────────────────────┴──────────────────────┘\n","<span style=\"font-weight: bold\">                                                                                                                        </span>\n","<span style=\"font-weight: bold\">                                           Total Parameters: 814,241 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(3.3 MB)</span><span style=\"font-weight: bold\">                                           </span>\n","</pre>\n"],"text/plain":["\u001b[3m                                                   SimplePred Summary                                                   \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mpath                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs              \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams              \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n","│                       │ SimplePred            │ \u001b[2mfloat32\u001b[0m[4,59,26]      │ \u001b[2mfloat32\u001b[0m[4,59,26]      │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ TimeSeriesTransformer │ deterministic: False  │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                      │\n","│                       │                       │ numeric_inputs:       │                       │                      │\n","│                       │                       │ \u001b[2mfloat32\u001b[0m[4,59,26]      │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Embed                 │ \u001b[2mint32\u001b[0m[26]             │ \u001b[2mfloat32\u001b[0m[26,256]       │ embedding:           │\n","│                       │                       │                       │                       │ \u001b[2mfloat32\u001b[0m[29,256]      │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m7,424 \u001b[0m\u001b[1;2m(29.7 KB)\u001b[0m      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ PositionalEncoding    │ \u001b[2mfloat32\u001b[0m[4,59,26,256]  │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ TransformerBlock      │ deterministic: False  │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                      │\n","│                       │                       │ x:                    │                       │                      │\n","│                       │                       │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ MultiHeadDotProductA… │ -                     │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                      │\n","│                       │                       │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                       │                      │\n","│                       │                       │ - deterministic:      │                       │                      │\n","│                       │                       │ False                 │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ DenseGeneral          │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ \u001b[2mfloat32\u001b[0m[4,59,26,4,68] │ bias: \u001b[2mfloat32\u001b[0m[4,68]  │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ \u001b[2mfloat32\u001b[0m[272,4,68]    │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m74,256 \u001b[0m\u001b[1;2m(297.0 KB)\u001b[0m    │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ DenseGeneral          │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ \u001b[2mfloat32\u001b[0m[4,59,26,4,68] │ bias: \u001b[2mfloat32\u001b[0m[4,68]  │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ \u001b[2mfloat32\u001b[0m[272,4,68]    │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m74,256 \u001b[0m\u001b[1;2m(297.0 KB)\u001b[0m    │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ DenseGeneral          │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ \u001b[2mfloat32\u001b[0m[4,59,26,4,68] │ bias: \u001b[2mfloat32\u001b[0m[4,68]  │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ \u001b[2mfloat32\u001b[0m[272,4,68]    │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m74,256 \u001b[0m\u001b[1;2m(297.0 KB)\u001b[0m    │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ DenseGeneral          │ \u001b[2mfloat32\u001b[0m[4,59,26,4,68] │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ bias: \u001b[2mfloat32\u001b[0m[272]   │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ \u001b[2mfloat32\u001b[0m[4,68,272]    │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m74,256 \u001b[0m\u001b[1;2m(297.0 KB)\u001b[0m    │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ LayerNorm             │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ bias: \u001b[2mfloat32\u001b[0m[272]   │\n","│                       │                       │                       │                       │ scale: \u001b[2mfloat32\u001b[0m[272]  │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m544 \u001b[0m\u001b[1;2m(2.2 KB)\u001b[0m         │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ FeedForwardNetwork    │ -                     │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                      │\n","│                       │                       │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                       │                      │\n","│                       │                       │ - deterministic:      │                       │                      │\n","│                       │                       │ False                 │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Dense                 │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ \u001b[2mfloat32\u001b[0m[4,59,26,64]   │ bias: \u001b[2mfloat32\u001b[0m[64]    │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ \u001b[2mfloat32\u001b[0m[272,64]      │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m17,472 \u001b[0m\u001b[1;2m(69.9 KB)\u001b[0m     │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Dropout               │ - \u001b[2mfloat32\u001b[0m[4,59,26,64] │ \u001b[2mfloat32\u001b[0m[4,59,26,64]   │                      │\n","│                       │                       │ - deterministic:      │                       │                      │\n","│                       │                       │ False                 │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Dense                 │ \u001b[2mfloat32\u001b[0m[4,59,26,64]   │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ bias: \u001b[2mfloat32\u001b[0m[272]   │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ \u001b[2mfloat32\u001b[0m[64,272]      │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m17,680 \u001b[0m\u001b[1;2m(70.7 KB)\u001b[0m     │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Dropout               │ -                     │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                      │\n","│                       │                       │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                       │                      │\n","│                       │                       │ - deterministic:      │                       │                      │\n","│                       │                       │ False                 │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ LayerNorm             │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ bias: \u001b[2mfloat32\u001b[0m[272]   │\n","│                       │                       │                       │                       │ scale: \u001b[2mfloat32\u001b[0m[272]  │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m544 \u001b[0m\u001b[1;2m(2.2 KB)\u001b[0m         │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ TransformerBlock      │ deterministic: False  │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                      │\n","│                       │                       │ x:                    │                       │                      │\n","│                       │                       │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ MultiHeadDotProductA… │ -                     │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                      │\n","│                       │                       │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                       │                      │\n","│                       │                       │ - deterministic:      │                       │                      │\n","│                       │                       │ False                 │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ DenseGeneral          │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ \u001b[2mfloat32\u001b[0m[4,59,26,4,68] │ bias: \u001b[2mfloat32\u001b[0m[4,68]  │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ \u001b[2mfloat32\u001b[0m[272,4,68]    │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m74,256 \u001b[0m\u001b[1;2m(297.0 KB)\u001b[0m    │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ DenseGeneral          │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ \u001b[2mfloat32\u001b[0m[4,59,26,4,68] │ bias: \u001b[2mfloat32\u001b[0m[4,68]  │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ \u001b[2mfloat32\u001b[0m[272,4,68]    │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m74,256 \u001b[0m\u001b[1;2m(297.0 KB)\u001b[0m    │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ DenseGeneral          │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ \u001b[2mfloat32\u001b[0m[4,59,26,4,68] │ bias: \u001b[2mfloat32\u001b[0m[4,68]  │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ \u001b[2mfloat32\u001b[0m[272,4,68]    │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m74,256 \u001b[0m\u001b[1;2m(297.0 KB)\u001b[0m    │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ DenseGeneral          │ \u001b[2mfloat32\u001b[0m[4,59,26,4,68] │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ bias: \u001b[2mfloat32\u001b[0m[272]   │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ \u001b[2mfloat32\u001b[0m[4,68,272]    │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m74,256 \u001b[0m\u001b[1;2m(297.0 KB)\u001b[0m    │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ LayerNorm             │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ bias: \u001b[2mfloat32\u001b[0m[272]   │\n","│                       │                       │                       │                       │ scale: \u001b[2mfloat32\u001b[0m[272]  │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m544 \u001b[0m\u001b[1;2m(2.2 KB)\u001b[0m         │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ FeedForwardNetwork    │ -                     │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                      │\n","│                       │                       │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                       │                      │\n","│                       │                       │ - deterministic:      │                       │                      │\n","│                       │                       │ False                 │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Dense                 │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ \u001b[2mfloat32\u001b[0m[4,59,26,64]   │ bias: \u001b[2mfloat32\u001b[0m[64]    │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ \u001b[2mfloat32\u001b[0m[272,64]      │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m17,472 \u001b[0m\u001b[1;2m(69.9 KB)\u001b[0m     │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Dropout               │ - \u001b[2mfloat32\u001b[0m[4,59,26,64] │ \u001b[2mfloat32\u001b[0m[4,59,26,64]   │                      │\n","│                       │                       │ - deterministic:      │                       │                      │\n","│                       │                       │ False                 │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Dense                 │ \u001b[2mfloat32\u001b[0m[4,59,26,64]   │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ bias: \u001b[2mfloat32\u001b[0m[272]   │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ \u001b[2mfloat32\u001b[0m[64,272]      │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m17,680 \u001b[0m\u001b[1;2m(70.7 KB)\u001b[0m     │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ Dropout               │ -                     │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                      │\n","│                       │                       │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │                       │                      │\n","│                       │                       │ - deterministic:      │                       │                      │\n","│                       │                       │ False                 │                       │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ TimeSeriesTransforme… │ LayerNorm             │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ bias: \u001b[2mfloat32\u001b[0m[272]   │\n","│                       │                       │                       │                       │ scale: \u001b[2mfloat32\u001b[0m[272]  │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m544 \u001b[0m\u001b[1;2m(2.2 KB)\u001b[0m         │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ RegressionOutputChain │ Sequential            │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ \u001b[2mfloat32\u001b[0m[4,59,26,1]    │                      │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ RegressionDense1      │ Dense                 │ \u001b[2mfloat32\u001b[0m[4,59,26,272]  │ \u001b[2mfloat32\u001b[0m[4,59,26,512]  │ bias: \u001b[2mfloat32\u001b[0m[512]   │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ \u001b[2mfloat32\u001b[0m[272,512]     │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m139,776 \u001b[0m\u001b[1;2m(559.1 KB)\u001b[0m   │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│ RegressionDense2      │ Dense                 │ \u001b[2mfloat32\u001b[0m[4,59,26,512]  │ \u001b[2mfloat32\u001b[0m[4,59,26,1]    │ bias: \u001b[2mfloat32\u001b[0m[1]     │\n","│                       │                       │                       │                       │ kernel:              │\n","│                       │                       │                       │                       │ \u001b[2mfloat32\u001b[0m[512,1]       │\n","│                       │                       │                       │                       │                      │\n","│                       │                       │                       │                       │ \u001b[1m513 \u001b[0m\u001b[1;2m(2.1 KB)\u001b[0m         │\n","├───────────────────────┼───────────────────────┼───────────────────────┼───────────────────────┼──────────────────────┤\n","│\u001b[1m \u001b[0m\u001b[1m                     \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                     \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                     \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m814,241 \u001b[0m\u001b[1;2m(3.3 MB)\u001b[0m\u001b[1m    \u001b[0m\u001b[1m \u001b[0m│\n","└───────────────────────┴───────────────────────┴───────────────────────┴───────────────────────┴──────────────────────┘\n","\u001b[1m                                                                                                                        \u001b[0m\n","\u001b[1m                                           Total Parameters: 814,241 \u001b[0m\u001b[1;2m(3.3 MB)\u001b[0m\u001b[1m                                           \u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["'\\n\\n'"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["time_series_regressor.tabulate(\n","    {\"params\": init_key, \"dropout\": dropout_key},\n","    batch,\n","    console_kwargs={\"force_jupyter\": True, \"width\": 120},\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Memory of custom: 5.35 MB with 1,337,441 parameters\n","Memory of nn.Attention: 5.35 MB, with 1,337,441 parameters\n"]}],"source":["def calculate_memory_footprint(params):\n","    \"\"\"Calculate total memory footprint of JAX model parameters and total\n","    number of parameters.\"\"\"\n","    total_bytes = 0\n","    # Flatten the parameter tree structure into a list of arrays\n","    flat_params, _ = tree_flatten(params)\n","    for param in flat_params:\n","        # Calculate bytes: number of elements * size of each element\n","        bytes_per_param = param.size * param.dtype.itemsize\n","        total_bytes += bytes_per_param\n","    return total_bytes\n","\n","\n","def count_parameters(params):\n","    return sum(jnp.prod(jnp.array(p.shape)) for p in jax.tree_util.tree_leaves(params))\n","\n","\n","mem = calculate_memory_footprint(vars)\n","total_params = count_parameters(vars)\n","\n","\n","print(f\"Memory of custom: {mem / 1e6:.2f} MB with {total_params:,} parameters\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["(4, 59, 26)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["batch.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_causal_mask(tensor: jnp.ndarray):\n","    \"\"\"Create a causal mask to mask out future values.\"\"\"\n","    mask = jnp.tril(jnp.ones((tensor.shape[0], tensor.shape[1])))\n","    return mask\n","\n","\n","def create_padding_mask(tensor: jnp.ndarray):\n","    \"\"\"Create a padding mask to mask out padded values.\"\"\"\n","    mask = jnp.isnan(tensor)\n","    return mask\n","\n","\n","def mask_array(tensor: jnp.array):\n","    \"\"\"Create a mask for the tensor\"\"\"\n","    causal_mask = create_causal_mask(tensor)\n","    padding_mask = create_padding_mask(tensor)\n","    mask = jnp.logical_or(causal_mask, padding_mask)\n","    return mask"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["(59, 26)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["jnp.array(train_ds[0]).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["4dd64f565fcf4b259ef24944493477bf","25ce5408138f4cb28e163f1fdffdee5f","47793d3155614c8cbd7d3337dcb2f895","f9ec11073e484927a4cc1ee2e3da132f","62e750769d364ab2b80d5b9646b10ea6","8b35a70a3b9e4f3b87cf2a7280439ac6","0e26d28846fb449789510e4748c01c6a","f8093c8ddeec4c55ac6cc5f8f7df30bf","4151c7353de54909aa9deecbe8c1e1e1","9ced4485d5c641eda90ab0634e0691d3","6f5ca39406174a4fb1bee9eb1e35fccb","5e15315a77864badafe48c2baf31b030","dfd9dd713862479bba73b6c0a12a1902","88be535ce5a945f4979fe72d9f086365","31d37adf891a4da68675945509051210","4da9993585914618a35d8d5382fef850","acee2c1ed91e44188d5ca40bddace4b1","cc57eeb92d32434ca82b7c3f669865d1","f8382d8bbb7f42f6a0f4d4028203615f","851b29291123491a821b1ce8088ca785","055681c8159b4b6c8104d4e06279803c","9f5e5843559b4f9e8f3440ebd85743f8","a317a54aa1a349f490e388aacb2d1e4d","c470a61e692a4459988f63f0024f7eac","5edd903b07594d128ded9b4844835159","2ac0cceac38e43adb00a10b778fad2df","c7f61bdee3ca4ca0a3f7d021aa558deb","70c7b641da5c4b82bbd5b2e1750f3b39","462d9e93a94f44868fec1ece82f0a241","4bc81f6d94394d7f90e070d8b11a7059","d3bc58d04d5f42a5a24bd467a9569e01","ddb1791795134ac0a754748f9793c214","d95aea2b915b44f38b5090ee186abafd","01ca2dcb14e647dfb1e68750ca6de39c","a51aa7b827db4f768dce6315dbebf379","634935ff0a6b4d65b72f87359d9f82fc","09b63c98bee543dfb557a007d50c41d1","dc4a8a3b0a8b4ef49dcd6269a1b32e14","5f1fb4d40e154e27baa0d4f330df540e","948787e6434f419a86d9d7da831e2572","e4de75777bba4d72b61936baf3d84cbb","0481743e80634f7a8e718fb528950e54","2fb36a08ca1540228919af722727572c","3bd47d84fac442d4bcf49dceeb699d45"]},"executionInfo":{"elapsed":841688,"status":"ok","timestamp":1708904334771,"user":{"displayName":"Kai Lukowiak","userId":"12340107642472090190"},"user_tz":420},"id":"RIXu3GkdYzNA","outputId":"dee6137d-c096-4df6-fbb4-baa3764d359e"},"outputs":[],"source":["mts_root_key = random.PRNGKey(44)\n","mts_main_key, ts_params_key, ts_data_key = random.split(mts_root_key, 3)\n","\n","\n","def clip_gradients(gradients, max_norm):\n","    total_norm = jnp.sqrt(sum(jnp.sum(jnp.square(grad)) for grad in gradients.values()))\n","    scale = max_norm / (total_norm + 1e-6)\n","    clipped_gradients = jax.tree_map(\n","        lambda grad: jnp.where(total_norm > max_norm, grad * scale, grad), gradients\n","    )\n","    return clipped_gradients\n","\n","\n","def base_loss(inputs, outputs):\n","    # Create mask for nan inputs\n","    nan_mask = jnp.isnan(inputs)\n","    inputs = jnp.where(nan_mask, jnp.zeros_like(inputs), inputs)\n","    outputs = jnp.where(nan_mask, jnp.zeros_like(outputs), outputs)\n","\n","    raw_loss = optax.squared_error(outputs, inputs)\n","    masked_loss = jnp.where(nan_mask, 0.0, raw_loss)\n","    loss = masked_loss.sum() / (~nan_mask).sum()\n","\n","    return loss\n","\n","\n","def calculate_loss(params, state, inputs, dataset: SimpleDS, dropout_key, mask_key):\n","    outputs = state.apply_fn(\n","        {\"params\": params},\n","        hp.mask_tensor(inputs, dataset, prng_key=mask_key),\n","        rngs={\"dropout\": dropout_key},\n","    )\n","    loss = base_loss(inputs, outputs)\n","    # Create mask for nan inputs\n","\n","    return loss\n","\n","\n","def evaluate(params, state, inputs, dataset: SimpleDS):\n","    outputs = state.apply_fn(\n","        {\"params\": params},\n","        hp.mask_tensor(inputs, dataset, prng_key=ts_data_key),\n","    )\n","    loss = base_loss(inputs, outputs)\n","    return loss\n","\n","\n","@jax.jit\n","def eval_step(state: train_state.TrainState, batch, mask_key):\n","    def loss_fn(params):\n","        return evaluate(params, state, batch, train_ds)\n","\n","    # (loss, individual_losses), grad = grad_fn(state.params)\n","    loss = loss_fn(state.params)\n","    return loss\n","\n","\n","@jax.jit\n","def train_step(state: train_state.TrainState, batch, base_key):\n","    dropout_key, mask_key, new_key = jax.random.split(base_key, 3)\n","    def loss_fn(params):\n","        return calculate_loss(params, state, batch, train_ds, dropout_key, mask_key)\n","\n","    grad_fn = jax.value_and_grad(loss_fn)\n","\n","    # (loss, individual_losses), grad = grad_fn(state.params)\n","    loss, grad = grad_fn(state.params)\n","    # grad = replace_nans(grad)\n","    # grad = clip_gradients(grad, 1.0)\n","    state = state.apply_gradients(grads=grad)\n","\n","    return state, loss, new_key\n","\n","\n","def create_train_state(model, prng, batch, lr):\n","    params = model.init(prng, batch)\n","    # optimizer = optax.chain(optax.adam(lr))\n","    optimizer = optax.chain(optax.clip_by_global_norm(0.4), optax.adam(lr))\n","    # optimizer_state = optimizer.init(params)\n","    return train_state.TrainState.create(\n","        apply_fn=model.apply,\n","        params=params[\"params\"],\n","        tx=optimizer,\n","        # tx_state=optimizer_state,\n","    )\n","\n","\n","batch_size = 2\n","# batch = train_ds[0]\n","# state = create_train_state(time_series_regressor, mts_main_key, batch, 0.0001)\n","state = create_train_state(time_series_regressor, mts_main_key, batch, 0.0001)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d239eade74bd4fc1a827c11cbd18f42f","version_major":2,"version_minor":0},"text/plain":["epochs for runs/2024-05-20T03:05:03nnAttentionWithBlock:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c014c24dd52348329008eeab0378ca11","version_major":2,"version_minor":0},"text/plain":["batches:   0%|          | 0/196 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d05b778ad414e1e822c1d27556aec10","version_major":2,"version_minor":0},"text/plain":["batches:   0%|          | 0/196 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["writer_name = \"NewBlock\"\n","\n","writer_time = dt.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n","\n","train_summary_writer = SummaryWriter(\"runs/\" + writer_time + writer_name)\n","\n","\n","test_set_key = random.PRNGKey(4454)\n","\n","train_data_loader = DataLoader(train_ds, batch_size=512, shuffle=True)\n","test_data_loader = DataLoader(test_ds, batch_size=512, shuffle=True)\n","batch_count = 0\n","for j in trange(2, desc=f\"epochs for {train_summary_writer.log_dir}\"):\n","    # arrs = train_data_loader()\n","    for i in tqdm(train_data_loader, leave=False, desc=\"batches\"):\n","        # for i in trange(len(pre_train) // batch_size, leave=False):\n","        # for i in trange(len(pre_train) // batch_size //10, leave=False):\n","        # batch = make_batch(train_ds, i[0], 4)\n","\n","        state, loss = train_step(state, jnp.array(i))\n","        if jnp.isnan(loss):\n","            raise ValueError(\"Nan Value in loss, stopping\")\n","        batch_count += 1\n","\n","        if batch_count % 1 == 0:\n","            train_summary_writer.add_scalar(\n","                \"loss/loss\", np.array(loss.item()), batch_count\n","            )\n","        if batch_count % 10 == 0:\n","            test_loss = eval_step(state, jnp.array(next(iter(test_data_loader))))\n","            train_summary_writer.add_scalar(\n","                \"loss/test_loss\", np.array(test_loss.item()), batch_count\n","            )\n","\n","train_summary_writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0;31mInit signature:\u001b[0m\n","\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mnum_heads\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupportsDType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mparam_dtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupportsDType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'jax.numpy.float32'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mqkv_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mout_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mbroadcast_dropout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdropout_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mprecision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNoneType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mkernel_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mvariance_scaling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x710a44331000\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mbias_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mzeros\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x710afc8d0ee0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mattention_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mdot_product_attention\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x710a443743a0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdecode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mnormalize_qk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mqkv_dot_general\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mout_dot_general\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mqkv_dot_general_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mout_dot_general_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mparent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Sentinel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Sentinel\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x710a4431e500\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDocstring:\u001b[0m     \n","Multi-head dot-product attention.\n","Alias for ``MultiHeadDotProductAttention``.\n","\n","**NOTE**: ``MultiHeadAttention`` is a wrapper of ``MultiHeadDotProductAttention``,\n","and so their implementations are identical. However ``MultiHeadAttention`` layers\n","will, by default, be named ``MultiHeadAttention_{index}``, whereas ``MultiHeadDotProductAttention``\n","will be named ``MultiHeadDotProductAttention_{index}``. Therefore, this could affect\n","checkpointing, param collection names and RNG threading (since the layer name is\n","used when generating new RNG's) within the module.\n","\n","Example usage::\n","\n","  >>> import flax.linen as nn\n","  >>> import jax\n","\n","  >>> layer = nn.MultiHeadAttention(num_heads=8, qkv_features=16)\n","  >>> key1, key2, key3, key4, key5, key6 = jax.random.split(jax.random.key(0), 6)\n","  >>> shape = (4, 3, 2, 5)\n","  >>> q, k, v = jax.random.uniform(key1, shape), jax.random.uniform(key2, shape), jax.random.uniform(key3, shape)\n","  >>> variables = layer.init(jax.random.key(0), q)\n","\n","  >>> # different inputs for inputs_q, inputs_k and inputs_v\n","  >>> out = layer.apply(variables, q, k, v)\n","  >>> # equivalent to layer.apply(variables, inputs_q=q, inputs_k=k, inputs_v=k)\n","  >>> out = layer.apply(variables, q, k)\n","  >>> # equivalent to layer.apply(variables, inputs_q=q, inputs_k=q) and layer.apply(variables, inputs_q=q, inputs_k=q, inputs_v=q)\n","  >>> out = layer.apply(variables, q)\n","\n","  >>> attention_kwargs = dict(\n","  ...     num_heads=8,\n","  ...     qkv_features=16,\n","  ...     kernel_init=nn.initializers.ones,\n","  ...     bias_init=nn.initializers.zeros,\n","  ...     dropout_rate=0.5,\n","  ...     deterministic=False,\n","  ...     )\n","  >>> class Module(nn.Module):\n","  ...   attention_kwargs: dict\n","  ...\n","  ...   @nn.compact\n","  ...   def __call__(self, x, dropout_rng=None):\n","  ...     out1 = nn.MultiHeadAttention(**self.attention_kwargs)(x, dropout_rng=dropout_rng)\n","  ...     out2 = nn.MultiHeadAttention(**self.attention_kwargs)(x, dropout_rng=dropout_rng)\n","  ...     return out1, out2\n","  >>> module = Module(attention_kwargs)\n","  >>> variables = module.init({'params': key1, 'dropout': key2}, q)\n","\n","  >>> # out1 and out2 are different.\n","  >>> out1, out2 = module.apply(variables, q, rngs={'dropout': key3})\n","  >>> # out3 and out4 are different.\n","  >>> # out1 and out3 are different. out2 and out4 are different.\n","  >>> out3, out4 = module.apply(variables, q, rngs={'dropout': key4})\n","  >>> # out1 and out2 are the same.\n","  >>> out1, out2 = module.apply(variables, q, dropout_rng=key5)\n","  >>> # out1 and out2 are the same as out3 and out4.\n","  >>> # providing a `dropout_rng` arg will take precedence over the `rngs` arg in `.apply`\n","  >>> out3, out4 = module.apply(variables, q, rngs={'dropout': key6}, dropout_rng=key5)\n","\n","Attributes:\n","  num_heads: number of attention heads. Features (i.e. inputs_q.shape[-1])\n","    should be divisible by the number of heads.\n","  dtype: the dtype of the computation (default: infer from inputs and params)\n","  param_dtype: the dtype passed to parameter initializers (default: float32)\n","  qkv_features: dimension of the key, query, and value.\n","  out_features: dimension of the last projection\n","  broadcast_dropout: bool: use a broadcasted dropout along batch dims.\n","  dropout_rate: dropout rate\n","  deterministic: if false, the attention weight is masked randomly using\n","    dropout, whereas if true, the attention weights are deterministic.\n","  precision: numerical precision of the computation see ``jax.lax.Precision``\n","    for details.\n","  kernel_init: initializer for the kernel of the Dense layers.\n","  bias_init: initializer for the bias of the Dense layers.\n","  use_bias: bool: whether pointwise QKVO dense transforms use bias.\n","  attention_fn: dot_product_attention or compatible function. Accepts query,\n","    key, value, and returns output of shape ``[bs, dim1, dim2, ..., dimN,,\n","    num_heads, value_channels]``\n","  decode: whether to prepare and use an autoregressive cache.\n","  normalize_qk: should QK normalization be applied (arxiv.org/abs/2302.05442).\n","\u001b[0;31mFile:\u001b[0m           ~/environment/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py\n","\u001b[0;31mType:\u001b[0m           type\n","\u001b[0;31mSubclasses:\u001b[0m     "]}],"source":["import flax.linen as nn\n","\n","?nn.MultiHeadAttention"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from jax import random\n","\n","test_key = jax.random.key(12)\n","att = random.normal(test_key, (2, 10, 6, 16))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["Array([[ 0.10210431,  1.6902981 , -0.7320057 , ...,  1.3139411 ,\n","         1.3139411 ,  1.3139411 ],\n","       [ 0.50146335,  1.7779434 , -0.4218275 , ...,  1.3701122 ,\n","         1.3701122 ,  1.3701122 ],\n","       [ 0.94414103,  1.7903221 , -0.04172196, ...,  1.3825907 ,\n","         1.3825907 ,  1.3825907 ],\n","       ...,\n","       [ 5.3527484 ,  5.3527484 ,  5.3527484 , ...,  5.3527484 ,\n","         5.3527484 ,  5.3527484 ],\n","       [ 5.198748  ,  5.198748  ,  5.198748  , ...,  5.198748  ,\n","         5.198748  ,  5.198748  ],\n","       [ 4.5573378 ,  4.5573378 ,  4.5573378 , ...,  4.5573378 ,\n","         4.5573378 ,  4.5573378 ]], dtype=float32)"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["test_result = jnp.squeeze(\n","    state.apply_fn({\"params\": state.params}, jnp.array([train_ds[0]]))\n",")\n","test_result"]},{"cell_type":"code","execution_count":null,"metadata":{"notebookRunGroups":{"groupValue":""}},"outputs":[{"data":{"text/plain":["((59, 26), (59, 26))"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["jnp.array(train_ds[0]).shape, test_result.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["(59, 26)"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["jnp.squeeze(test_result).shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_pred = pd.DataFrame(test_result)\n","df_actual = pd.DataFrame(train_ds[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.102104</td>\n","      <td>1.690298</td>\n","      <td>-0.732006</td>\n","      <td>0.847111</td>\n","      <td>0.798189</td>\n","      <td>1.040970</td>\n","      <td>2.628688</td>\n","      <td>1.239484</td>\n","      <td>1.978418</td>\n","      <td>1.943971</td>\n","      <td>...</td>\n","      <td>1.313941</td>\n","      <td>1.313941</td>\n","      <td>1.313941</td>\n","      <td>1.313941</td>\n","      <td>1.313941</td>\n","      <td>1.313941</td>\n","      <td>1.313941</td>\n","      <td>1.313941</td>\n","      <td>1.313941</td>\n","      <td>1.313941</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.501463</td>\n","      <td>1.777943</td>\n","      <td>-0.421827</td>\n","      <td>0.414215</td>\n","      <td>1.039330</td>\n","      <td>0.775214</td>\n","      <td>2.701208</td>\n","      <td>1.304612</td>\n","      <td>2.060616</td>\n","      <td>1.984488</td>\n","      <td>...</td>\n","      <td>1.370112</td>\n","      <td>1.370112</td>\n","      <td>1.370112</td>\n","      <td>1.370112</td>\n","      <td>1.370112</td>\n","      <td>1.370112</td>\n","      <td>1.370112</td>\n","      <td>1.370112</td>\n","      <td>1.370112</td>\n","      <td>1.370112</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.944141</td>\n","      <td>1.790322</td>\n","      <td>-0.041722</td>\n","      <td>-0.014226</td>\n","      <td>1.098226</td>\n","      <td>0.527439</td>\n","      <td>2.716042</td>\n","      <td>1.333475</td>\n","      <td>2.067372</td>\n","      <td>1.990813</td>\n","      <td>...</td>\n","      <td>1.382591</td>\n","      <td>1.382591</td>\n","      <td>1.382591</td>\n","      <td>1.382591</td>\n","      <td>1.382591</td>\n","      <td>1.382591</td>\n","      <td>1.382591</td>\n","      <td>1.382591</td>\n","      <td>1.382591</td>\n","      <td>1.382591</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.406419</td>\n","      <td>1.771376</td>\n","      <td>0.328494</td>\n","      <td>-0.468852</td>\n","      <td>0.900589</td>\n","      <td>0.234735</td>\n","      <td>2.559814</td>\n","      <td>1.310214</td>\n","      <td>1.993624</td>\n","      <td>2.038106</td>\n","      <td>...</td>\n","      <td>1.440563</td>\n","      <td>1.440563</td>\n","      <td>1.440563</td>\n","      <td>1.440563</td>\n","      <td>1.440563</td>\n","      <td>1.440563</td>\n","      <td>1.440563</td>\n","      <td>1.440563</td>\n","      <td>1.440563</td>\n","      <td>1.440563</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.862925</td>\n","      <td>1.687651</td>\n","      <td>0.708704</td>\n","      <td>-0.807441</td>\n","      <td>0.597644</td>\n","      <td>-0.047129</td>\n","      <td>2.610654</td>\n","      <td>1.325231</td>\n","      <td>2.038662</td>\n","      <td>2.025807</td>\n","      <td>...</td>\n","      <td>1.370153</td>\n","      <td>1.370153</td>\n","      <td>1.370153</td>\n","      <td>1.370153</td>\n","      <td>1.370153</td>\n","      <td>1.370153</td>\n","      <td>1.370153</td>\n","      <td>1.370153</td>\n","      <td>1.370153</td>\n","      <td>1.370153</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2.451734</td>\n","      <td>1.575453</td>\n","      <td>0.996607</td>\n","      <td>-0.920642</td>\n","      <td>0.186981</td>\n","      <td>-0.310957</td>\n","      <td>2.704166</td>\n","      <td>1.389927</td>\n","      <td>2.059216</td>\n","      <td>1.951594</td>\n","      <td>...</td>\n","      <td>1.416731</td>\n","      <td>1.416731</td>\n","      <td>1.416731</td>\n","      <td>1.416731</td>\n","      <td>1.416731</td>\n","      <td>1.416731</td>\n","      <td>1.416731</td>\n","      <td>1.416731</td>\n","      <td>1.416731</td>\n","      <td>1.416731</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2.912332</td>\n","      <td>1.383239</td>\n","      <td>1.314543</td>\n","      <td>-0.802508</td>\n","      <td>-0.317605</td>\n","      <td>-0.643567</td>\n","      <td>2.750806</td>\n","      <td>1.344543</td>\n","      <td>2.028069</td>\n","      <td>1.945646</td>\n","      <td>...</td>\n","      <td>1.416792</td>\n","      <td>1.416792</td>\n","      <td>1.416792</td>\n","      <td>1.416792</td>\n","      <td>1.416792</td>\n","      <td>1.416792</td>\n","      <td>1.416792</td>\n","      <td>1.416792</td>\n","      <td>1.416792</td>\n","      <td>1.416792</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>3.326980</td>\n","      <td>1.158393</td>\n","      <td>1.570038</td>\n","      <td>-0.575280</td>\n","      <td>-0.735511</td>\n","      <td>-0.914323</td>\n","      <td>2.736963</td>\n","      <td>1.323641</td>\n","      <td>2.054521</td>\n","      <td>2.018994</td>\n","      <td>...</td>\n","      <td>1.380926</td>\n","      <td>1.380926</td>\n","      <td>1.380926</td>\n","      <td>1.380926</td>\n","      <td>1.380926</td>\n","      <td>1.380926</td>\n","      <td>1.380926</td>\n","      <td>1.380926</td>\n","      <td>1.380926</td>\n","      <td>1.380926</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>3.833704</td>\n","      <td>0.878748</td>\n","      <td>1.732224</td>\n","      <td>-0.178839</td>\n","      <td>-0.916972</td>\n","      <td>-1.126539</td>\n","      <td>2.614489</td>\n","      <td>1.275802</td>\n","      <td>2.033884</td>\n","      <td>1.982288</td>\n","      <td>...</td>\n","      <td>1.306998</td>\n","      <td>1.306998</td>\n","      <td>1.306998</td>\n","      <td>1.306998</td>\n","      <td>1.306998</td>\n","      <td>1.306998</td>\n","      <td>1.306998</td>\n","      <td>1.306998</td>\n","      <td>1.306998</td>\n","      <td>1.306998</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>4.233853</td>\n","      <td>0.607751</td>\n","      <td>1.860583</td>\n","      <td>0.321860</td>\n","      <td>-0.945738</td>\n","      <td>-1.337776</td>\n","      <td>2.339664</td>\n","      <td>1.270190</td>\n","      <td>1.970824</td>\n","      <td>1.993912</td>\n","      <td>...</td>\n","      <td>1.225813</td>\n","      <td>1.225813</td>\n","      <td>1.225813</td>\n","      <td>1.225813</td>\n","      <td>1.225813</td>\n","      <td>1.225813</td>\n","      <td>1.225813</td>\n","      <td>1.225813</td>\n","      <td>1.225813</td>\n","      <td>1.225813</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>4.685832</td>\n","      <td>0.212953</td>\n","      <td>1.907901</td>\n","      <td>0.706380</td>\n","      <td>-0.728275</td>\n","      <td>-1.640437</td>\n","      <td>2.241001</td>\n","      <td>1.291227</td>\n","      <td>1.968604</td>\n","      <td>1.997046</td>\n","      <td>...</td>\n","      <td>1.220493</td>\n","      <td>1.220493</td>\n","      <td>1.220493</td>\n","      <td>1.220493</td>\n","      <td>1.220493</td>\n","      <td>1.220493</td>\n","      <td>1.220493</td>\n","      <td>1.220493</td>\n","      <td>1.220493</td>\n","      <td>1.220493</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>5.203953</td>\n","      <td>-0.117075</td>\n","      <td>1.954860</td>\n","      <td>1.059242</td>\n","      <td>-0.363866</td>\n","      <td>-1.824970</td>\n","      <td>2.221196</td>\n","      <td>1.314638</td>\n","      <td>1.996140</td>\n","      <td>1.937963</td>\n","      <td>...</td>\n","      <td>1.218443</td>\n","      <td>1.218443</td>\n","      <td>1.218443</td>\n","      <td>1.218443</td>\n","      <td>1.218443</td>\n","      <td>1.218443</td>\n","      <td>1.218443</td>\n","      <td>1.218443</td>\n","      <td>1.218443</td>\n","      <td>1.218443</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>5.662090</td>\n","      <td>-0.412495</td>\n","      <td>2.009937</td>\n","      <td>1.160680</td>\n","      <td>0.148391</td>\n","      <td>-1.953367</td>\n","      <td>2.133152</td>\n","      <td>1.278162</td>\n","      <td>1.987777</td>\n","      <td>1.954777</td>\n","      <td>...</td>\n","      <td>1.215312</td>\n","      <td>1.215312</td>\n","      <td>1.215312</td>\n","      <td>1.215312</td>\n","      <td>1.215312</td>\n","      <td>1.215312</td>\n","      <td>1.215312</td>\n","      <td>1.215312</td>\n","      <td>1.215312</td>\n","      <td>1.215312</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>6.105591</td>\n","      <td>-0.803587</td>\n","      <td>1.968009</td>\n","      <td>0.939955</td>\n","      <td>0.610174</td>\n","      <td>-2.068435</td>\n","      <td>1.962591</td>\n","      <td>1.215610</td>\n","      <td>1.989384</td>\n","      <td>1.936396</td>\n","      <td>...</td>\n","      <td>1.190726</td>\n","      <td>1.190726</td>\n","      <td>1.190726</td>\n","      <td>1.190726</td>\n","      <td>1.190726</td>\n","      <td>1.190726</td>\n","      <td>1.190726</td>\n","      <td>1.190726</td>\n","      <td>1.190726</td>\n","      <td>1.190726</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>6.555169</td>\n","      <td>-1.120058</td>\n","      <td>1.847507</td>\n","      <td>0.622577</td>\n","      <td>0.970299</td>\n","      <td>-2.208078</td>\n","      <td>1.771914</td>\n","      <td>1.269590</td>\n","      <td>2.013824</td>\n","      <td>1.892061</td>\n","      <td>...</td>\n","      <td>1.194049</td>\n","      <td>1.194049</td>\n","      <td>1.194049</td>\n","      <td>1.194049</td>\n","      <td>1.194049</td>\n","      <td>1.194049</td>\n","      <td>1.194049</td>\n","      <td>1.194049</td>\n","      <td>1.194049</td>\n","      <td>1.194049</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>7.040878</td>\n","      <td>-1.380053</td>\n","      <td>1.592001</td>\n","      <td>0.198409</td>\n","      <td>1.128852</td>\n","      <td>-2.358706</td>\n","      <td>1.536971</td>\n","      <td>1.325490</td>\n","      <td>2.027486</td>\n","      <td>1.989601</td>\n","      <td>...</td>\n","      <td>1.189842</td>\n","      <td>1.189842</td>\n","      <td>1.189842</td>\n","      <td>1.189842</td>\n","      <td>1.189842</td>\n","      <td>1.189842</td>\n","      <td>1.189842</td>\n","      <td>1.189842</td>\n","      <td>1.189842</td>\n","      <td>1.189842</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>7.510933</td>\n","      <td>-1.566063</td>\n","      <td>1.427753</td>\n","      <td>-0.265491</td>\n","      <td>1.065583</td>\n","      <td>-2.459860</td>\n","      <td>1.373685</td>\n","      <td>1.380803</td>\n","      <td>1.979468</td>\n","      <td>2.003028</td>\n","      <td>...</td>\n","      <td>1.195024</td>\n","      <td>1.195024</td>\n","      <td>1.195024</td>\n","      <td>1.195024</td>\n","      <td>1.195024</td>\n","      <td>1.195024</td>\n","      <td>1.195024</td>\n","      <td>1.195024</td>\n","      <td>1.195024</td>\n","      <td>1.195024</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>7.964792</td>\n","      <td>-1.708781</td>\n","      <td>1.163213</td>\n","      <td>-0.612184</td>\n","      <td>0.866549</td>\n","      <td>-2.531974</td>\n","      <td>1.150535</td>\n","      <td>1.318331</td>\n","      <td>2.040222</td>\n","      <td>1.963529</td>\n","      <td>...</td>\n","      <td>1.283897</td>\n","      <td>1.283897</td>\n","      <td>1.283897</td>\n","      <td>1.283897</td>\n","      <td>1.283897</td>\n","      <td>1.283897</td>\n","      <td>1.283897</td>\n","      <td>1.283897</td>\n","      <td>1.283897</td>\n","      <td>1.283897</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>8.458739</td>\n","      <td>-1.872046</td>\n","      <td>0.960763</td>\n","      <td>-0.829677</td>\n","      <td>0.460477</td>\n","      <td>-2.471641</td>\n","      <td>0.887863</td>\n","      <td>1.292379</td>\n","      <td>2.022391</td>\n","      <td>1.988254</td>\n","      <td>...</td>\n","      <td>1.267249</td>\n","      <td>1.267249</td>\n","      <td>1.267249</td>\n","      <td>1.267249</td>\n","      <td>1.267249</td>\n","      <td>1.267249</td>\n","      <td>1.267249</td>\n","      <td>1.267249</td>\n","      <td>1.267249</td>\n","      <td>1.267249</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>8.905715</td>\n","      <td>-1.970730</td>\n","      <td>0.656055</td>\n","      <td>-0.928925</td>\n","      <td>-0.040399</td>\n","      <td>-2.519527</td>\n","      <td>0.589854</td>\n","      <td>1.308432</td>\n","      <td>2.201051</td>\n","      <td>2.040962</td>\n","      <td>...</td>\n","      <td>1.175727</td>\n","      <td>1.175727</td>\n","      <td>1.175727</td>\n","      <td>1.175727</td>\n","      <td>1.175727</td>\n","      <td>1.175727</td>\n","      <td>1.175727</td>\n","      <td>1.175727</td>\n","      <td>1.175727</td>\n","      <td>1.175727</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>9.489461</td>\n","      <td>-2.007576</td>\n","      <td>0.378425</td>\n","      <td>-0.770124</td>\n","      <td>-0.474626</td>\n","      <td>-2.568841</td>\n","      <td>0.338569</td>\n","      <td>1.361914</td>\n","      <td>2.092309</td>\n","      <td>2.029656</td>\n","      <td>...</td>\n","      <td>1.155570</td>\n","      <td>1.155570</td>\n","      <td>1.155570</td>\n","      <td>1.155570</td>\n","      <td>1.155570</td>\n","      <td>1.155570</td>\n","      <td>1.155570</td>\n","      <td>1.155570</td>\n","      <td>1.155570</td>\n","      <td>1.155570</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>9.881207</td>\n","      <td>-1.965259</td>\n","      <td>0.032193</td>\n","      <td>-0.339574</td>\n","      <td>-0.805379</td>\n","      <td>-2.594219</td>\n","      <td>0.027828</td>\n","      <td>1.358518</td>\n","      <td>2.002009</td>\n","      <td>1.987297</td>\n","      <td>...</td>\n","      <td>1.190591</td>\n","      <td>1.190591</td>\n","      <td>1.190591</td>\n","      <td>1.190591</td>\n","      <td>1.190591</td>\n","      <td>1.190591</td>\n","      <td>1.190591</td>\n","      <td>1.190591</td>\n","      <td>1.190591</td>\n","      <td>1.190591</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>10.362476</td>\n","      <td>-1.977012</td>\n","      <td>-0.285220</td>\n","      <td>0.131274</td>\n","      <td>-0.985359</td>\n","      <td>-2.538407</td>\n","      <td>-0.230604</td>\n","      <td>1.386007</td>\n","      <td>2.025130</td>\n","      <td>2.030919</td>\n","      <td>...</td>\n","      <td>1.126897</td>\n","      <td>1.126897</td>\n","      <td>1.126897</td>\n","      <td>1.126897</td>\n","      <td>1.126897</td>\n","      <td>1.126897</td>\n","      <td>1.126897</td>\n","      <td>1.126897</td>\n","      <td>1.126897</td>\n","      <td>1.126897</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>10.853230</td>\n","      <td>-1.996317</td>\n","      <td>-0.625069</td>\n","      <td>0.514308</td>\n","      <td>-0.838237</td>\n","      <td>-2.574621</td>\n","      <td>-0.533825</td>\n","      <td>1.350077</td>\n","      <td>2.064152</td>\n","      <td>1.974409</td>\n","      <td>...</td>\n","      <td>1.130117</td>\n","      <td>1.130117</td>\n","      <td>1.130117</td>\n","      <td>1.130117</td>\n","      <td>1.130117</td>\n","      <td>1.130117</td>\n","      <td>1.130117</td>\n","      <td>1.130117</td>\n","      <td>1.130117</td>\n","      <td>1.130117</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>11.250608</td>\n","      <td>-1.849073</td>\n","      <td>-0.859870</td>\n","      <td>0.813235</td>\n","      <td>-0.567881</td>\n","      <td>-2.496695</td>\n","      <td>-0.855951</td>\n","      <td>1.332058</td>\n","      <td>1.969266</td>\n","      <td>1.892757</td>\n","      <td>...</td>\n","      <td>1.209563</td>\n","      <td>1.209563</td>\n","      <td>1.209563</td>\n","      <td>1.209563</td>\n","      <td>1.209563</td>\n","      <td>1.209563</td>\n","      <td>1.209563</td>\n","      <td>1.209563</td>\n","      <td>1.209563</td>\n","      <td>1.209563</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>11.716143</td>\n","      <td>-1.661743</td>\n","      <td>-1.123806</td>\n","      <td>1.005433</td>\n","      <td>-0.126745</td>\n","      <td>-2.473880</td>\n","      <td>-1.077214</td>\n","      <td>1.349595</td>\n","      <td>1.955442</td>\n","      <td>1.950277</td>\n","      <td>...</td>\n","      <td>1.155678</td>\n","      <td>1.155678</td>\n","      <td>1.155678</td>\n","      <td>1.155678</td>\n","      <td>1.155678</td>\n","      <td>1.155678</td>\n","      <td>1.155678</td>\n","      <td>1.155678</td>\n","      <td>1.155678</td>\n","      <td>1.155678</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>12.193757</td>\n","      <td>-1.495709</td>\n","      <td>-1.374814</td>\n","      <td>1.114572</td>\n","      <td>0.406672</td>\n","      <td>-2.355959</td>\n","      <td>-1.308114</td>\n","      <td>1.445689</td>\n","      <td>2.005842</td>\n","      <td>1.979107</td>\n","      <td>...</td>\n","      <td>1.193662</td>\n","      <td>1.193662</td>\n","      <td>1.193662</td>\n","      <td>1.193662</td>\n","      <td>1.193662</td>\n","      <td>1.193662</td>\n","      <td>1.193662</td>\n","      <td>1.193662</td>\n","      <td>1.193662</td>\n","      <td>1.193662</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>12.629419</td>\n","      <td>-1.200317</td>\n","      <td>-1.567409</td>\n","      <td>0.914688</td>\n","      <td>0.763860</td>\n","      <td>-2.198554</td>\n","      <td>-1.521383</td>\n","      <td>1.390230</td>\n","      <td>2.059901</td>\n","      <td>1.944596</td>\n","      <td>...</td>\n","      <td>1.168713</td>\n","      <td>1.168713</td>\n","      <td>1.168713</td>\n","      <td>1.168713</td>\n","      <td>1.168713</td>\n","      <td>1.168713</td>\n","      <td>1.168713</td>\n","      <td>1.168713</td>\n","      <td>1.168713</td>\n","      <td>1.168713</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>13.147985</td>\n","      <td>-0.877854</td>\n","      <td>-1.741907</td>\n","      <td>0.461046</td>\n","      <td>1.050642</td>\n","      <td>-2.087086</td>\n","      <td>-1.654615</td>\n","      <td>1.261961</td>\n","      <td>2.105665</td>\n","      <td>1.952030</td>\n","      <td>...</td>\n","      <td>1.159076</td>\n","      <td>1.159076</td>\n","      <td>1.159076</td>\n","      <td>1.159076</td>\n","      <td>1.159076</td>\n","      <td>1.159076</td>\n","      <td>1.159076</td>\n","      <td>1.159076</td>\n","      <td>1.159076</td>\n","      <td>1.159076</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>13.665074</td>\n","      <td>-0.608018</td>\n","      <td>-1.789005</td>\n","      <td>-0.024892</td>\n","      <td>1.126160</td>\n","      <td>-2.026989</td>\n","      <td>-1.890971</td>\n","      <td>1.295572</td>\n","      <td>2.062782</td>\n","      <td>1.892592</td>\n","      <td>...</td>\n","      <td>1.156044</td>\n","      <td>1.156044</td>\n","      <td>1.156044</td>\n","      <td>1.156044</td>\n","      <td>1.156044</td>\n","      <td>1.156044</td>\n","      <td>1.156044</td>\n","      <td>1.156044</td>\n","      <td>1.156044</td>\n","      <td>1.156044</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>14.121745</td>\n","      <td>-0.272861</td>\n","      <td>-1.731532</td>\n","      <td>-0.464983</td>\n","      <td>0.986064</td>\n","      <td>-1.850236</td>\n","      <td>-2.065401</td>\n","      <td>1.265879</td>\n","      <td>1.990031</td>\n","      <td>1.949625</td>\n","      <td>...</td>\n","      <td>1.182898</td>\n","      <td>1.182898</td>\n","      <td>1.182898</td>\n","      <td>1.182898</td>\n","      <td>1.182898</td>\n","      <td>1.182898</td>\n","      <td>1.182898</td>\n","      <td>1.182898</td>\n","      <td>1.182898</td>\n","      <td>1.182898</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>14.535714</td>\n","      <td>0.051644</td>\n","      <td>-1.724272</td>\n","      <td>-0.870369</td>\n","      <td>0.637197</td>\n","      <td>-1.671938</td>\n","      <td>-2.127759</td>\n","      <td>1.250739</td>\n","      <td>1.988081</td>\n","      <td>2.008314</td>\n","      <td>...</td>\n","      <td>1.160646</td>\n","      <td>1.160646</td>\n","      <td>1.160646</td>\n","      <td>1.160646</td>\n","      <td>1.160646</td>\n","      <td>1.160646</td>\n","      <td>1.160646</td>\n","      <td>1.160646</td>\n","      <td>1.160646</td>\n","      <td>1.160646</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>14.987533</td>\n","      <td>0.464888</td>\n","      <td>-1.727793</td>\n","      <td>-1.028553</td>\n","      <td>0.154119</td>\n","      <td>-1.455605</td>\n","      <td>-2.271544</td>\n","      <td>1.354711</td>\n","      <td>2.043738</td>\n","      <td>2.088737</td>\n","      <td>...</td>\n","      <td>1.171341</td>\n","      <td>1.171341</td>\n","      <td>1.171341</td>\n","      <td>1.171341</td>\n","      <td>1.171341</td>\n","      <td>1.171341</td>\n","      <td>1.171341</td>\n","      <td>1.171341</td>\n","      <td>1.171341</td>\n","      <td>1.171341</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>15.414639</td>\n","      <td>0.850695</td>\n","      <td>-1.674123</td>\n","      <td>-0.865624</td>\n","      <td>-0.314915</td>\n","      <td>-1.194274</td>\n","      <td>-2.371500</td>\n","      <td>1.373983</td>\n","      <td>2.006507</td>\n","      <td>2.001806</td>\n","      <td>...</td>\n","      <td>1.199650</td>\n","      <td>1.199650</td>\n","      <td>1.199650</td>\n","      <td>1.199650</td>\n","      <td>1.199650</td>\n","      <td>1.199650</td>\n","      <td>1.199650</td>\n","      <td>1.199650</td>\n","      <td>1.199650</td>\n","      <td>1.199650</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>15.893472</td>\n","      <td>1.128237</td>\n","      <td>-1.478276</td>\n","      <td>-0.568940</td>\n","      <td>-0.668780</td>\n","      <td>-0.928554</td>\n","      <td>-2.409913</td>\n","      <td>1.371653</td>\n","      <td>1.993132</td>\n","      <td>2.004360</td>\n","      <td>...</td>\n","      <td>1.161861</td>\n","      <td>1.161861</td>\n","      <td>1.161861</td>\n","      <td>1.161861</td>\n","      <td>1.161861</td>\n","      <td>1.161861</td>\n","      <td>1.161861</td>\n","      <td>1.161861</td>\n","      <td>1.161861</td>\n","      <td>1.161861</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>16.362226</td>\n","      <td>1.356057</td>\n","      <td>-1.216460</td>\n","      <td>-0.135859</td>\n","      <td>-0.915095</td>\n","      <td>-0.693576</td>\n","      <td>-2.452169</td>\n","      <td>1.347094</td>\n","      <td>2.002794</td>\n","      <td>1.984830</td>\n","      <td>...</td>\n","      <td>1.251091</td>\n","      <td>1.251091</td>\n","      <td>1.251091</td>\n","      <td>1.251091</td>\n","      <td>1.251091</td>\n","      <td>1.251091</td>\n","      <td>1.251091</td>\n","      <td>1.251091</td>\n","      <td>1.251091</td>\n","      <td>1.251091</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>16.837095</td>\n","      <td>1.476590</td>\n","      <td>-0.952995</td>\n","      <td>0.338013</td>\n","      <td>-0.962856</td>\n","      <td>-0.442441</td>\n","      <td>-2.432537</td>\n","      <td>1.325617</td>\n","      <td>2.036079</td>\n","      <td>1.942637</td>\n","      <td>...</td>\n","      <td>1.198299</td>\n","      <td>1.198299</td>\n","      <td>1.198299</td>\n","      <td>1.198299</td>\n","      <td>1.198299</td>\n","      <td>1.198299</td>\n","      <td>1.198299</td>\n","      <td>1.198299</td>\n","      <td>1.198299</td>\n","      <td>1.198299</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>17.263840</td>\n","      <td>1.565697</td>\n","      <td>-0.666991</td>\n","      <td>0.703971</td>\n","      <td>-0.728087</td>\n","      <td>-0.192379</td>\n","      <td>-2.446404</td>\n","      <td>1.334536</td>\n","      <td>2.022475</td>\n","      <td>2.007936</td>\n","      <td>...</td>\n","      <td>1.173424</td>\n","      <td>1.173424</td>\n","      <td>1.173424</td>\n","      <td>1.173424</td>\n","      <td>1.173424</td>\n","      <td>1.173424</td>\n","      <td>1.173424</td>\n","      <td>1.173424</td>\n","      <td>1.173424</td>\n","      <td>1.173424</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>17.728121</td>\n","      <td>1.717774</td>\n","      <td>-0.280518</td>\n","      <td>1.027978</td>\n","      <td>-0.324609</td>\n","      <td>0.151083</td>\n","      <td>-2.409065</td>\n","      <td>1.321695</td>\n","      <td>1.991041</td>\n","      <td>2.008463</td>\n","      <td>...</td>\n","      <td>1.160475</td>\n","      <td>1.160475</td>\n","      <td>1.160475</td>\n","      <td>1.160475</td>\n","      <td>1.160475</td>\n","      <td>1.160475</td>\n","      <td>1.160475</td>\n","      <td>1.160475</td>\n","      <td>1.160475</td>\n","      <td>1.160475</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>18.264576</td>\n","      <td>1.892494</td>\n","      <td>0.071246</td>\n","      <td>1.070242</td>\n","      <td>0.150970</td>\n","      <td>0.419870</td>\n","      <td>-2.412294</td>\n","      <td>1.327611</td>\n","      <td>1.986576</td>\n","      <td>2.038126</td>\n","      <td>...</td>\n","      <td>1.172468</td>\n","      <td>1.172468</td>\n","      <td>1.172468</td>\n","      <td>1.172468</td>\n","      <td>1.172468</td>\n","      <td>1.172468</td>\n","      <td>1.172468</td>\n","      <td>1.172468</td>\n","      <td>1.172468</td>\n","      <td>1.172468</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>18.775183</td>\n","      <td>1.789234</td>\n","      <td>0.425397</td>\n","      <td>0.897809</td>\n","      <td>0.572748</td>\n","      <td>0.701037</td>\n","      <td>-2.444402</td>\n","      <td>1.394814</td>\n","      <td>2.047763</td>\n","      <td>2.010178</td>\n","      <td>...</td>\n","      <td>1.183922</td>\n","      <td>1.183922</td>\n","      <td>1.183922</td>\n","      <td>1.183922</td>\n","      <td>1.183922</td>\n","      <td>1.183922</td>\n","      <td>1.183922</td>\n","      <td>1.183922</td>\n","      <td>1.183922</td>\n","      <td>1.183922</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>19.260757</td>\n","      <td>1.640134</td>\n","      <td>0.821083</td>\n","      <td>0.612720</td>\n","      <td>0.905220</td>\n","      <td>0.971377</td>\n","      <td>-2.421750</td>\n","      <td>1.282454</td>\n","      <td>2.025453</td>\n","      <td>1.954841</td>\n","      <td>...</td>\n","      <td>1.144301</td>\n","      <td>1.144301</td>\n","      <td>1.144301</td>\n","      <td>1.144301</td>\n","      <td>1.144301</td>\n","      <td>1.144301</td>\n","      <td>1.144301</td>\n","      <td>1.144301</td>\n","      <td>1.144301</td>\n","      <td>1.144301</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>19.697371</td>\n","      <td>1.486856</td>\n","      <td>1.153629</td>\n","      <td>0.191102</td>\n","      <td>1.127527</td>\n","      <td>1.248524</td>\n","      <td>-2.303687</td>\n","      <td>1.273171</td>\n","      <td>1.970830</td>\n","      <td>1.891732</td>\n","      <td>...</td>\n","      <td>1.151490</td>\n","      <td>1.151490</td>\n","      <td>1.151490</td>\n","      <td>1.151490</td>\n","      <td>1.151490</td>\n","      <td>1.151490</td>\n","      <td>1.151490</td>\n","      <td>1.151490</td>\n","      <td>1.151490</td>\n","      <td>1.151490</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>20.211489</td>\n","      <td>1.337195</td>\n","      <td>1.416607</td>\n","      <td>-0.254755</td>\n","      <td>1.096678</td>\n","      <td>1.516737</td>\n","      <td>-2.191261</td>\n","      <td>1.345246</td>\n","      <td>1.976211</td>\n","      <td>1.970134</td>\n","      <td>...</td>\n","      <td>1.247605</td>\n","      <td>1.247605</td>\n","      <td>1.247605</td>\n","      <td>1.247605</td>\n","      <td>1.247605</td>\n","      <td>1.247605</td>\n","      <td>1.247605</td>\n","      <td>1.247605</td>\n","      <td>1.247605</td>\n","      <td>1.247605</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","      <td>...</td>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","      <td>4.717976</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","      <td>...</td>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","      <td>4.660798</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","      <td>...</td>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","      <td>4.633826</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","      <td>...</td>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","      <td>4.539379</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","      <td>...</td>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","      <td>4.545885</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","      <td>...</td>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","      <td>4.713014</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","      <td>...</td>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","      <td>5.021049</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","      <td>...</td>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","      <td>5.036703</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","      <td>...</td>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","      <td>5.001430</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","      <td>...</td>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","      <td>4.761061</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","      <td>...</td>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","      <td>4.897291</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","      <td>...</td>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","      <td>5.122707</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","      <td>...</td>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","      <td>5.352748</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","      <td>...</td>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","      <td>5.198748</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","      <td>...</td>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","      <td>4.557338</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>59 rows × 26 columns</p>\n","</div>"],"text/plain":["           0         1         2         3         4         5         6   \\\n","0    0.102104  1.690298 -0.732006  0.847111  0.798189  1.040970  2.628688   \n","1    0.501463  1.777943 -0.421827  0.414215  1.039330  0.775214  2.701208   \n","2    0.944141  1.790322 -0.041722 -0.014226  1.098226  0.527439  2.716042   \n","3    1.406419  1.771376  0.328494 -0.468852  0.900589  0.234735  2.559814   \n","4    1.862925  1.687651  0.708704 -0.807441  0.597644 -0.047129  2.610654   \n","5    2.451734  1.575453  0.996607 -0.920642  0.186981 -0.310957  2.704166   \n","6    2.912332  1.383239  1.314543 -0.802508 -0.317605 -0.643567  2.750806   \n","7    3.326980  1.158393  1.570038 -0.575280 -0.735511 -0.914323  2.736963   \n","8    3.833704  0.878748  1.732224 -0.178839 -0.916972 -1.126539  2.614489   \n","9    4.233853  0.607751  1.860583  0.321860 -0.945738 -1.337776  2.339664   \n","10   4.685832  0.212953  1.907901  0.706380 -0.728275 -1.640437  2.241001   \n","11   5.203953 -0.117075  1.954860  1.059242 -0.363866 -1.824970  2.221196   \n","12   5.662090 -0.412495  2.009937  1.160680  0.148391 -1.953367  2.133152   \n","13   6.105591 -0.803587  1.968009  0.939955  0.610174 -2.068435  1.962591   \n","14   6.555169 -1.120058  1.847507  0.622577  0.970299 -2.208078  1.771914   \n","15   7.040878 -1.380053  1.592001  0.198409  1.128852 -2.358706  1.536971   \n","16   7.510933 -1.566063  1.427753 -0.265491  1.065583 -2.459860  1.373685   \n","17   7.964792 -1.708781  1.163213 -0.612184  0.866549 -2.531974  1.150535   \n","18   8.458739 -1.872046  0.960763 -0.829677  0.460477 -2.471641  0.887863   \n","19   8.905715 -1.970730  0.656055 -0.928925 -0.040399 -2.519527  0.589854   \n","20   9.489461 -2.007576  0.378425 -0.770124 -0.474626 -2.568841  0.338569   \n","21   9.881207 -1.965259  0.032193 -0.339574 -0.805379 -2.594219  0.027828   \n","22  10.362476 -1.977012 -0.285220  0.131274 -0.985359 -2.538407 -0.230604   \n","23  10.853230 -1.996317 -0.625069  0.514308 -0.838237 -2.574621 -0.533825   \n","24  11.250608 -1.849073 -0.859870  0.813235 -0.567881 -2.496695 -0.855951   \n","25  11.716143 -1.661743 -1.123806  1.005433 -0.126745 -2.473880 -1.077214   \n","26  12.193757 -1.495709 -1.374814  1.114572  0.406672 -2.355959 -1.308114   \n","27  12.629419 -1.200317 -1.567409  0.914688  0.763860 -2.198554 -1.521383   \n","28  13.147985 -0.877854 -1.741907  0.461046  1.050642 -2.087086 -1.654615   \n","29  13.665074 -0.608018 -1.789005 -0.024892  1.126160 -2.026989 -1.890971   \n","30  14.121745 -0.272861 -1.731532 -0.464983  0.986064 -1.850236 -2.065401   \n","31  14.535714  0.051644 -1.724272 -0.870369  0.637197 -1.671938 -2.127759   \n","32  14.987533  0.464888 -1.727793 -1.028553  0.154119 -1.455605 -2.271544   \n","33  15.414639  0.850695 -1.674123 -0.865624 -0.314915 -1.194274 -2.371500   \n","34  15.893472  1.128237 -1.478276 -0.568940 -0.668780 -0.928554 -2.409913   \n","35  16.362226  1.356057 -1.216460 -0.135859 -0.915095 -0.693576 -2.452169   \n","36  16.837095  1.476590 -0.952995  0.338013 -0.962856 -0.442441 -2.432537   \n","37  17.263840  1.565697 -0.666991  0.703971 -0.728087 -0.192379 -2.446404   \n","38  17.728121  1.717774 -0.280518  1.027978 -0.324609  0.151083 -2.409065   \n","39  18.264576  1.892494  0.071246  1.070242  0.150970  0.419870 -2.412294   \n","40  18.775183  1.789234  0.425397  0.897809  0.572748  0.701037 -2.444402   \n","41  19.260757  1.640134  0.821083  0.612720  0.905220  0.971377 -2.421750   \n","42  19.697371  1.486856  1.153629  0.191102  1.127527  1.248524 -2.303687   \n","43  20.211489  1.337195  1.416607 -0.254755  1.096678  1.516737 -2.191261   \n","44   4.717976  4.717976  4.717976  4.717976  4.717976  4.717976  4.717976   \n","45   4.660798  4.660798  4.660798  4.660798  4.660798  4.660798  4.660798   \n","46   4.633826  4.633826  4.633826  4.633826  4.633826  4.633826  4.633826   \n","47   4.539379  4.539379  4.539379  4.539379  4.539379  4.539379  4.539379   \n","48   4.545885  4.545885  4.545885  4.545885  4.545885  4.545885  4.545885   \n","49   4.713014  4.713014  4.713014  4.713014  4.713014  4.713014  4.713014   \n","50   5.021049  5.021049  5.021049  5.021049  5.021049  5.021049  5.021049   \n","51   5.036703  5.036703  5.036703  5.036703  5.036703  5.036703  5.036703   \n","52   5.001430  5.001430  5.001430  5.001430  5.001430  5.001430  5.001430   \n","53   4.761061  4.761061  4.761061  4.761061  4.761061  4.761061  4.761061   \n","54   4.897291  4.897291  4.897291  4.897291  4.897291  4.897291  4.897291   \n","55   5.122707  5.122707  5.122707  5.122707  5.122707  5.122707  5.122707   \n","56   5.352748  5.352748  5.352748  5.352748  5.352748  5.352748  5.352748   \n","57   5.198748  5.198748  5.198748  5.198748  5.198748  5.198748  5.198748   \n","58   4.557338  4.557338  4.557338  4.557338  4.557338  4.557338  4.557338   \n","\n","          7         8         9   ...        16        17        18        19  \\\n","0   1.239484  1.978418  1.943971  ...  1.313941  1.313941  1.313941  1.313941   \n","1   1.304612  2.060616  1.984488  ...  1.370112  1.370112  1.370112  1.370112   \n","2   1.333475  2.067372  1.990813  ...  1.382591  1.382591  1.382591  1.382591   \n","3   1.310214  1.993624  2.038106  ...  1.440563  1.440563  1.440563  1.440563   \n","4   1.325231  2.038662  2.025807  ...  1.370153  1.370153  1.370153  1.370153   \n","5   1.389927  2.059216  1.951594  ...  1.416731  1.416731  1.416731  1.416731   \n","6   1.344543  2.028069  1.945646  ...  1.416792  1.416792  1.416792  1.416792   \n","7   1.323641  2.054521  2.018994  ...  1.380926  1.380926  1.380926  1.380926   \n","8   1.275802  2.033884  1.982288  ...  1.306998  1.306998  1.306998  1.306998   \n","9   1.270190  1.970824  1.993912  ...  1.225813  1.225813  1.225813  1.225813   \n","10  1.291227  1.968604  1.997046  ...  1.220493  1.220493  1.220493  1.220493   \n","11  1.314638  1.996140  1.937963  ...  1.218443  1.218443  1.218443  1.218443   \n","12  1.278162  1.987777  1.954777  ...  1.215312  1.215312  1.215312  1.215312   \n","13  1.215610  1.989384  1.936396  ...  1.190726  1.190726  1.190726  1.190726   \n","14  1.269590  2.013824  1.892061  ...  1.194049  1.194049  1.194049  1.194049   \n","15  1.325490  2.027486  1.989601  ...  1.189842  1.189842  1.189842  1.189842   \n","16  1.380803  1.979468  2.003028  ...  1.195024  1.195024  1.195024  1.195024   \n","17  1.318331  2.040222  1.963529  ...  1.283897  1.283897  1.283897  1.283897   \n","18  1.292379  2.022391  1.988254  ...  1.267249  1.267249  1.267249  1.267249   \n","19  1.308432  2.201051  2.040962  ...  1.175727  1.175727  1.175727  1.175727   \n","20  1.361914  2.092309  2.029656  ...  1.155570  1.155570  1.155570  1.155570   \n","21  1.358518  2.002009  1.987297  ...  1.190591  1.190591  1.190591  1.190591   \n","22  1.386007  2.025130  2.030919  ...  1.126897  1.126897  1.126897  1.126897   \n","23  1.350077  2.064152  1.974409  ...  1.130117  1.130117  1.130117  1.130117   \n","24  1.332058  1.969266  1.892757  ...  1.209563  1.209563  1.209563  1.209563   \n","25  1.349595  1.955442  1.950277  ...  1.155678  1.155678  1.155678  1.155678   \n","26  1.445689  2.005842  1.979107  ...  1.193662  1.193662  1.193662  1.193662   \n","27  1.390230  2.059901  1.944596  ...  1.168713  1.168713  1.168713  1.168713   \n","28  1.261961  2.105665  1.952030  ...  1.159076  1.159076  1.159076  1.159076   \n","29  1.295572  2.062782  1.892592  ...  1.156044  1.156044  1.156044  1.156044   \n","30  1.265879  1.990031  1.949625  ...  1.182898  1.182898  1.182898  1.182898   \n","31  1.250739  1.988081  2.008314  ...  1.160646  1.160646  1.160646  1.160646   \n","32  1.354711  2.043738  2.088737  ...  1.171341  1.171341  1.171341  1.171341   \n","33  1.373983  2.006507  2.001806  ...  1.199650  1.199650  1.199650  1.199650   \n","34  1.371653  1.993132  2.004360  ...  1.161861  1.161861  1.161861  1.161861   \n","35  1.347094  2.002794  1.984830  ...  1.251091  1.251091  1.251091  1.251091   \n","36  1.325617  2.036079  1.942637  ...  1.198299  1.198299  1.198299  1.198299   \n","37  1.334536  2.022475  2.007936  ...  1.173424  1.173424  1.173424  1.173424   \n","38  1.321695  1.991041  2.008463  ...  1.160475  1.160475  1.160475  1.160475   \n","39  1.327611  1.986576  2.038126  ...  1.172468  1.172468  1.172468  1.172468   \n","40  1.394814  2.047763  2.010178  ...  1.183922  1.183922  1.183922  1.183922   \n","41  1.282454  2.025453  1.954841  ...  1.144301  1.144301  1.144301  1.144301   \n","42  1.273171  1.970830  1.891732  ...  1.151490  1.151490  1.151490  1.151490   \n","43  1.345246  1.976211  1.970134  ...  1.247605  1.247605  1.247605  1.247605   \n","44  4.717976  4.717976  4.717976  ...  4.717976  4.717976  4.717976  4.717976   \n","45  4.660798  4.660798  4.660798  ...  4.660798  4.660798  4.660798  4.660798   \n","46  4.633826  4.633826  4.633826  ...  4.633826  4.633826  4.633826  4.633826   \n","47  4.539379  4.539379  4.539379  ...  4.539379  4.539379  4.539379  4.539379   \n","48  4.545885  4.545885  4.545885  ...  4.545885  4.545885  4.545885  4.545885   \n","49  4.713014  4.713014  4.713014  ...  4.713014  4.713014  4.713014  4.713014   \n","50  5.021049  5.021049  5.021049  ...  5.021049  5.021049  5.021049  5.021049   \n","51  5.036703  5.036703  5.036703  ...  5.036703  5.036703  5.036703  5.036703   \n","52  5.001430  5.001430  5.001430  ...  5.001430  5.001430  5.001430  5.001430   \n","53  4.761061  4.761061  4.761061  ...  4.761061  4.761061  4.761061  4.761061   \n","54  4.897291  4.897291  4.897291  ...  4.897291  4.897291  4.897291  4.897291   \n","55  5.122707  5.122707  5.122707  ...  5.122707  5.122707  5.122707  5.122707   \n","56  5.352748  5.352748  5.352748  ...  5.352748  5.352748  5.352748  5.352748   \n","57  5.198748  5.198748  5.198748  ...  5.198748  5.198748  5.198748  5.198748   \n","58  4.557338  4.557338  4.557338  ...  4.557338  4.557338  4.557338  4.557338   \n","\n","          20        21        22        23        24        25  \n","0   1.313941  1.313941  1.313941  1.313941  1.313941  1.313941  \n","1   1.370112  1.370112  1.370112  1.370112  1.370112  1.370112  \n","2   1.382591  1.382591  1.382591  1.382591  1.382591  1.382591  \n","3   1.440563  1.440563  1.440563  1.440563  1.440563  1.440563  \n","4   1.370153  1.370153  1.370153  1.370153  1.370153  1.370153  \n","5   1.416731  1.416731  1.416731  1.416731  1.416731  1.416731  \n","6   1.416792  1.416792  1.416792  1.416792  1.416792  1.416792  \n","7   1.380926  1.380926  1.380926  1.380926  1.380926  1.380926  \n","8   1.306998  1.306998  1.306998  1.306998  1.306998  1.306998  \n","9   1.225813  1.225813  1.225813  1.225813  1.225813  1.225813  \n","10  1.220493  1.220493  1.220493  1.220493  1.220493  1.220493  \n","11  1.218443  1.218443  1.218443  1.218443  1.218443  1.218443  \n","12  1.215312  1.215312  1.215312  1.215312  1.215312  1.215312  \n","13  1.190726  1.190726  1.190726  1.190726  1.190726  1.190726  \n","14  1.194049  1.194049  1.194049  1.194049  1.194049  1.194049  \n","15  1.189842  1.189842  1.189842  1.189842  1.189842  1.189842  \n","16  1.195024  1.195024  1.195024  1.195024  1.195024  1.195024  \n","17  1.283897  1.283897  1.283897  1.283897  1.283897  1.283897  \n","18  1.267249  1.267249  1.267249  1.267249  1.267249  1.267249  \n","19  1.175727  1.175727  1.175727  1.175727  1.175727  1.175727  \n","20  1.155570  1.155570  1.155570  1.155570  1.155570  1.155570  \n","21  1.190591  1.190591  1.190591  1.190591  1.190591  1.190591  \n","22  1.126897  1.126897  1.126897  1.126897  1.126897  1.126897  \n","23  1.130117  1.130117  1.130117  1.130117  1.130117  1.130117  \n","24  1.209563  1.209563  1.209563  1.209563  1.209563  1.209563  \n","25  1.155678  1.155678  1.155678  1.155678  1.155678  1.155678  \n","26  1.193662  1.193662  1.193662  1.193662  1.193662  1.193662  \n","27  1.168713  1.168713  1.168713  1.168713  1.168713  1.168713  \n","28  1.159076  1.159076  1.159076  1.159076  1.159076  1.159076  \n","29  1.156044  1.156044  1.156044  1.156044  1.156044  1.156044  \n","30  1.182898  1.182898  1.182898  1.182898  1.182898  1.182898  \n","31  1.160646  1.160646  1.160646  1.160646  1.160646  1.160646  \n","32  1.171341  1.171341  1.171341  1.171341  1.171341  1.171341  \n","33  1.199650  1.199650  1.199650  1.199650  1.199650  1.199650  \n","34  1.161861  1.161861  1.161861  1.161861  1.161861  1.161861  \n","35  1.251091  1.251091  1.251091  1.251091  1.251091  1.251091  \n","36  1.198299  1.198299  1.198299  1.198299  1.198299  1.198299  \n","37  1.173424  1.173424  1.173424  1.173424  1.173424  1.173424  \n","38  1.160475  1.160475  1.160475  1.160475  1.160475  1.160475  \n","39  1.172468  1.172468  1.172468  1.172468  1.172468  1.172468  \n","40  1.183922  1.183922  1.183922  1.183922  1.183922  1.183922  \n","41  1.144301  1.144301  1.144301  1.144301  1.144301  1.144301  \n","42  1.151490  1.151490  1.151490  1.151490  1.151490  1.151490  \n","43  1.247605  1.247605  1.247605  1.247605  1.247605  1.247605  \n","44  4.717976  4.717976  4.717976  4.717976  4.717976  4.717976  \n","45  4.660798  4.660798  4.660798  4.660798  4.660798  4.660798  \n","46  4.633826  4.633826  4.633826  4.633826  4.633826  4.633826  \n","47  4.539379  4.539379  4.539379  4.539379  4.539379  4.539379  \n","48  4.545885  4.545885  4.545885  4.545885  4.545885  4.545885  \n","49  4.713014  4.713014  4.713014  4.713014  4.713014  4.713014  \n","50  5.021049  5.021049  5.021049  5.021049  5.021049  5.021049  \n","51  5.036703  5.036703  5.036703  5.036703  5.036703  5.036703  \n","52  5.001430  5.001430  5.001430  5.001430  5.001430  5.001430  \n","53  4.761061  4.761061  4.761061  4.761061  4.761061  4.761061  \n","54  4.897291  4.897291  4.897291  4.897291  4.897291  4.897291  \n","55  5.122707  5.122707  5.122707  5.122707  5.122707  5.122707  \n","56  5.352748  5.352748  5.352748  5.352748  5.352748  5.352748  \n","57  5.198748  5.198748  5.198748  5.198748  5.198748  5.198748  \n","58  4.557338  4.557338  4.557338  4.557338  4.557338  4.557338  \n","\n","[59 rows x 26 columns]"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["df_pred"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>1.560060</td>\n","      <td>-0.854437</td>\n","      <td>0.720639</td>\n","      <td>0.691729</td>\n","      <td>0.944008</td>\n","      <td>2.700632</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.465116</td>\n","      <td>1.689858</td>\n","      <td>-0.514359</td>\n","      <td>0.333295</td>\n","      <td>0.942289</td>\n","      <td>0.681604</td>\n","      <td>2.785811</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.930233</td>\n","      <td>1.753589</td>\n","      <td>-0.154209</td>\n","      <td>-0.124995</td>\n","      <td>0.992368</td>\n","      <td>0.412951</td>\n","      <td>2.845461</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.395349</td>\n","      <td>1.748068</td>\n","      <td>0.212022</td>\n","      <td>-0.556775</td>\n","      <td>0.831727</td>\n","      <td>0.140540</td>\n","      <td>2.879232</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.860465</td>\n","      <td>1.673573</td>\n","      <td>0.569904</td>\n","      <td>-0.870579</td>\n","      <td>0.494812</td>\n","      <td>-0.133144</td>\n","      <td>2.887018</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2.325581</td>\n","      <td>1.533811</td>\n","      <td>0.905607</td>\n","      <td>-1.000151</td>\n","      <td>0.053178</td>\n","      <td>-0.405638</td>\n","      <td>2.868949</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2.790698</td>\n","      <td>1.335526</td>\n","      <td>1.206832</td>\n","      <td>-0.918192</td>\n","      <td>-0.399688</td>\n","      <td>-0.674530</td>\n","      <td>2.825381</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>3.255814</td>\n","      <td>1.087824</td>\n","      <td>1.463513</td>\n","      <td>-0.641980</td>\n","      <td>-0.767958</td>\n","      <td>-0.937474</td>\n","      <td>2.756889</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>3.720930</td>\n","      <td>0.801365</td>\n","      <td>1.668203</td>\n","      <td>-0.229808</td>\n","      <td>-0.973515</td>\n","      <td>-1.192211</td>\n","      <td>2.664256</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>4.186047</td>\n","      <td>0.487549</td>\n","      <td>1.816168</td>\n","      <td>0.231092</td>\n","      <td>-0.972449</td>\n","      <td>-1.436585</td>\n","      <td>2.548458</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>4.651163</td>\n","      <td>0.157822</td>\n","      <td>1.905225</td>\n","      <td>0.642811</td>\n","      <td>-0.764575</td>\n","      <td>-1.668554</td>\n","      <td>2.410650</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>5.116279</td>\n","      <td>-0.176870</td>\n","      <td>1.935436</td>\n","      <td>0.917559</td>\n","      <td>-0.393801</td>\n","      <td>-1.886207</td>\n","      <td>2.252158</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>5.581395</td>\n","      <td>-0.506443</td>\n","      <td>1.908720</td>\n","      <td>0.996574</td>\n","      <td>0.061000</td>\n","      <td>-2.087774</td>\n","      <td>2.074456</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>6.046512</td>\n","      <td>-0.821885</td>\n","      <td>1.828463</td>\n","      <td>0.862921</td>\n","      <td>0.502832</td>\n","      <td>-2.271633</td>\n","      <td>1.879159</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>6.511628</td>\n","      <td>-1.115352</td>\n","      <td>1.699162</td>\n","      <td>0.545212</td>\n","      <td>0.837490</td>\n","      <td>-2.436322</td>\n","      <td>1.668004</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>6.976744</td>\n","      <td>-1.380170</td>\n","      <td>1.526119</td>\n","      <td>0.111347</td>\n","      <td>0.993853</td>\n","      <td>-2.580544</td>\n","      <td>1.442838</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>7.441860</td>\n","      <td>-1.610795</td>\n","      <td>1.315203</td>\n","      <td>-0.346239</td>\n","      <td>0.939015</td>\n","      <td>-2.703172</td>\n","      <td>1.205599</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>7.906977</td>\n","      <td>-1.802741</td>\n","      <td>1.072663</td>\n","      <td>-0.730422</td>\n","      <td>0.685027</td>\n","      <td>-2.803255</td>\n","      <td>0.958309</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>8.372093</td>\n","      <td>-1.952503</td>\n","      <td>0.804994</td>\n","      <td>-0.959957</td>\n","      <td>0.286020</td>\n","      <td>-2.880023</td>\n","      <td>0.703051</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>8.837209</td>\n","      <td>-2.057485</td>\n","      <td>0.518845</td>\n","      <td>-0.986439</td>\n","      <td>-0.173427</td>\n","      <td>-2.932885</td>\n","      <td>0.441961</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>9.302326</td>\n","      <td>-2.115938</td>\n","      <td>0.220954</td>\n","      <td>-0.804305</td>\n","      <td>-0.596103</td>\n","      <td>-2.961438</td>\n","      <td>0.177211</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>9.767442</td>\n","      <td>-2.126912</td>\n","      <td>-0.081893</td>\n","      <td>-0.451965</td>\n","      <td>-0.892498</td>\n","      <td>-2.965464</td>\n","      <td>-0.089004</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>10.232558</td>\n","      <td>-2.090237</td>\n","      <td>-0.382893</td>\n","      <td>-0.003872</td>\n","      <td>-0.999585</td>\n","      <td>-2.944932</td>\n","      <td>-0.354477</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>10.697674</td>\n","      <td>-2.006509</td>\n","      <td>-0.675252</td>\n","      <td>0.444975</td>\n","      <td>-0.894252</td>\n","      <td>-2.899997</td>\n","      <td>-0.617010</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>11.162791</td>\n","      <td>-1.877113</td>\n","      <td>-0.952210</td>\n","      <td>0.799049</td>\n","      <td>-0.598498</td>\n","      <td>-2.831004</td>\n","      <td>-0.874419</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>11.627907</td>\n","      <td>-1.704255</td>\n","      <td>-1.207074</td>\n","      <td>0.982727</td>\n","      <td>-0.175061</td>\n","      <td>-2.738478</td>\n","      <td>-1.124556</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>12.093023</td>\n","      <td>-1.491017</td>\n","      <td>-1.433278</td>\n","      <td>0.956673</td>\n","      <td>0.285841</td>\n","      <td>-2.623132</td>\n","      <td>-1.365315</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>12.558140</td>\n","      <td>-1.241423</td>\n","      <td>-1.624455</td>\n","      <td>0.726448</td>\n","      <td>0.685888</td>\n","      <td>-2.485858</td>\n","      <td>-1.594656</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>13.023256</td>\n","      <td>-0.960518</td>\n","      <td>-1.774551</td>\n","      <td>0.341306</td>\n","      <td>0.939879</td>\n","      <td>-2.327724</td>\n","      <td>-1.810607</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>13.488372</td>\n","      <td>-0.654444</td>\n","      <td>-1.877989</td>\n","      <td>-0.116555</td>\n","      <td>0.994010</td>\n","      <td>-2.149972</td>\n","      <td>-2.011292</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>13.953488</td>\n","      <td>-0.330492</td>\n","      <td>-1.929876</td>\n","      <td>-0.549761</td>\n","      <td>0.837182</td>\n","      <td>-1.954013</td>\n","      <td>-2.194933</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>14.418605</td>\n","      <td>0.002871</td>\n","      <td>-1.926281</td>\n","      <td>-0.866538</td>\n","      <td>0.503039</td>\n","      <td>-1.741415</td>\n","      <td>-2.359874</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>14.883721</td>\n","      <td>0.336052</td>\n","      <td>-1.864565</td>\n","      <td>-1.000002</td>\n","      <td>0.062565</td>\n","      <td>-1.513904</td>\n","      <td>-2.504591</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>15.348837</td>\n","      <td>0.658485</td>\n","      <td>-1.743757</td>\n","      <td>-0.922043</td>\n","      <td>-0.390993</td>\n","      <td>-1.273345</td>\n","      <td>-2.627708</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>15.813953</td>\n","      <td>0.958914</td>\n","      <td>-1.564952</td>\n","      <td>-0.649103</td>\n","      <td>-0.761665</td>\n","      <td>-1.021741</td>\n","      <td>-2.728011</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>16.279070</td>\n","      <td>1.225853</td>\n","      <td>-1.331663</td>\n","      <td>-0.238785</td>\n","      <td>-0.970834</td>\n","      <td>-0.761211</td>\n","      <td>-2.804462</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>16.744186</td>\n","      <td>1.448211</td>\n","      <td>-1.050060</td>\n","      <td>0.222075</td>\n","      <td>-0.973830</td>\n","      <td>-0.493987</td>\n","      <td>-2.856214</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>17.209302</td>\n","      <td>1.616059</td>\n","      <td>-0.729007</td>\n","      <td>0.635585</td>\n","      <td>-0.769602</td>\n","      <td>-0.222387</td>\n","      <td>-2.882622</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>17.674419</td>\n","      <td>1.721456</td>\n","      <td>-0.379819</td>\n","      <td>0.913575</td>\n","      <td>-0.401274</td>\n","      <td>0.051193</td>\n","      <td>-2.883258</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>18.139535</td>\n","      <td>1.759203</td>\n","      <td>-0.015710</td>\n","      <td>0.996587</td>\n","      <td>0.052806</td>\n","      <td>0.324305</td>\n","      <td>-2.857916</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>18.604651</td>\n","      <td>1.727385</td>\n","      <td>0.349052</td>\n","      <td>0.866830</td>\n","      <td>0.495796</td>\n","      <td>0.594465</td>\n","      <td>-2.806627</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>19.069767</td>\n","      <td>1.627587</td>\n","      <td>0.700142</td>\n","      <td>0.552081</td>\n","      <td>0.833240</td>\n","      <td>0.859181</td>\n","      <td>-2.729662</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>19.534884</td>\n","      <td>1.464716</td>\n","      <td>1.024199</td>\n","      <td>0.119611</td>\n","      <td>0.993425</td>\n","      <td>1.115970</td>\n","      <td>-2.627536</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>20.000000</td>\n","      <td>1.246492</td>\n","      <td>1.309683</td>\n","      <td>-0.338443</td>\n","      <td>0.942632</td>\n","      <td>1.362393</td>\n","      <td>-2.501014</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>59 rows × 26 columns</p>\n","</div>"],"text/plain":["           0         1         2         3         4         5         6   \\\n","0    0.000000  1.560060 -0.854437  0.720639  0.691729  0.944008  2.700632   \n","1    0.465116  1.689858 -0.514359  0.333295  0.942289  0.681604  2.785811   \n","2    0.930233  1.753589 -0.154209 -0.124995  0.992368  0.412951  2.845461   \n","3    1.395349  1.748068  0.212022 -0.556775  0.831727  0.140540  2.879232   \n","4    1.860465  1.673573  0.569904 -0.870579  0.494812 -0.133144  2.887018   \n","5    2.325581  1.533811  0.905607 -1.000151  0.053178 -0.405638  2.868949   \n","6    2.790698  1.335526  1.206832 -0.918192 -0.399688 -0.674530  2.825381   \n","7    3.255814  1.087824  1.463513 -0.641980 -0.767958 -0.937474  2.756889   \n","8    3.720930  0.801365  1.668203 -0.229808 -0.973515 -1.192211  2.664256   \n","9    4.186047  0.487549  1.816168  0.231092 -0.972449 -1.436585  2.548458   \n","10   4.651163  0.157822  1.905225  0.642811 -0.764575 -1.668554  2.410650   \n","11   5.116279 -0.176870  1.935436  0.917559 -0.393801 -1.886207  2.252158   \n","12   5.581395 -0.506443  1.908720  0.996574  0.061000 -2.087774  2.074456   \n","13   6.046512 -0.821885  1.828463  0.862921  0.502832 -2.271633  1.879159   \n","14   6.511628 -1.115352  1.699162  0.545212  0.837490 -2.436322  1.668004   \n","15   6.976744 -1.380170  1.526119  0.111347  0.993853 -2.580544  1.442838   \n","16   7.441860 -1.610795  1.315203 -0.346239  0.939015 -2.703172  1.205599   \n","17   7.906977 -1.802741  1.072663 -0.730422  0.685027 -2.803255  0.958309   \n","18   8.372093 -1.952503  0.804994 -0.959957  0.286020 -2.880023  0.703051   \n","19   8.837209 -2.057485  0.518845 -0.986439 -0.173427 -2.932885  0.441961   \n","20   9.302326 -2.115938  0.220954 -0.804305 -0.596103 -2.961438  0.177211   \n","21   9.767442 -2.126912 -0.081893 -0.451965 -0.892498 -2.965464 -0.089004   \n","22  10.232558 -2.090237 -0.382893 -0.003872 -0.999585 -2.944932 -0.354477   \n","23  10.697674 -2.006509 -0.675252  0.444975 -0.894252 -2.899997 -0.617010   \n","24  11.162791 -1.877113 -0.952210  0.799049 -0.598498 -2.831004 -0.874419   \n","25  11.627907 -1.704255 -1.207074  0.982727 -0.175061 -2.738478 -1.124556   \n","26  12.093023 -1.491017 -1.433278  0.956673  0.285841 -2.623132 -1.365315   \n","27  12.558140 -1.241423 -1.624455  0.726448  0.685888 -2.485858 -1.594656   \n","28  13.023256 -0.960518 -1.774551  0.341306  0.939879 -2.327724 -1.810607   \n","29  13.488372 -0.654444 -1.877989 -0.116555  0.994010 -2.149972 -2.011292   \n","30  13.953488 -0.330492 -1.929876 -0.549761  0.837182 -1.954013 -2.194933   \n","31  14.418605  0.002871 -1.926281 -0.866538  0.503039 -1.741415 -2.359874   \n","32  14.883721  0.336052 -1.864565 -1.000002  0.062565 -1.513904 -2.504591   \n","33  15.348837  0.658485 -1.743757 -0.922043 -0.390993 -1.273345 -2.627708   \n","34  15.813953  0.958914 -1.564952 -0.649103 -0.761665 -1.021741 -2.728011   \n","35  16.279070  1.225853 -1.331663 -0.238785 -0.970834 -0.761211 -2.804462   \n","36  16.744186  1.448211 -1.050060  0.222075 -0.973830 -0.493987 -2.856214   \n","37  17.209302  1.616059 -0.729007  0.635585 -0.769602 -0.222387 -2.882622   \n","38  17.674419  1.721456 -0.379819  0.913575 -0.401274  0.051193 -2.883258   \n","39  18.139535  1.759203 -0.015710  0.996587  0.052806  0.324305 -2.857916   \n","40  18.604651  1.727385  0.349052  0.866830  0.495796  0.594465 -2.806627   \n","41  19.069767  1.627587  0.700142  0.552081  0.833240  0.859181 -2.729662   \n","42  19.534884  1.464716  1.024199  0.119611  0.993425  1.115970 -2.627536   \n","43  20.000000  1.246492  1.309683 -0.338443  0.942632  1.362393 -2.501014   \n","44        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","45        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","46        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","47        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","48        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","49        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","50        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","51        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","52        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","53        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","54        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","55        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","56        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","57        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","58        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","\n","          7         8         9   ...  16  17  18  19  20  21  22  23  24  25  \n","0   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","1   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","2   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","3   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","4   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","5   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","6   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","7   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","8   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","9   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","10  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","11  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","12  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","13  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","14  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","15  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","16  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","17  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","18  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","19  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","20  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","21  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","22  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","23  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","24  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","25  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","26  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","27  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","28  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","29  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","30  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","31  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","32  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","33  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","34  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","35  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","36  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","37  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","38  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","39  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","40  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","41  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","42  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","43  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","44       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","45       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","46       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","47       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","48       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","49       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","50       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","51       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","52       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","53       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","54       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","55       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","56       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","57       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","58       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","\n","[59 rows x 26 columns]"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["df_actual"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_name = f\"big_train_{dt.now()}_\"\n","\n","current_dir = os.getcwd()\n","\n","if not os.path.exists(\"./pre_trained_models/\"):\n","    os.makedirs(\"./pre_trained_models/\")\n","\n","path = os.path.join(current_dir, \"./pre_trained_models/\")\n","\n","\n","ckpt_dir = f\"./pre_trained_models/{model_name}\"\n","\n","# checkpoints.save_checkpoint(\n","#     ckpt_dir=path, target=state, step=batch_count, overwrite=True, prefix=model_name\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vz702qVy_AKz"},"outputs":[{"ename":"AttributeError","evalue":"'ArrayImpl' object has no attribute 'categorical'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m     memory_usage_bytes \u001b[38;5;241m=\u001b[39m bytes_per_element \u001b[38;5;241m*\u001b[39m total_elements\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m memory_usage_bytes\n\u001b[0;32m---> 12\u001b[0m cat_memory_usage \u001b[38;5;241m=\u001b[39m jax_array_memory_usage(\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical\u001b[49m)\n\u001b[1;32m     13\u001b[0m num_memory_usage \u001b[38;5;241m=\u001b[39m jax_array_memory_usage(batch\u001b[38;5;241m.\u001b[39mnumeric)\n\u001b[1;32m     14\u001b[0m memory_usage \u001b[38;5;241m=\u001b[39m cat_memory_usage \u001b[38;5;241m+\u001b[39m num_memory_usage\n","\u001b[0;31mAttributeError\u001b[0m: 'ArrayImpl' object has no attribute 'categorical'"]}],"source":["def jax_array_memory_usage(array):\n","    \"\"\"Calculate the memory usage of a JAX array in bytes.\"\"\"\n","    # Get the number of bytes per element based on the data type\n","    bytes_per_element = array.dtype.itemsize\n","    # Calculate the total number of elements in the array\n","    total_elements = np.prod(array.shape)\n","    # Calculate total memory usage\n","    memory_usage_bytes = bytes_per_element * total_elements\n","    return memory_usage_bytes\n","\n","\n","cat_memory_usage = jax_array_memory_usage(batch.categorical)\n","num_memory_usage = jax_array_memory_usage(batch.numeric)\n","memory_usage = cat_memory_usage + num_memory_usage\n","memory_usage_gb = memory_usage / 1024 / 1024 / 1024\n","print(f\"Memory usage: {memory_usage} bytes\")\n","print(f\"Memory usage: {memory_usage_gb} gb\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import jax\n","import jax.numpy as jnp\n","\n","\n","def create_future_mask(seq_len):\n","    # Create a basic mask for future sequences\n","    mask = jnp.triu(jnp.ones((seq_len, seq_len)), k=1).astype(bool)\n","    return mask\n","\n","\n","def create_custom_mask(batch_size, seq_len, n_columns):\n","    # Create the future sequence mask\n","    future_mask = create_future_mask(seq_len)\n","\n","    # Expand the mask to match the required dimensions\n","    mask = jnp.expand_dims(future_mask, axis=0)  # Shape: (1, seq_len, seq_len)\n","    mask = jnp.expand_dims(mask, axis=-1)  # Shape: (1, seq_len, seq_len, 1)\n","    mask = jnp.tile(\n","        mask, (batch_size, 1, 1, n_columns)\n","    )  # Shape: (batch_size, seq_len, seq_len, n_columns)\n","\n","    return mask\n","\n","\n","# Example usage:\n","batch_size = 2\n","seq_len = 5\n","n_columns = 3\n","embedding_dim = 4\n","\n","mask = create_custom_mask(batch_size, seq_len, n_columns)\n","\n","print(mask)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import jax\n","import jax.numpy as jnp\n","from jax import random\n","\n","\n","# Create the custom mask function\n","def create_future_mask(seq_len):\n","    # Create a basic mask for future sequences\n","    mask = jnp.triu(jnp.ones((seq_len, seq_len)), k=1).astype(bool)\n","    return mask\n","\n","\n","def create_custom_mask(batch_size, seq_len, n_columns):\n","    # Create the future sequence mask\n","    future_mask = create_future_mask(seq_len)\n","\n","    # Expand the mask to match the required dimensions\n","    mask = jnp.expand_dims(future_mask, axis=0)  # Shape: (1, seq_len, seq_len)\n","    mask = jnp.expand_dims(mask, axis=-1)  # Shape: (1, seq_len, seq_len, 1)\n","    mask = jnp.tile(\n","        mask, (batch_size, 1, 1, n_columns)\n","    )  # Shape: (batch_size, seq_len, seq_len, n_columns)\n","\n","    return mask\n","\n","\n","# Generate random input data\n","key = random.PRNGKey(0)\n","batch_size = 2\n","seq_len = 5\n","n_columns = 3\n","embedding_dim = 4\n","\n","input_data = random.normal(key, (batch_size, seq_len, n_columns, embedding_dim))\n","print(\"Input Data:\")\n","print(input_data)\n","\n","# Create the mask\n","mask = create_custom_mask(batch_size, seq_len, n_columns)\n","print(\"Mask:\")\n","print(mask)\n","\n","# Apply the mask to the input data (for demonstration purposes, we'll just print the mask applied to some dummy attention scores)\n","dummy_attention_scores = random.normal(key, (batch_size, seq_len, seq_len, n_columns))\n","masked_attention_scores = jnp.where(mask, -jnp.inf, dummy_attention_scores)\n","\n","print(\"Dummy Attention Scores:\")\n","print(dummy_attention_scores)\n","\n","print(\"Masked Attention Scores:\")\n","print(masked_attention_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from flax import linen as nn\n","\n","?nn.MultiHeadAttention"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import jax\n","import jax.numpy as jnp\n","from jax import random\n","from flax import linen as nn\n","\n","\n","class MyModel(nn.Module):\n","    n_heads: int\n","    embedding_dim: int\n","\n","    @nn.compact\n","    def __call__(self, x, mask):\n","        batch_size, seq_len, n_columns, embedding_dim = x.shape\n","\n","        # Merge seq_len and n_columns dimensions\n","        x_reshaped = x.reshape(batch_size, seq_len * n_columns, embedding_dim)\n","\n","        # Project the inputs to query, key, and value\n","        qkv_features = self.embedding_dim // self.n_heads\n","        q = nn.DenseGeneral(features=(self.n_heads, qkv_features))(x_reshaped)\n","        k = nn.DenseGeneral(features=(self.n_heads, qkv_features))(x_reshaped)\n","        v = nn.DenseGeneral(features=(self.n_heads, qkv_features))(x_reshaped)\n","\n","        # Compute attention logits\n","        attn_logits = jnp.einsum(\"bthd,bThd->bhtT\", q, k)\n","\n","        # Apply mask: broadcast mask to match attention logits shape\n","        attn_logits = jnp.where(mask, -jnp.inf, attn_logits)\n","\n","        # Compute attention weights\n","        attn_weights = nn.softmax(attn_logits, axis=-1)\n","\n","        # Compute the attention output\n","        attn_output = jnp.einsum(\"bhtT,bThd->bthd\", attn_weights, v)\n","\n","        # Combine heads and reshape back to original dimensions\n","        attn_output = attn_output.reshape(\n","            batch_size, seq_len, n_columns, self.embedding_dim\n","        )\n","\n","        return attn_output\n","\n","\n","def create_future_mask(seq_len):\n","    # Create a basic mask for future sequences\n","    mask = jnp.triu(jnp.ones((seq_len, seq_len)), k=1).astype(bool)\n","    return mask\n","\n","\n","def create_custom_mask(batch_size, seq_len, n_columns, num_heads):\n","    # Create the future sequence mask\n","    future_mask = create_future_mask(seq_len)\n","\n","    # Expand the mask to match the required dimensions for attention\n","    mask = jnp.expand_dims(future_mask, axis=0)  # Shape: (1, seq_len, seq_len)\n","    mask = jnp.expand_dims(mask, axis=1)  # Shape: (1, 1, seq_len, seq_len)\n","    mask = jnp.tile(\n","        mask, (batch_size, num_heads, 1, 1)\n","    )  # Shape: (batch_size, num_heads, seq_len, seq_len)\n","\n","    return mask\n","\n","\n","# Generate random input data\n","key = random.PRNGKey(0)\n","batch_size = 2\n","seq_len = 5\n","n_columns = 3\n","embedding_dim = 16\n","num_heads = 4\n","\n","input_data = random.normal(key, (batch_size, seq_len, n_columns, embedding_dim))\n","print(\"Input Data:\")\n","print(input_data)\n","\n","# Create the mask\n","mask = create_custom_mask(batch_size, seq_len, n_columns, num_heads)\n","print(\"Mask:\")\n","print(mask)\n","\n","# Initialize and apply the model\n","model = MyModel(n_heads=num_heads, embedding_dim=embedding_dim)\n","variables = model.init(key, input_data, mask)\n","output = model.apply(variables, input_data, mask)\n","\n","print(\"Output:\")\n","print(output)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import jax\n","import jax.numpy as jnp\n","from jax import random\n","from flax import linen as nn\n","\n","\n","class MyModel(nn.Module):\n","    n_heads: int\n","\n","    @nn.compact\n","    def __call__(self, x, mask):\n","        # Apply multi-head attention with the custom mask\n","        attn = nn.MultiHeadAttention(\n","            num_heads=self.n_heads, qkv_features=16, use_bias=False\n","        )(x, mask=mask)\n","        return attn\n","\n","\n","def create_future_mask(seq_len):\n","    # Create a basic mask for future sequences\n","    mask = jnp.triu(jnp.ones((seq_len, seq_len)), k=1).astype(bool)\n","    return mask\n","\n","\n","def create_custom_mask(batch_size, seq_len, num_heads):\n","    # Create the future sequence mask\n","    future_mask = create_future_mask(seq_len)\n","\n","    # Expand the mask to match the required dimensions for attention\n","    mask = jnp.expand_dims(future_mask, axis=0)  # Shape: (1, seq_len, seq_len)\n","    mask = jnp.expand_dims(mask, axis=1)  # Shape: (1, 1, seq_len, seq_len)\n","    mask = jnp.tile(\n","        mask, (batch_size, num_heads, 1, 1)\n","    )  # Shape: (batch_size, num_heads, seq_len, seq_len)\n","\n","    return mask\n","\n","\n","# Generate random input data\n","key = random.PRNGKey(0)\n","batch_size = 2\n","seq_len = 5\n","n_columns = 3\n","embedding_dim = 16\n","num_heads = 4\n","\n","input_data = random.normal(key, (batch_size, seq_len, n_columns, embedding_dim))\n","# print(\"Input Data:\")\n","# print(input_data)\n","\n","# Create the mask\n","mask = create_custom_mask(batch_size, seq_len, num_heads)\n","# print(\"Mask:\")\n","# print(mask)\n","\n","# Initialize and apply the model\n","model = MyModel(n_heads=num_heads)\n","variables = model.init(key, input_data, mask)\n","output = model.apply(variables, input_data, mask)\n","\n","print(\"Output:\")\n","print(output)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import jax\n","import jax.numpy as jnp\n","from jax import random\n","from flax import linen as nn\n","\n","\n","class MyModel(nn.Module):\n","    n_heads: int\n","\n","    @nn.compact\n","    def __call__(self, x, mask):\n","        # Reshape input to combine seq_len and n_columns\n","        batch_size, seq_len, n_columns, embedding_dim = x.shape\n","        x = x.reshape(batch_size, seq_len * n_columns, embedding_dim)\n","\n","        mask = nn.make_causal_mask(x)\n","        print(mask.shape)  #                                        (2, 15, 1, 16, 16)\n","        # ValueError: Incompatible shapes for broadcasting: shapes=[(2, 15, 1, 16, 16),\n","        #                                                           (2, 4, 15, 15), ()]\n","        mask = mask.reshape(batch_size, seq_len, n_columns, seq_len, n_columns)\n","        # Apply multi-head attention with the custom mask\n","        attn = nn.MultiHeadAttention(\n","            num_heads=self.n_heads, qkv_features=embedding_dim, use_bias=False\n","        )(x, mask=mask)\n","        print(attn.shape)\n","        # Reshape the output back to the original shape\n","        attn = attn.reshape(batch_size, seq_len, n_columns, embedding_dim)\n","        return attn\n","\n","\n","# Generate random input data\n","key = random.PRNGKey(0)\n","batch_size = 2\n","seq_len = 5\n","n_columns = 3\n","embedding_dim = 16\n","num_heads = 4\n","\n","input_data = random.normal(key, (batch_size, seq_len, n_columns, embedding_dim))\n","\n","# Create the mask\n","mask = create_custom_mask(batch_size, seq_len, num_heads, n_columns)\n","\n","# Initialize and apply the model\n","model = MyModel(n_heads=num_heads)\n","variables = model.init(key, input_data, mask)\n","output = model.apply(variables, input_data, mask)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nn.make_causal_mask(jnp.ones((2, 5, 3, 16))).shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import jax.numpy as jnp\n","import flax.linen as nn\n","from flax.linen import partitioning\n","from typing import Any\n","\n","\n","class DecoderBlock(nn.Module):\n","    num_heads: int\n","    qkv_dim: int\n","    mlp_dim: int\n","    dropout_rate: float\n","\n","    def setup(self):\n","        self.self_attention = nn.MultiHeadDotProductAttention(\n","            num_heads=self.num_heads,\n","            qkv_features=self.qkv_dim,\n","            kernel_init=nn.initializers.xavier_uniform(),\n","            bias_init=nn.initializers.normal(stddev=1e-6),\n","            dropout_rate=self.dropout_rate,\n","        )\n","        self.mlp = nn.Sequential(\n","            [nn.Dense(self.mlp_dim), nn.relu, nn.Dense(self.qkv_dim)]\n","        )\n","        self.layer_norm1 = nn.LayerNorm()\n","        self.layer_norm2 = nn.LayerNorm()\n","        self.dropout = nn.Dropout(rate=self.dropout_rate)\n","\n","    def __call__(self, x, causal_mask, deterministic):\n","        # Self-attention block\n","        x = self.layer_norm1(x)\n","        attn_out = self.self_attention(\n","            query=x, key=x, value=x, mask=causal_mask, deterministic=deterministic\n","        )\n","        x = x + self.dropout(attn_out, deterministic=deterministic)\n","\n","        # Feed-forward block\n","        x = self.layer_norm2(x)\n","        mlp_out = self.mlp(x)\n","        x = x + self.dropout(mlp_out, deterministic=deterministic)\n","\n","        return x\n","\n","\n","class Decoder(nn.Module):\n","    vocab_size: int\n","    num_layers: int\n","    num_heads: int\n","    qkv_dim: int\n","    mlp_dim: int\n","    max_len: int\n","    dropout_rate: float\n","\n","    def setup(self):\n","        self.token_embedding = nn.Embed(self.vocab_size, self.qkv_dim)\n","        self.position_embedding = nn.Embed(self.max_len, self.qkv_dim)\n","        self.decoder_blocks = [\n","            DecoderBlock(\n","                num_heads=self.num_heads,\n","                qkv_dim=self.qkv_dim,\n","                mlp_dim=self.mlp_dim,\n","                dropout_rate=self.dropout_rate,\n","            )\n","            for _ in range(self.num_layers)\n","        ]\n","\n","    def __call__(self, x, deterministic=True):\n","        # Embedding and positional encoding\n","        seq_len = x.shape[1]\n","        x = self.token_embedding(x) + self.position_embedding(jnp.arange(seq_len))\n","\n","        # Create the causal mask\n","        causal_mask = nn.make_causal_mask(x)\n","\n","        # Apply decoder blocks\n","        for block in self.decoder_blocks:\n","            x = block(x, causal_mask, deterministic)\n","\n","        return x\n","\n","\n","# Example usage\n","vocab_size = 32000\n","num_layers = 6\n","num_heads = 8\n","qkv_dim = 512\n","mlp_dim = 2048\n","max_len = 512\n","dropout_rate = 0.1\n","\n","decoder = Decoder(\n","    vocab_size=vocab_size,\n","    num_layers=num_layers,\n","    num_heads=num_heads,\n","    qkv_dim=qkv_dim,\n","    mlp_dim=mlp_dim,\n","    max_len=max_len,\n","    dropout_rate=dropout_rate,\n",")\n","\n","# Create a random input sequence of token IDs\n","import jax.random as random\n","\n","key = random.PRNGKey(0)\n","x = random.randint(key, (1, max_len), 0, vocab_size)\n","\n","# Forward pass\n","deterministic = True\n","output = decoder(x, deterministic=deterministic)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import jax\n","import jax.numpy as jnp\n","from flax import linen as nn\n","from flax.training import train_state\n","from typing import Any\n","\n","\n","class DecoderBlock(nn.Module):\n","    num_heads: int\n","    embed_dim: int\n","    mlp_dim: int\n","\n","    @nn.compact\n","    def __call__(self, x, mask=None):\n","        # Self-attention block\n","        x = nn.LayerNorm()(x)\n","        x = nn.SelfAttention(\n","            num_heads=self.num_heads,\n","            qkv_features=self.embed_dim,\n","            kernel_init=nn.initializers.xavier_uniform(),\n","        )(x, mask=mask)\n","        x = nn.Dropout(0.1)(x, deterministic=True)\n","        residual = x\n","\n","        # Feed-forward block\n","        x = nn.LayerNorm()(x)\n","        x = nn.Dense(self.mlp_dim, kernel_init=nn.initializers.xavier_uniform())(x)\n","        x = nn.relu(x)\n","        x = nn.Dense(self.embed_dim, kernel_init=nn.initializers.xavier_uniform())(x)\n","        x = nn.Dropout(0.1)(x, deterministic=True)\n","\n","        return residual + x\n","\n","\n","class SimpleDecoder(nn.Module):\n","    num_layers: int\n","    num_heads: int\n","    embed_dim: int\n","    mlp_dim: int\n","\n","    @nn.compact\n","    def __call__(self, x):\n","        # Create a causal mask\n","        mask = nn.make_causal_mask(x)\n","\n","        for _ in range(self.num_layers):\n","            x = DecoderBlock(self.num_heads, self.embed_dim, self.mlp_dim)(x, mask)\n","\n","        return x\n","\n","\n","# Example usage\n","key = jax.random.PRNGKey(0)\n","x = jax.random.normal(\n","    key, (1, 10, 64)\n",")  # Example input: batch size 1, sequence length 10, embedding size 64\n","\n","model = SimpleDecoder(num_layers=2, num_heads=8, embed_dim=64, mlp_dim=256)\n","params = model.init(key, x)\n","y = model.apply(params, x)\n","\n","print(y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import jax\n","import jax.numpy as jnp\n","from flax import linen as nn\n","from flax.training import train_state\n","from typing import Any\n","\n","\n","class DecoderBlock(nn.Module):\n","    num_heads: int\n","    embed_dim: int\n","    mlp_dim: int\n","\n","    @nn.compact\n","    def __call__(self, x, mask=None):\n","        # Self-attention block\n","        x = nn.LayerNorm()(x)\n","        x = nn.SelfAttention(\n","            num_heads=self.num_heads,\n","            qkv_features=self.embed_dim,\n","            kernel_init=nn.initializers.xavier_uniform(),\n","        )(x, mask=mask)\n","        x = nn.Dropout(0.1)(x, deterministic=True)\n","        residual = x\n","\n","        # Feed-forward block\n","        x = nn.LayerNorm()(x)\n","        x = nn.Dense(self.mlp_dim, kernel_init=nn.initializers.xavier_uniform())(x)\n","        x = nn.relu(x)\n","        x = nn.Dense(self.embed_dim, kernel_init=nn.initializers.xavier_uniform())(x)\n","        x = nn.Dropout(0.1)(x, deterministic=True)\n","\n","        return residual + x\n","\n","\n","class SimpleDecoder(nn.Module):\n","    num_layers: int\n","    num_heads: int\n","    embed_dim: int\n","    mlp_dim: int\n","\n","    @nn.compact\n","    def __call__(self, x):\n","        # Create a causal mask\n","        mask = nn.make_causal_mask(x)\n","\n","        for _ in range(self.num_layers):\n","            x = DecoderBlock(self.num_heads, self.embed_dim, self.mlp_dim)(x, mask)\n","\n","        return x\n","\n","\n","# Example usage\n","key = jax.random.PRNGKey(0)\n","x = jax.random.normal(\n","    key, (1, 10, 64)\n",")  # Example input: batch size 1, sequence length 10, embedding size 64\n","\n","model = SimpleDecoder(num_layers=2, num_heads=8, embed_dim=64, mlp_dim=256)\n","params = model.init(key, x)\n","y = model.apply(params, x)\n","\n","print(y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import (\n","    Layer,\n","    LayerNormalization,\n","    Dense,\n","    Dropout,\n","    MultiHeadAttention,\n",")\n","\n","\n","class DecoderBlock(Layer):\n","    def __init__(self, num_heads, embed_dim, mlp_dim, dropout_rate=0.1):\n","        super(DecoderBlock, self).__init__()\n","        self.num_heads = num_heads\n","        self.embed_dim = embed_dim\n","        self.mlp_dim = mlp_dim\n","        self.dropout_rate = dropout_rate\n","\n","        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = tf.keras.Sequential(\n","            [Dense(mlp_dim, activation=\"relu\"), Dense(embed_dim)]\n","        )\n","        self.layernorm1 = LayerNormalization()\n","        self.layernorm2 = LayerNormalization()\n","        self.dropout1 = Dropout(dropout_rate)\n","        self.dropout2 = Dropout(dropout_rate)\n","\n","    def call(self, x, training, mask=None):\n","        attn_output = self.attention(x, x, attention_mask=mask)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)\n","\n","        ffn_output = self.dense_proj(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","\n","class SimpleDecoder(Layer):\n","    def __init__(self, num_layers, num_heads, embed_dim, mlp_dim, dropout_rate=0.1):\n","        super(SimpleDecoder, self).__init__()\n","        self.num_layers = num_layers\n","        self.embed_dim = embed_dim\n","        self.mlp_dim = mlp_dim\n","\n","        self.dec_layers = [\n","            DecoderBlock(num_heads, embed_dim, mlp_dim, dropout_rate)\n","            for _ in range(num_layers)\n","        ]\n","\n","    def call(self, x, training, mask=None):\n","        for i in range(self.num_layers):\n","            x = self.dec_layers[i](x, training, mask)\n","        return x\n","\n","\n","def create_causal_mask(seq_len):\n","    mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","    return mask\n","\n","\n","# Example usage\n","batch_size = 1\n","seq_len = 10\n","embed_dim = 64\n","\n","# Input tensor\n","x = tf.random.normal((batch_size, seq_len, embed_dim))\n","\n","# Create the model\n","num_layers = 2\n","num_heads = 8\n","mlp_dim = 256\n","dropout_rate = 0.1\n","\n","decoder = SimpleDecoder(num_layers, num_heads, embed_dim, mlp_dim, dropout_rate)\n","\n","# Create a causal mask\n","causal_mask = create_causal_mask(seq_len)\n","causal_mask = causal_mask[tf.newaxis, tf.newaxis, :, :]  # (1, 1, seq_len, seq_len)\n","\n","# Apply the decoder\n","output = decoder(x, training=True, mask=causal_mask)\n","\n","print(output)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP1swsXnq2jqt/Hz4IBTZm2","gpuType":"V100","machine_shape":"hm","mount_file_id":"1zHmvVqlKJh0x9vjCRSkYKkko5buvMMEd","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"01ca2dcb14e647dfb1e68750ca6de39c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a51aa7b827db4f768dce6315dbebf379","IPY_MODEL_634935ff0a6b4d65b72f87359d9f82fc","IPY_MODEL_09b63c98bee543dfb557a007d50c41d1"],"layout":"IPY_MODEL_dc4a8a3b0a8b4ef49dcd6269a1b32e14"}},"0481743e80634f7a8e718fb528950e54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"055681c8159b4b6c8104d4e06279803c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09b63c98bee543dfb557a007d50c41d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fb36a08ca1540228919af722727572c","placeholder":"​","style":"IPY_MODEL_3bd47d84fac442d4bcf49dceeb699d45","value":" 807/807 [04:29&lt;00:00,  3.10it/s]"}},"0e26d28846fb449789510e4748c01c6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25ce5408138f4cb28e163f1fdffdee5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b35a70a3b9e4f3b87cf2a7280439ac6","placeholder":"​","style":"IPY_MODEL_0e26d28846fb449789510e4748c01c6a","value":"100%"}},"2ac0cceac38e43adb00a10b778fad2df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddb1791795134ac0a754748f9793c214","placeholder":"​","style":"IPY_MODEL_d95aea2b915b44f38b5090ee186abafd","value":" 807/807 [04:31&lt;00:00,  2.80it/s]"}},"2fb36a08ca1540228919af722727572c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31d37adf891a4da68675945509051210":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_055681c8159b4b6c8104d4e06279803c","placeholder":"​","style":"IPY_MODEL_9f5e5843559b4f9e8f3440ebd85743f8","value":" 807/807 [04:52&lt;00:00,  3.03it/s]"}},"3bd47d84fac442d4bcf49dceeb699d45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4151c7353de54909aa9deecbe8c1e1e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"462d9e93a94f44868fec1ece82f0a241":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47793d3155614c8cbd7d3337dcb2f895":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8093c8ddeec4c55ac6cc5f8f7df30bf","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4151c7353de54909aa9deecbe8c1e1e1","value":3}},"4bc81f6d94394d7f90e070d8b11a7059":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4da9993585914618a35d8d5382fef850":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"4dd64f565fcf4b259ef24944493477bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_25ce5408138f4cb28e163f1fdffdee5f","IPY_MODEL_47793d3155614c8cbd7d3337dcb2f895","IPY_MODEL_f9ec11073e484927a4cc1ee2e3da132f"],"layout":"IPY_MODEL_62e750769d364ab2b80d5b9646b10ea6"}},"5e15315a77864badafe48c2baf31b030":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dfd9dd713862479bba73b6c0a12a1902","IPY_MODEL_88be535ce5a945f4979fe72d9f086365","IPY_MODEL_31d37adf891a4da68675945509051210"],"layout":"IPY_MODEL_4da9993585914618a35d8d5382fef850"}},"5edd903b07594d128ded9b4844835159":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bc81f6d94394d7f90e070d8b11a7059","max":807,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d3bc58d04d5f42a5a24bd467a9569e01","value":807}},"5f1fb4d40e154e27baa0d4f330df540e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62e750769d364ab2b80d5b9646b10ea6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"634935ff0a6b4d65b72f87359d9f82fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4de75777bba4d72b61936baf3d84cbb","max":807,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0481743e80634f7a8e718fb528950e54","value":807}},"6f5ca39406174a4fb1bee9eb1e35fccb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70c7b641da5c4b82bbd5b2e1750f3b39":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"851b29291123491a821b1ce8088ca785":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"88be535ce5a945f4979fe72d9f086365":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8382d8bbb7f42f6a0f4d4028203615f","max":807,"min":0,"orientation":"horizontal","style":"IPY_MODEL_851b29291123491a821b1ce8088ca785","value":807}},"8b35a70a3b9e4f3b87cf2a7280439ac6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"948787e6434f419a86d9d7da831e2572":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ced4485d5c641eda90ab0634e0691d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f5e5843559b4f9e8f3440ebd85743f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a317a54aa1a349f490e388aacb2d1e4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c470a61e692a4459988f63f0024f7eac","IPY_MODEL_5edd903b07594d128ded9b4844835159","IPY_MODEL_2ac0cceac38e43adb00a10b778fad2df"],"layout":"IPY_MODEL_c7f61bdee3ca4ca0a3f7d021aa558deb"}},"a51aa7b827db4f768dce6315dbebf379":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f1fb4d40e154e27baa0d4f330df540e","placeholder":"​","style":"IPY_MODEL_948787e6434f419a86d9d7da831e2572","value":"100%"}},"acee2c1ed91e44188d5ca40bddace4b1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c470a61e692a4459988f63f0024f7eac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70c7b641da5c4b82bbd5b2e1750f3b39","placeholder":"​","style":"IPY_MODEL_462d9e93a94f44868fec1ece82f0a241","value":"100%"}},"c7f61bdee3ca4ca0a3f7d021aa558deb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"cc57eeb92d32434ca82b7c3f669865d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3bc58d04d5f42a5a24bd467a9569e01":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d95aea2b915b44f38b5090ee186abafd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc4a8a3b0a8b4ef49dcd6269a1b32e14":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"ddb1791795134ac0a754748f9793c214":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfd9dd713862479bba73b6c0a12a1902":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acee2c1ed91e44188d5ca40bddace4b1","placeholder":"​","style":"IPY_MODEL_cc57eeb92d32434ca82b7c3f669865d1","value":"100%"}},"e4de75777bba4d72b61936baf3d84cbb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8093c8ddeec4c55ac6cc5f8f7df30bf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8382d8bbb7f42f6a0f4d4028203615f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9ec11073e484927a4cc1ee2e3da132f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ced4485d5c641eda90ab0634e0691d3","placeholder":"​","style":"IPY_MODEL_6f5ca39406174a4fb1bee9eb1e35fccb","value":" 3/3 [13:53&lt;00:00, 275.36s/it]"}}}}},"nbformat":4,"nbformat_minor":0}
