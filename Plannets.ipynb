{"cells":[{"cell_type":"markdown","metadata":{"id":"trbcfMV6vked"},"source":["<https://github.com/PolymathicAI/xVal>\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"yKsx1R26dLGC"},"outputs":[],"source":["import os\n","\n","os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.95\""]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2102,"status":"ok","timestamp":1708903394514,"user":{"displayName":"Kai Lukowiak","userId":"12340107642472090190"},"user_tz":420},"id":"RYm4WOeS1B8x","outputId":"a5c71de2-9597-4fcb-a9a7-b743f1a29feb"},"outputs":[{"data":{"text/plain":["{CpuDevice(id=0)}"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import jax.numpy as jnp  # Oddly works in colab to set gpu\n","\n","arr = jnp.array([1, 2, 3])\n","arr.devices()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Jqmm_5s9KXkI"},"outputs":[],"source":["import icecream\n","from icecream import ic\n","\n","icecream.install()\n","ic_disable = True\n","if ic_disable:\n","    ic.disable()\n","ic.configureOutput(includeContext=True, contextAbsPath=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18045,"status":"ok","timestamp":1708903436762,"user":{"displayName":"Kai Lukowiak","userId":"12340107642472090190"},"user_tz":420},"id":"oSKniUdxtiTd","outputId":"49871258-6ef7-4956-ad3b-90af827f7253"},"outputs":[],"source":["import os\n","import ast\n","\n","from datetime import datetime as dt\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","import hephaestus as hp\n","import jax\n","import jax.numpy as jnp\n","import numpy as np\n","import optax\n","import pandas as pd\n","from flax.training import train_state\n","from icecream import ic\n","from jax import random\n","from flax import struct\n","from jax.tree_util import tree_flatten\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm.notebook import tqdm, trange\n","\n","pd.options.mode.copy_on_write = True"]},{"cell_type":"markdown","metadata":{},"source":["\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def line2df(line, idx):\n","    data_rows = []\n","    line = ast.literal_eval(line)\n","    for i, time_step in enumerate(line[\"data\"]):\n","        row = {\"time_step\": i}\n","        # Add position data for each planet\n","        for j, position in enumerate(time_step):\n","            row[f\"planet{j}_x\"] = position[0]\n","            row[f\"planet{j}_y\"] = position[1]\n","        data_rows.append(row)\n","\n","    df = pd.DataFrame(data_rows)\n","    description = line.pop(\"description\")\n","    step_size = description.pop(\"stepsize\")\n","    for k, v in description.items():\n","        for k_prop, v_prop in v.items():\n","            df[f\"{k}_{k_prop}\"] = v_prop\n","    df[\"time_step\"] = df[\"time_step\"] * step_size\n","    df.insert(0, \"idx\", idx)\n","\n","    return df"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["files = os.listdir(\"data\")\n","if \"planets.parquet\" not in files:\n","    with open(\"data/planets.data\") as f:\n","        data = f.read().splitlines()\n","\n","        dfs = []\n","        for idx, line in enumerate(tqdm(data)):\n","            dfs.append(line2df(line, idx))\n","        print(\"Concatenating dfs...\")\n","        df = pd.concat(dfs)\n","    df.to_parquet(\"data/planets.parquet\")\n","else:\n","    df = pd.read_parquet(\"data/planets.parquet\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["min     30.000000\n","mean    44.511656\n","max     59.000000\n","Name: time_step, dtype: float64"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Get min, mean, and max number of time steps\n","df.groupby(\"idx\").count().time_step.agg([\"min\", \"mean\", \"max\"])"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class SimpleDS(Dataset):\n","    def __init__(self, df):\n","        # Add nan padding to make sure all sequences are the same length\n","        # use the idx column to group by\n","        self.max_seq_len = df.groupby(\"idx\").count().time_step.max()\n","\n","        self.df = df\n","        self.batch_size = self.max_seq_len\n","\n","        self.special_tokens = [\"[PAD]\", \"[NUMERIC_MASK]\", \"[MASK]\"]\n","        self.cat_mask = \"[MASK]\"\n","        self.numeric_mask = \"[NUMERIC_MASK]\"\n","\n","        self.col_tokens = [col_name for col_name in df.columns if col_name != \"idx\"]\n","\n","        self.tokens = self.special_tokens + self.col_tokens\n","\n","        self.token_dict = {token: i for i, token in enumerate(self.tokens)}\n","        self.token_decoder_dict = {i: token for i, token in enumerate(self.tokens)}\n","        self.n_tokens = len(self.tokens)\n","        self.numeric_indices = jnp.array(\n","            [self.tokens.index(i) for i in self.col_tokens]\n","        )\n","\n","        self.numeric_mask_token = self.tokens.index(self.numeric_mask)\n","\n","    def __len__(self):\n","        return df.idx.max() + 1  # probably should be max idx + 1 thanks\n","\n","    def __getitem__(self, set_idx):\n","        batch = self.df.loc[\n","            df.idx == set_idx, [col for col in self.df.columns if col != \"idx\"]\n","        ]\n","        batch = np.array(batch.values)\n","        # Add padding\n","        batch_len, n_cols = batch.shape\n","        pad_len = self.max_seq_len - batch_len\n","        padding = np.full((pad_len, n_cols), jnp.nan)\n","        batch = np.concatenate([batch, padding], axis=0)\n","        return batch"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["array([[nan, nan, nan],\n","       [nan, nan, nan],\n","       [nan, nan, nan]])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["np.full((3, 3), np.nan)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"4RZof2SNKXkK"},"outputs":[],"source":["train_ds = SimpleDS(df)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Y4b7IkMWKXkK"},"outputs":[],"source":["time_series_regressor = hp.simple_time_series.SimplePred(train_ds, d_model=64 * 4)"]},{"cell_type":"markdown","metadata":{},"source":["\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["array([[ 0.        ,  1.56006022, -0.85443699, ...,         nan,\n","                nan,         nan],\n","       [ 0.46511628,  1.68985799, -0.5143588 , ...,         nan,\n","                nan,         nan],\n","       [ 0.93023256,  1.75358875, -0.15420858, ...,         nan,\n","                nan,         nan],\n","       ...,\n","       [        nan,         nan,         nan, ...,         nan,\n","                nan,         nan],\n","       [        nan,         nan,         nan, ...,         nan,\n","                nan,         nan],\n","       [        nan,         nan,         nan, ...,         nan,\n","                nan,         nan]])"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["train_ds[0]"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def make_batch(ds: SimpleDS, start: int, length: int):\n","    data = []\n","    for i in range(start, length + start):\n","        data.append(ds[i])\n","\n","    return jnp.array(data)\n","\n","\n","batch = make_batch(train_ds, 0, 4)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["vars = time_series_regressor.init(random.PRNGKey(0), batch)\n","\n","x = time_series_regressor.apply(vars, batch)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["Array([[[-0.01003024, -0.01003027, -0.01003026, ..., -0.01003027,\n","         -0.01003024, -0.01003024],\n","        [ 0.00563828,  0.00563828,  0.00563826, ...,  0.00563823,\n","          0.00563826,  0.00563826],\n","        [ 0.00717834,  0.00717837,  0.00717835, ...,  0.00717836,\n","          0.00717835,  0.00717835],\n","        ...,\n","        [-0.15934142, -0.15934142, -0.15934142, ..., -0.15934142,\n","         -0.15934141, -0.15934141],\n","        [-0.18229702, -0.18229702, -0.18229702, ..., -0.18229702,\n","         -0.18229693, -0.18229693],\n","        [-0.14743754, -0.14743754, -0.14743754, ..., -0.14743754,\n","         -0.14743745, -0.14743745]],\n","\n","       [[ 0.01027126,  0.01027129,  0.01027127, ...,  0.01027128,\n","          0.01027131,  0.01027131],\n","        [ 0.03700557,  0.03700562,  0.03700558, ...,  0.0370056 ,\n","          0.03700559,  0.03700559],\n","        [ 0.04097863,  0.04097859,  0.04097862, ...,  0.04097861,\n","          0.04097862,  0.04097862],\n","        ...,\n","        [-0.15934142, -0.15934142, -0.15934142, ..., -0.15934142,\n","         -0.15934141, -0.15934141],\n","        [-0.18229702, -0.18229702, -0.18229702, ..., -0.18229702,\n","         -0.18229693, -0.18229693],\n","        [-0.14743754, -0.14743754, -0.14743754, ..., -0.14743754,\n","         -0.14743745, -0.14743745]],\n","\n","       [[-0.03042472, -0.03042475, -0.03042477, ..., -0.03042473,\n","         -0.03042473, -0.03042473],\n","        [-0.019086  , -0.01908599, -0.01908602, ..., -0.01908598,\n","         -0.01908596, -0.01908596],\n","        [-0.02245415, -0.02245414, -0.02245413, ..., -0.02245415,\n","         -0.0224542 , -0.0224542 ],\n","        ...,\n","        [-0.15934142, -0.15934142, -0.15934142, ..., -0.15934142,\n","         -0.15934141, -0.15934141],\n","        [-0.18229702, -0.18229702, -0.18229702, ..., -0.18229702,\n","         -0.18229693, -0.18229693],\n","        [-0.14743754, -0.14743754, -0.14743754, ..., -0.14743754,\n","         -0.14743745, -0.14743745]],\n","\n","       [[ 0.01363335,  0.01363333,  0.01363335, ...,  0.01363332,\n","          0.0136334 ,  0.01363334],\n","        [ 0.04608091,  0.04608093,  0.04608094, ...,  0.04608098,\n","          0.04608092,  0.04608095],\n","        [ 0.05953463,  0.05953467,  0.05953464, ...,  0.05953467,\n","          0.0595346 ,  0.05953459],\n","        ...,\n","        [-0.15934142, -0.15934142, -0.15934142, ..., -0.15934142,\n","         -0.15934141, -0.15934141],\n","        [-0.18229702, -0.18229702, -0.18229702, ..., -0.18229702,\n","         -0.18229693, -0.18229693],\n","        [-0.14743754, -0.14743754, -0.14743754, ..., -0.14743754,\n","         -0.14743745, -0.14743745]]], dtype=float32)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["x"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["(4, 59, 26)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["batch.shape"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"pMS-gjmiVGsn"},"outputs":[],"source":["def calculate_memory_footprint(params):\n","    \"\"\"Calculate total memory footprint of JAX model parameters.\"\"\"\n","    total_bytes = 0\n","    # Flatten the parameter tree structure into a list of arrays\n","    flat_params, _ = tree_flatten(params)\n","    for param in flat_params:\n","        # Calculate bytes: number of elements * size of each element\n","        bytes_per_param = param.size * param.dtype.itemsize\n","        total_bytes += bytes_per_param\n","    return total_bytes"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["def create_causal_mask(tensor: jnp.ndarray):\n","    \"\"\"Create a causal mask to mask out future values.\"\"\"\n","    mask = jnp.tril(jnp.ones((tensor.shape[0], tensor.shape[1])))\n","    return mask\n","\n","\n","def create_padding_mask(tensor: jnp.ndarray):\n","    \"\"\"Create a padding mask to mask out padded values.\"\"\"\n","    mask = jnp.isnan(tensor)\n","    return mask\n","\n","\n","def mask_array(tensor: jnp.array):\n","    \"\"\"Create a mask for the tensor\"\"\"\n","    causal_mask = create_causal_mask(tensor)\n","    padding_mask = create_padding_mask(tensor)\n","    mask = jnp.logical_or(causal_mask, padding_mask)\n","    return mask"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["(59, 26)"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["jnp.array(train_ds[0]).shape"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["4dd64f565fcf4b259ef24944493477bf","25ce5408138f4cb28e163f1fdffdee5f","47793d3155614c8cbd7d3337dcb2f895","f9ec11073e484927a4cc1ee2e3da132f","62e750769d364ab2b80d5b9646b10ea6","8b35a70a3b9e4f3b87cf2a7280439ac6","0e26d28846fb449789510e4748c01c6a","f8093c8ddeec4c55ac6cc5f8f7df30bf","4151c7353de54909aa9deecbe8c1e1e1","9ced4485d5c641eda90ab0634e0691d3","6f5ca39406174a4fb1bee9eb1e35fccb","5e15315a77864badafe48c2baf31b030","dfd9dd713862479bba73b6c0a12a1902","88be535ce5a945f4979fe72d9f086365","31d37adf891a4da68675945509051210","4da9993585914618a35d8d5382fef850","acee2c1ed91e44188d5ca40bddace4b1","cc57eeb92d32434ca82b7c3f669865d1","f8382d8bbb7f42f6a0f4d4028203615f","851b29291123491a821b1ce8088ca785","055681c8159b4b6c8104d4e06279803c","9f5e5843559b4f9e8f3440ebd85743f8","a317a54aa1a349f490e388aacb2d1e4d","c470a61e692a4459988f63f0024f7eac","5edd903b07594d128ded9b4844835159","2ac0cceac38e43adb00a10b778fad2df","c7f61bdee3ca4ca0a3f7d021aa558deb","70c7b641da5c4b82bbd5b2e1750f3b39","462d9e93a94f44868fec1ece82f0a241","4bc81f6d94394d7f90e070d8b11a7059","d3bc58d04d5f42a5a24bd467a9569e01","ddb1791795134ac0a754748f9793c214","d95aea2b915b44f38b5090ee186abafd","01ca2dcb14e647dfb1e68750ca6de39c","a51aa7b827db4f768dce6315dbebf379","634935ff0a6b4d65b72f87359d9f82fc","09b63c98bee543dfb557a007d50c41d1","dc4a8a3b0a8b4ef49dcd6269a1b32e14","5f1fb4d40e154e27baa0d4f330df540e","948787e6434f419a86d9d7da831e2572","e4de75777bba4d72b61936baf3d84cbb","0481743e80634f7a8e718fb528950e54","2fb36a08ca1540228919af722727572c","3bd47d84fac442d4bcf49dceeb699d45"]},"executionInfo":{"elapsed":841688,"status":"ok","timestamp":1708904334771,"user":{"displayName":"Kai Lukowiak","userId":"12340107642472090190"},"user_tz":420},"id":"RIXu3GkdYzNA","outputId":"dee6137d-c096-4df6-fbb4-baa3764d359e"},"outputs":[],"source":["mts_root_key = random.PRNGKey(44)\n","mts_main_key, ts_params_key, ts_data_key = random.split(mts_root_key, 3)\n","\n","\n","def clip_gradients(gradients, max_norm):\n","    total_norm = jnp.sqrt(sum(jnp.sum(jnp.square(grad)) for grad in gradients.values()))\n","    scale = max_norm / (total_norm + 1e-6)\n","    clipped_gradients = jax.tree_map(\n","        lambda grad: jnp.where(total_norm > max_norm, grad * scale, grad), gradients\n","    )\n","    return clipped_gradients\n","\n","\n","# @jax.jit\n","def calculate_loss(params, state, inputs, dataset: SimpleDS):\n","    out = state.apply_fn(\n","        {\"params\": params},\n","        hp.mask_tensor(inputs, dataset, prng_key=ts_data_key),\n","    )\n","\n","    # Create mask for nan inputs\n","    nan_mask = jnp.isnan(inputs)\n","    inputs = jnp.where(nan_mask, jnp.zeros_like(inputs), inputs)\n","    out = jnp.where(nan_mask, jnp.zeros_like(out), out)\n","\n","    raw_loss = optax.squared_error(out, inputs)\n","    masked_loss = jnp.where(nan_mask, 0.0, raw_loss)\n","    loss = masked_loss.sum() / (~nan_mask).sum()\n","\n","    return loss\n","\n","\n","@jax.jit\n","def eval_step(state: train_state.TrainState, batch):\n","    def loss_fn(params):\n","        return calculate_loss(params, state, batch, train_ds)\n","\n","    # (loss, individual_losses), grad = grad_fn(state.params)\n","    loss, loss_dict = loss_fn(state.params)\n","    return loss, loss_dict\n","\n","\n","@jax.jit\n","def train_step(state: train_state.TrainState, batch):\n","    def loss_fn(params):\n","        return calculate_loss(params, state, batch, train_ds)\n","\n","    grad_fn = jax.value_and_grad(loss_fn)\n","\n","    # (loss, individual_losses), grad = grad_fn(state.params)\n","    loss, grad = grad_fn(state.params)\n","    # grad = replace_nans(grad)\n","    # grad = clip_gradients(grad, 1.0)\n","    state = state.apply_gradients(grads=grad)\n","\n","    return state, loss\n","\n","\n","def create_train_state(model, prng, batch, lr):\n","    params = model.init(prng, batch)\n","    # optimizer = optax.chain(optax.adam(lr))\n","    optimizer = optax.chain(optax.clip_by_global_norm(0.4), optax.adam(lr))\n","    # optimizer_state = optimizer.init(params)\n","    return train_state.TrainState.create(\n","        apply_fn=model.apply,\n","        params=params[\"params\"],\n","        tx=optimizer,\n","        # tx_state=optimizer_state,\n","    )\n","\n","\n","batch_size = 2\n","# batch = train_ds[0]\n","\n","state = create_train_state(time_series_regressor, mts_main_key, batch, 0.0001)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8d3b4051bb9429e8716bac260925741","version_major":2,"version_minor":0},"text/plain":["epochs for runs/2024-05-17T07:00:16_wow_SimpleTS_train:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"985636a3777447f2908ca64b623313d5","version_major":2,"version_minor":0},"text/plain":["batches:   0%|          | 0/245 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Loss ITEM!!!! 10.729887008666992\n","Loss ITEM!!!! 10.38684368133545\n","Loss ITEM!!!! 10.335070610046387\n"]}],"source":["writer_name = \"SimpleTS\"\n","\n","writer_time = dt.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n","\n","train_summary_writer = SummaryWriter(\n","    \"runs/\" + writer_time + \"_wow_\" + writer_name + \"_train\"\n",")\n","\n","\n","test_set_key = random.PRNGKey(4454)\n","\n","train_data_loader = DataLoader(train_ds, batch_size=512, shuffle=True)\n","batch_count = 0\n","for j in trange(2, desc=f\"epochs for {train_summary_writer.log_dir}\"):\n","    # arrs = train_data_loader()\n","    for i in tqdm(train_data_loader, leave=False, desc=\"batches\"):\n","        # for i in trange(len(pre_train) // batch_size, leave=False):\n","        # for i in trange(len(pre_train) // batch_size //10, leave=False):\n","        # batch = make_batch(train_ds, i[0], 4)\n","\n","        state, loss = train_step(state, jnp.array(i))\n","        if jnp.isnan(loss):\n","            raise ValueError(\"Nan Value in loss, stopping\")\n","        batch_count += 1\n","\n","        if batch_count % 1 == 0:\n","            train_summary_writer.add_scalar(\n","                \"loss/loss\", np.array(loss.item()), batch_count\n","            )"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/plain":["Array(nan, dtype=float32)"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["loss"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"data":{"text/plain":["Array([[[[-2.03602821e-01, -4.18960065e-01,  6.27424300e-01, ...,\n","           6.86264992e-01,  1.20993710e+00,  1.41247106e+00],\n","         [-6.63501620e-01,  1.23872066e+00,  1.37265220e-01, ...,\n","          -7.40885794e-01,  1.60935748e+00, -1.24089427e-01],\n","         [-1.12007725e+00, -6.64127707e-01,  1.24177806e-01, ...,\n","           1.79821587e+00,  6.10290289e-01, -1.49627197e+00],\n","         [-1.44152296e+00,  9.04342234e-01, -2.00189734e+00, ...,\n","           1.95660397e-01,  9.29458857e-01,  6.07653081e-01],\n","         [-1.78585327e+00,  8.42333257e-01, -9.85428929e-01, ...,\n","          -4.24917191e-01, -1.54935861e+00, -1.33396161e+00],\n","         [ 9.56269979e-01,  1.75554633e+00, -1.74412513e+00, ...,\n","          -1.57703924e+00,  9.96053576e-01,  6.64331377e-01]],\n","\n","        [[-1.80250907e+00, -6.59279302e-02,  2.72393763e-01, ...,\n","           2.05398977e-01,  1.58877790e+00, -2.01322198e+00],\n","         [-5.26887298e-01,  2.09386006e-01,  3.31552535e-01, ...,\n","          -1.14491749e+00, -3.01765874e-02, -2.03591228e-01],\n","         [-3.08344698e+00,  1.87342063e-01,  1.31670618e+00, ...,\n","           7.95289755e-01,  4.09949571e-01,  1.36420393e+00],\n","         [-4.70832050e-01,  1.79586387e+00,  6.37132943e-01, ...,\n","           1.09207630e+00, -3.76822144e-01, -1.69693780e+00],\n","         [ 1.65441537e+00,  2.84960359e-01,  1.41118801e+00, ...,\n","          -1.67141783e+00, -2.11996540e-01, -1.07400000e+00],\n","         [ 1.94482815e+00,  1.13145858e-01, -4.98959869e-01, ...,\n","           4.35751714e-02, -7.76915019e-03,  7.95068592e-02]],\n","\n","        [[-3.28789115e-01, -7.06099570e-01,  3.20607567e+00, ...,\n","           9.67915177e-01,  1.11527348e+00,  9.54978883e-01],\n","         [ 8.69345665e-01,  2.86064386e+00,  7.68697202e-01, ...,\n","           1.41177940e+00, -2.13866577e-01, -1.44395196e+00],\n","         [-1.42808402e+00,  5.51577747e-01,  4.40169387e-02, ...,\n","          -4.40397263e-01, -5.45694947e-01, -9.03069973e-01],\n","         [-4.00344729e-01, -1.20738626e-01,  7.80880034e-01, ...,\n","           2.94842422e-01, -1.05313301e+00, -1.12124193e+00],\n","         [-1.30744600e+00,  3.98921758e-01, -1.98490351e-01, ...,\n","           2.11774921e+00,  1.23922002e+00, -4.49548304e-01],\n","         [-1.23768473e+00, -9.37024951e-02,  1.54088974e-01, ...,\n","          -3.46173167e-01, -6.78293943e-01,  1.33119845e+00]],\n","\n","        ...,\n","\n","        [[-2.33509040e+00,  1.16573381e+00,  1.42881215e+00, ...,\n","           1.56571105e-01,  1.39680445e+00, -4.71618712e-01],\n","         [-1.03593290e+00,  1.49305835e-01,  1.07123709e+00, ...,\n","           2.14007664e+00,  7.30548918e-01, -7.83977449e-01],\n","         [-3.54372889e-01,  2.50366151e-01, -9.37139571e-01, ...,\n","           6.94660068e-01,  2.38837862e+00,  1.51063740e-01],\n","         [ 1.06999528e+00,  6.24387562e-01,  3.28090936e-02, ...,\n","           8.79903138e-02, -2.32376957e+00, -2.18154478e+00],\n","         [ 8.28918159e-01, -1.65641880e+00, -1.76615965e+00, ...,\n","           1.76293707e+00, -1.96328536e-01,  8.31428409e-01],\n","         [ 1.46755576e+00,  7.37379849e-01, -1.09780288e+00, ...,\n","          -2.74595410e-01, -4.21065807e-01,  1.20226252e+00]],\n","\n","        [[-2.55856812e-01, -3.64808775e-02,  6.92968428e-01, ...,\n","           2.38744378e-01,  1.28138401e-02, -1.21428299e+00],\n","         [ 3.49642485e-01,  1.08567691e+00,  2.24249744e+00, ...,\n","           8.36118877e-01,  5.05303681e-01,  1.65706873e-01],\n","         [ 5.92407763e-01,  6.80234551e-01, -6.62963688e-01, ...,\n","           3.06440026e-01,  9.62429225e-01,  7.37940311e-01],\n","         [ 2.21339154e+00,  5.89318693e-01, -8.68302211e-02, ...,\n","           4.92073774e-01,  6.50533661e-02, -1.09852767e+00],\n","         [ 2.24006701e+00, -5.71644127e-01,  3.60312402e-01, ...,\n","           6.23485029e-01,  2.72012055e-01, -4.86276329e-01],\n","         [ 1.22343588e+00,  5.46037674e-01, -1.97691119e+00, ...,\n","          -7.54255354e-02,  5.35913289e-01,  7.00129628e-01]],\n","\n","        [[-1.51470041e+00, -6.32194042e-01,  7.91804969e-01, ...,\n","           1.23320019e+00,  4.57395345e-01,  3.89459789e-01],\n","         [-1.46765575e-01, -6.22581482e-01,  1.47542357e+00, ...,\n","          -1.04318643e+00,  6.23832047e-01,  1.87210083e-01],\n","         [-1.63267326e+00, -4.27642107e-01, -6.17007494e-01, ...,\n","           9.44134593e-01, -2.56806016e-01, -5.83787680e-01],\n","         [ 2.23164010e+00, -1.56498694e+00, -1.60341442e+00, ...,\n","          -7.39160359e-01, -9.06495333e-01, -4.89515960e-01],\n","         [ 8.20706069e-01,  7.46189773e-01, -6.49396896e-01, ...,\n","          -8.21164668e-01,  8.17339480e-01, -1.69931686e+00],\n","         [-4.33473974e-01,  8.00054729e-01,  3.02869344e+00, ...,\n","          -1.75256622e+00,  6.50486290e-01, -2.32097760e-01]]],\n","\n","\n","       [[[ 1.65823531e+00,  5.14275953e-02, -1.61061180e+00, ...,\n","          -5.56204736e-01, -6.35048389e-01, -2.95723110e-01],\n","         [-2.48324469e-01,  1.68801451e+00, -3.50208461e-01, ...,\n","           3.63650098e-02,  1.37758419e-01,  1.27849829e+00],\n","         [ 1.66596699e+00, -3.61318678e-01, -1.65393448e+00, ...,\n","          -2.43647599e+00,  1.76455110e-01, -1.05103290e+00],\n","         [ 4.18139249e-01, -1.05120735e-02,  2.43787214e-01, ...,\n","          -2.00408959e+00, -1.35889724e-01, -1.94900349e-01],\n","         [-1.12894797e+00, -6.09810293e-01,  1.41944206e+00, ...,\n","          -7.80748427e-01,  1.06321979e+00,  4.53954965e-01],\n","         [ 1.07261503e+00, -7.44399488e-01, -8.77844930e-01, ...,\n","           7.35714883e-02, -7.70161390e-01,  1.74276125e+00]],\n","\n","        [[ 6.31708980e-01, -1.07662880e+00, -1.29173696e-01, ...,\n","          -5.89893982e-02, -2.36541843e+00,  1.58299088e+00],\n","         [-2.36735836e-01,  5.28354943e-01,  1.15368974e+00, ...,\n","           2.88536083e-02, -7.46286750e-01, -5.47408722e-02],\n","         [-9.45182025e-01,  2.84240961e-01,  3.63033235e-01, ...,\n","          -1.18516982e+00, -1.21403635e+00,  6.54340923e-01],\n","         [ 6.28894091e-01,  8.55858564e-01, -1.28654867e-01, ...,\n","           3.08618903e-01, -1.88800573e-01,  9.03971732e-01],\n","         [ 9.10262764e-01,  8.24077785e-01,  4.73703504e-01, ...,\n","           3.06431293e+00,  5.12261212e-01,  6.41139805e-01],\n","         [ 1.91708967e-01,  9.81379330e-01,  1.21546447e+00, ...,\n","           4.59459513e-01,  1.38852328e-01, -1.10508859e+00]],\n","\n","        [[-1.26831365e+00, -2.10523391e+00, -1.57273084e-01, ...,\n","           2.31434867e-01, -1.34910691e+00,  1.08739650e+00],\n","         [-8.67990196e-01, -3.57070416e-02, -6.89813554e-01, ...,\n","           1.47649169e+00,  1.24109304e+00,  5.22479355e-01],\n","         [-2.70339918e+00, -9.14617121e-01,  2.01696053e-01, ...,\n","           1.09268355e+00,  7.59959593e-02, -4.88157213e-01],\n","         [-2.02031970e-01,  4.04596955e-01, -1.33681223e-01, ...,\n","          -9.70932484e-01,  7.39232838e-01, -4.05842155e-01],\n","         [ 8.72621894e-01,  9.58321095e-01, -1.26478100e+00, ...,\n","           4.29589927e-01, -2.20343161e+00,  1.34743020e-01],\n","         [ 6.70327961e-01,  1.57545865e+00,  1.88166246e-01, ...,\n","          -2.52439678e-02,  2.95621455e-01,  6.16552234e-01]],\n","\n","        ...,\n","\n","        [[ 4.01256382e-02,  1.86230886e+00, -8.06734920e-01, ...,\n","          -7.47947544e-02,  3.58953446e-01, -8.23342562e-01],\n","         [ 1.27382195e+00, -2.71495044e-01, -1.42513955e+00, ...,\n","           5.35101056e-01,  1.15766525e-01,  6.94990158e-01],\n","         [ 3.33034813e-01, -3.13170463e-01, -1.31345415e+00, ...,\n","          -5.26497006e-01,  9.85826969e-01, -6.01899505e-01],\n","         [-1.17942107e+00,  6.62893474e-01,  7.59736717e-01, ...,\n","           8.86133671e-01, -4.21682000e-01,  8.70425403e-01],\n","         [-5.53126037e-01,  1.31775486e+00, -4.63191777e-01, ...,\n","          -1.24552894e+00,  2.30322957e+00,  7.88788795e-02],\n","         [ 1.87991068e-01,  1.77876806e+00,  9.99737382e-01, ...,\n","           1.62211835e+00, -7.53831468e-04,  7.23120868e-01]],\n","\n","        [[ 4.44248259e-01,  1.27718127e+00, -2.56039262e-01, ...,\n","          -4.75812107e-01,  4.29698735e-01,  4.36851621e-01],\n","         [ 1.69019079e+00,  6.21626914e-01, -2.51102984e-01, ...,\n","           2.62943208e-01,  1.80185652e+00,  3.37862104e-01],\n","         [ 8.15576077e-01,  7.52036348e-02, -6.67413473e-01, ...,\n","           2.65262008e+00,  1.67412683e-01,  6.40337050e-01],\n","         [ 2.58365780e-01,  1.19807076e+00,  8.09845448e-01, ...,\n","          -3.45953017e-01, -2.14739814e-01,  5.05382538e-01],\n","         [-1.23237956e+00, -1.85759276e-01, -1.27010643e+00, ...,\n","           3.60794552e-02,  7.03996062e-01,  9.63229597e-01],\n","         [ 8.78436089e-01, -6.43280685e-01,  1.15907502e+00, ...,\n","          -4.62454140e-01, -1.14100480e+00,  1.34940362e+00]],\n","\n","        [[-1.67682123e+00, -1.10073256e+00, -1.41343868e+00, ...,\n","          -9.56789672e-01, -1.27004135e+00, -6.03731096e-01],\n","         [-4.45506960e-01, -5.10565400e-01, -5.88533103e-01, ...,\n","           1.52295202e-01,  3.37515682e-01,  2.79987216e-01],\n","         [ 5.44326127e-01,  1.50278616e+00,  8.74152958e-01, ...,\n","          -1.29222763e+00, -1.29386440e-01,  7.28651166e-01],\n","         [ 7.10622847e-01, -3.25202793e-01,  1.39731556e-01, ...,\n","           8.11905265e-01,  2.62160987e-01, -5.06488085e-01],\n","         [ 3.22662592e-01, -1.36186218e+00,  1.10769379e+00, ...,\n","           1.88964689e+00,  2.93100178e-01,  4.43977803e-01],\n","         [-5.81427038e-01,  7.67650068e-01,  2.85747468e-01, ...,\n","          -2.62118673e+00,  4.42757010e-01, -1.89026284e+00]]]],      dtype=float32)"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["from jax import random\n","\n","test_key = jax.random.key(12)\n","att = random.normal(test_key, (2, 10, 6, 16))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["Array([[1.4373543, 1.4373543, 1.4373913, ..., 1.4373543, 1.4373543,\n","        1.4373543],\n","       [1.4121699, 1.4121698, 1.4121699, ..., 1.4121699, 1.4121699,\n","        1.4121699],\n","       [1.3919055, 1.3919055, 1.3919055, ..., 1.3919055, 1.3919055,\n","        1.3919055],\n","       ...,\n","       [2.2000782, 2.2000782, 2.2000782, ..., 2.2000782, 2.2000782,\n","        2.2000782],\n","       [2.1688452, 2.1688452, 2.1688452, ..., 2.1688452, 2.1688452,\n","        2.1688452],\n","       [2.1502194, 2.1502194, 2.1502194, ..., 2.1502194, 2.1502194,\n","        2.1502194]], dtype=float32)"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["test_result = jnp.squeeze(\n","    state.apply_fn({\"params\": state.params}, jnp.array([train_ds[0]]))\n",")\n","test_result"]},{"cell_type":"code","execution_count":null,"metadata":{"notebookRunGroups":{"groupValue":""}},"outputs":[{"data":{"text/plain":["((59, 26), (1, 59, 26))"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["jnp.array(train_ds[0]).shape, test_result.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["(59, 26)"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["jnp.squeeze(test_result).shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_pred = pd.DataFrame(test_result)\n","df_actual = pd.DataFrame(train_ds[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.437354</td>\n","      <td>1.437354</td>\n","      <td>1.437391</td>\n","      <td>1.437354</td>\n","      <td>1.437354</td>\n","      <td>1.437354</td>\n","      <td>1.437354</td>\n","      <td>1.437391</td>\n","      <td>1.437354</td>\n","      <td>1.437354</td>\n","      <td>...</td>\n","      <td>1.437354</td>\n","      <td>1.437354</td>\n","      <td>1.437354</td>\n","      <td>1.437354</td>\n","      <td>1.437354</td>\n","      <td>1.437354</td>\n","      <td>1.437354</td>\n","      <td>1.437354</td>\n","      <td>1.437354</td>\n","      <td>1.437354</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","      <td>...</td>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","      <td>1.412170</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","      <td>...</td>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","      <td>1.391906</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.366370</td>\n","      <td>1.366354</td>\n","      <td>1.366348</td>\n","      <td>1.366347</td>\n","      <td>1.366355</td>\n","      <td>1.366348</td>\n","      <td>1.366363</td>\n","      <td>1.366355</td>\n","      <td>1.366354</td>\n","      <td>1.366354</td>\n","      <td>...</td>\n","      <td>1.366355</td>\n","      <td>1.366355</td>\n","      <td>1.366355</td>\n","      <td>1.366355</td>\n","      <td>1.366355</td>\n","      <td>1.366355</td>\n","      <td>1.366355</td>\n","      <td>1.366355</td>\n","      <td>1.366355</td>\n","      <td>1.366355</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.323469</td>\n","      <td>1.321951</td>\n","      <td>1.321842</td>\n","      <td>1.321652</td>\n","      <td>1.321838</td>\n","      <td>1.321779</td>\n","      <td>1.322127</td>\n","      <td>1.321920</td>\n","      <td>1.321971</td>\n","      <td>1.321971</td>\n","      <td>...</td>\n","      <td>1.321885</td>\n","      <td>1.321885</td>\n","      <td>1.321885</td>\n","      <td>1.321885</td>\n","      <td>1.321885</td>\n","      <td>1.321885</td>\n","      <td>1.321885</td>\n","      <td>1.321885</td>\n","      <td>1.321885</td>\n","      <td>1.321885</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1.271234</td>\n","      <td>1.243167</td>\n","      <td>1.242631</td>\n","      <td>1.240222</td>\n","      <td>1.241583</td>\n","      <td>1.240969</td>\n","      <td>1.244538</td>\n","      <td>1.243003</td>\n","      <td>1.243547</td>\n","      <td>1.243487</td>\n","      <td>...</td>\n","      <td>1.242753</td>\n","      <td>1.242753</td>\n","      <td>1.242753</td>\n","      <td>1.242753</td>\n","      <td>1.242753</td>\n","      <td>1.242753</td>\n","      <td>1.242753</td>\n","      <td>1.242753</td>\n","      <td>1.242753</td>\n","      <td>1.242753</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2.286670</td>\n","      <td>1.145433</td>\n","      <td>1.144320</td>\n","      <td>1.115955</td>\n","      <td>1.124038</td>\n","      <td>1.120317</td>\n","      <td>1.162274</td>\n","      <td>1.145752</td>\n","      <td>1.152200</td>\n","      <td>1.151740</td>\n","      <td>...</td>\n","      <td>1.142506</td>\n","      <td>1.142506</td>\n","      <td>1.142506</td>\n","      <td>1.142506</td>\n","      <td>1.142506</td>\n","      <td>1.142506</td>\n","      <td>1.142506</td>\n","      <td>1.142506</td>\n","      <td>1.142506</td>\n","      <td>1.142506</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>4.521352</td>\n","      <td>1.146990</td>\n","      <td>1.187843</td>\n","      <td>0.945027</td>\n","      <td>0.935768</td>\n","      <td>0.912765</td>\n","      <td>1.324656</td>\n","      <td>1.174479</td>\n","      <td>1.234436</td>\n","      <td>1.230192</td>\n","      <td>...</td>\n","      <td>1.145297</td>\n","      <td>1.145297</td>\n","      <td>1.145297</td>\n","      <td>1.145297</td>\n","      <td>1.145297</td>\n","      <td>1.145297</td>\n","      <td>1.145297</td>\n","      <td>1.145297</td>\n","      <td>1.145297</td>\n","      <td>1.145297</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>3.774994</td>\n","      <td>1.007358</td>\n","      <td>1.450285</td>\n","      <td>0.487091</td>\n","      <td>0.141636</td>\n","      <td>0.032512</td>\n","      <td>1.942989</td>\n","      <td>1.287989</td>\n","      <td>1.576423</td>\n","      <td>1.552956</td>\n","      <td>...</td>\n","      <td>1.142628</td>\n","      <td>1.142628</td>\n","      <td>1.142628</td>\n","      <td>1.142628</td>\n","      <td>1.142628</td>\n","      <td>1.142628</td>\n","      <td>1.142628</td>\n","      <td>1.142628</td>\n","      <td>1.142628</td>\n","      <td>1.142628</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>4.412226</td>\n","      <td>0.588430</td>\n","      <td>1.866369</td>\n","      <td>0.349090</td>\n","      <td>-0.716906</td>\n","      <td>-1.021551</td>\n","      <td>2.443130</td>\n","      <td>1.454736</td>\n","      <td>1.956791</td>\n","      <td>1.914964</td>\n","      <td>...</td>\n","      <td>1.168766</td>\n","      <td>1.168766</td>\n","      <td>1.168766</td>\n","      <td>1.168766</td>\n","      <td>1.168766</td>\n","      <td>1.168766</td>\n","      <td>1.168766</td>\n","      <td>1.168766</td>\n","      <td>1.168766</td>\n","      <td>1.168766</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>5.019538</td>\n","      <td>0.227126</td>\n","      <td>1.888445</td>\n","      <td>0.791654</td>\n","      <td>-0.740773</td>\n","      <td>-1.389133</td>\n","      <td>2.334280</td>\n","      <td>1.421951</td>\n","      <td>1.906135</td>\n","      <td>1.866119</td>\n","      <td>...</td>\n","      <td>1.155034</td>\n","      <td>1.155034</td>\n","      <td>1.155034</td>\n","      <td>1.155034</td>\n","      <td>1.155034</td>\n","      <td>1.155034</td>\n","      <td>1.155034</td>\n","      <td>1.155034</td>\n","      <td>1.155034</td>\n","      <td>1.155034</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>5.358994</td>\n","      <td>-0.157142</td>\n","      <td>1.874660</td>\n","      <td>1.107809</td>\n","      <td>-0.432813</td>\n","      <td>-1.539848</td>\n","      <td>2.180945</td>\n","      <td>1.386842</td>\n","      <td>1.854647</td>\n","      <td>1.822358</td>\n","      <td>...</td>\n","      <td>1.179546</td>\n","      <td>1.179546</td>\n","      <td>1.179546</td>\n","      <td>1.179546</td>\n","      <td>1.179546</td>\n","      <td>1.179546</td>\n","      <td>1.179546</td>\n","      <td>1.179546</td>\n","      <td>1.179546</td>\n","      <td>1.179546</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>5.563088</td>\n","      <td>-0.667493</td>\n","      <td>1.897134</td>\n","      <td>1.186427</td>\n","      <td>0.060345</td>\n","      <td>-1.626834</td>\n","      <td>2.064752</td>\n","      <td>1.384620</td>\n","      <td>1.892544</td>\n","      <td>1.866678</td>\n","      <td>...</td>\n","      <td>1.185746</td>\n","      <td>1.185746</td>\n","      <td>1.185746</td>\n","      <td>1.185746</td>\n","      <td>1.185746</td>\n","      <td>1.185746</td>\n","      <td>1.185746</td>\n","      <td>1.185746</td>\n","      <td>1.185746</td>\n","      <td>1.185746</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>5.883640</td>\n","      <td>-1.058375</td>\n","      <td>1.881047</td>\n","      <td>1.000066</td>\n","      <td>0.534132</td>\n","      <td>-1.786740</td>\n","      <td>1.954329</td>\n","      <td>1.371077</td>\n","      <td>1.956908</td>\n","      <td>1.924489</td>\n","      <td>...</td>\n","      <td>1.149803</td>\n","      <td>1.149803</td>\n","      <td>1.149803</td>\n","      <td>1.149803</td>\n","      <td>1.149803</td>\n","      <td>1.149803</td>\n","      <td>1.149803</td>\n","      <td>1.149803</td>\n","      <td>1.149803</td>\n","      <td>1.149803</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>6.303962</td>\n","      <td>-1.234468</td>\n","      <td>1.797595</td>\n","      <td>0.633750</td>\n","      <td>0.895178</td>\n","      <td>-2.012424</td>\n","      <td>1.785623</td>\n","      <td>1.410010</td>\n","      <td>2.060913</td>\n","      <td>2.010454</td>\n","      <td>...</td>\n","      <td>1.146170</td>\n","      <td>1.146170</td>\n","      <td>1.146170</td>\n","      <td>1.146170</td>\n","      <td>1.146170</td>\n","      <td>1.146170</td>\n","      <td>1.146170</td>\n","      <td>1.146170</td>\n","      <td>1.146170</td>\n","      <td>1.146170</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>6.837324</td>\n","      <td>-1.339337</td>\n","      <td>1.672579</td>\n","      <td>0.024722</td>\n","      <td>1.110208</td>\n","      <td>-2.174672</td>\n","      <td>1.599338</td>\n","      <td>1.481306</td>\n","      <td>2.196213</td>\n","      <td>2.123794</td>\n","      <td>...</td>\n","      <td>1.168332</td>\n","      <td>1.168332</td>\n","      <td>1.168332</td>\n","      <td>1.168332</td>\n","      <td>1.168332</td>\n","      <td>1.168332</td>\n","      <td>1.168332</td>\n","      <td>1.168332</td>\n","      <td>1.168332</td>\n","      <td>1.168332</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>7.427869</td>\n","      <td>-1.491255</td>\n","      <td>1.433858</td>\n","      <td>-0.485388</td>\n","      <td>1.030484</td>\n","      <td>-2.273295</td>\n","      <td>1.329829</td>\n","      <td>1.459788</td>\n","      <td>2.227226</td>\n","      <td>2.148189</td>\n","      <td>...</td>\n","      <td>1.135512</td>\n","      <td>1.135512</td>\n","      <td>1.135512</td>\n","      <td>1.135512</td>\n","      <td>1.135512</td>\n","      <td>1.135512</td>\n","      <td>1.135512</td>\n","      <td>1.135512</td>\n","      <td>1.135512</td>\n","      <td>1.135512</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>7.982532</td>\n","      <td>-1.674590</td>\n","      <td>1.101997</td>\n","      <td>-0.758202</td>\n","      <td>0.738396</td>\n","      <td>-2.346509</td>\n","      <td>0.997491</td>\n","      <td>1.328974</td>\n","      <td>2.108857</td>\n","      <td>2.046296</td>\n","      <td>...</td>\n","      <td>1.096514</td>\n","      <td>1.096514</td>\n","      <td>1.096514</td>\n","      <td>1.096514</td>\n","      <td>1.096514</td>\n","      <td>1.096514</td>\n","      <td>1.096514</td>\n","      <td>1.096514</td>\n","      <td>1.096514</td>\n","      <td>1.096514</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>8.481820</td>\n","      <td>-1.786896</td>\n","      <td>0.806491</td>\n","      <td>-0.997023</td>\n","      <td>0.222622</td>\n","      <td>-2.342515</td>\n","      <td>0.705007</td>\n","      <td>1.272448</td>\n","      <td>1.977945</td>\n","      <td>1.933778</td>\n","      <td>...</td>\n","      <td>1.072738</td>\n","      <td>1.072738</td>\n","      <td>1.072738</td>\n","      <td>1.072738</td>\n","      <td>1.072738</td>\n","      <td>1.072738</td>\n","      <td>1.072738</td>\n","      <td>1.072738</td>\n","      <td>1.072738</td>\n","      <td>1.072738</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>8.934710</td>\n","      <td>-1.828657</td>\n","      <td>0.440821</td>\n","      <td>-0.973854</td>\n","      <td>-0.217193</td>\n","      <td>-2.368635</td>\n","      <td>0.369906</td>\n","      <td>1.267811</td>\n","      <td>1.945301</td>\n","      <td>1.902387</td>\n","      <td>...</td>\n","      <td>1.106788</td>\n","      <td>1.106788</td>\n","      <td>1.106788</td>\n","      <td>1.106788</td>\n","      <td>1.106788</td>\n","      <td>1.106788</td>\n","      <td>1.106788</td>\n","      <td>1.106788</td>\n","      <td>1.106788</td>\n","      <td>1.106788</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>9.377786</td>\n","      <td>-1.917150</td>\n","      <td>0.149085</td>\n","      <td>-0.789178</td>\n","      <td>-0.602083</td>\n","      <td>-2.473271</td>\n","      <td>0.105496</td>\n","      <td>1.329790</td>\n","      <td>2.001824</td>\n","      <td>1.946873</td>\n","      <td>...</td>\n","      <td>1.134605</td>\n","      <td>1.134605</td>\n","      <td>1.134605</td>\n","      <td>1.134605</td>\n","      <td>1.134605</td>\n","      <td>1.134605</td>\n","      <td>1.134605</td>\n","      <td>1.134605</td>\n","      <td>1.134605</td>\n","      <td>1.134605</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>9.834937</td>\n","      <td>-2.024716</td>\n","      <td>-0.164430</td>\n","      <td>-0.514289</td>\n","      <td>-0.915306</td>\n","      <td>-2.598916</td>\n","      <td>-0.173065</td>\n","      <td>1.454163</td>\n","      <td>2.049280</td>\n","      <td>1.982165</td>\n","      <td>...</td>\n","      <td>1.147231</td>\n","      <td>1.147231</td>\n","      <td>1.147231</td>\n","      <td>1.147231</td>\n","      <td>1.147231</td>\n","      <td>1.147231</td>\n","      <td>1.147231</td>\n","      <td>1.147231</td>\n","      <td>1.147231</td>\n","      <td>1.147231</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>10.318847</td>\n","      <td>-2.092089</td>\n","      <td>-0.485429</td>\n","      <td>-0.121387</td>\n","      <td>-1.087726</td>\n","      <td>-2.689350</td>\n","      <td>-0.458171</td>\n","      <td>1.453902</td>\n","      <td>2.018181</td>\n","      <td>1.951495</td>\n","      <td>...</td>\n","      <td>1.133224</td>\n","      <td>1.133224</td>\n","      <td>1.133224</td>\n","      <td>1.133224</td>\n","      <td>1.133224</td>\n","      <td>1.133224</td>\n","      <td>1.133224</td>\n","      <td>1.133224</td>\n","      <td>1.133224</td>\n","      <td>1.133224</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>10.813057</td>\n","      <td>-2.093420</td>\n","      <td>-0.791364</td>\n","      <td>0.356859</td>\n","      <td>-1.016962</td>\n","      <td>-2.706887</td>\n","      <td>-0.730867</td>\n","      <td>1.360321</td>\n","      <td>1.981212</td>\n","      <td>1.920103</td>\n","      <td>...</td>\n","      <td>1.144190</td>\n","      <td>1.144190</td>\n","      <td>1.144190</td>\n","      <td>1.144190</td>\n","      <td>1.144190</td>\n","      <td>1.144190</td>\n","      <td>1.144190</td>\n","      <td>1.144190</td>\n","      <td>1.144190</td>\n","      <td>1.144190</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>11.298434</td>\n","      <td>-2.022815</td>\n","      <td>-1.020691</td>\n","      <td>0.780895</td>\n","      <td>-0.666634</td>\n","      <td>-2.682947</td>\n","      <td>-0.932477</td>\n","      <td>1.300767</td>\n","      <td>2.020786</td>\n","      <td>1.981955</td>\n","      <td>...</td>\n","      <td>1.082682</td>\n","      <td>1.082682</td>\n","      <td>1.082682</td>\n","      <td>1.082682</td>\n","      <td>1.082682</td>\n","      <td>1.082682</td>\n","      <td>1.082682</td>\n","      <td>1.082682</td>\n","      <td>1.082682</td>\n","      <td>1.082682</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>11.766288</td>\n","      <td>-1.889057</td>\n","      <td>-1.299571</td>\n","      <td>1.017477</td>\n","      <td>-0.159251</td>\n","      <td>-2.673756</td>\n","      <td>-1.185550</td>\n","      <td>1.234440</td>\n","      <td>2.158800</td>\n","      <td>2.124010</td>\n","      <td>...</td>\n","      <td>1.073079</td>\n","      <td>1.073079</td>\n","      <td>1.073079</td>\n","      <td>1.073079</td>\n","      <td>1.073079</td>\n","      <td>1.073079</td>\n","      <td>1.073079</td>\n","      <td>1.073079</td>\n","      <td>1.073079</td>\n","      <td>1.073079</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>12.203382</td>\n","      <td>-1.689771</td>\n","      <td>-1.584885</td>\n","      <td>0.984972</td>\n","      <td>0.317080</td>\n","      <td>-2.680233</td>\n","      <td>-1.481935</td>\n","      <td>1.242981</td>\n","      <td>2.159756</td>\n","      <td>2.100759</td>\n","      <td>...</td>\n","      <td>1.078025</td>\n","      <td>1.078025</td>\n","      <td>1.078025</td>\n","      <td>1.078025</td>\n","      <td>1.078025</td>\n","      <td>1.078025</td>\n","      <td>1.078025</td>\n","      <td>1.078025</td>\n","      <td>1.078025</td>\n","      <td>1.078025</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>12.628815</td>\n","      <td>-1.409874</td>\n","      <td>-1.850814</td>\n","      <td>0.727735</td>\n","      <td>0.684783</td>\n","      <td>-2.650165</td>\n","      <td>-1.788742</td>\n","      <td>1.300387</td>\n","      <td>2.088112</td>\n","      <td>2.068450</td>\n","      <td>...</td>\n","      <td>1.103690</td>\n","      <td>1.103690</td>\n","      <td>1.103690</td>\n","      <td>1.103690</td>\n","      <td>1.103690</td>\n","      <td>1.103690</td>\n","      <td>1.103690</td>\n","      <td>1.103690</td>\n","      <td>1.103690</td>\n","      <td>1.103690</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>13.072464</td>\n","      <td>-1.121183</td>\n","      <td>-2.018804</td>\n","      <td>0.276828</td>\n","      <td>1.009536</td>\n","      <td>-2.555903</td>\n","      <td>-2.028759</td>\n","      <td>1.346703</td>\n","      <td>2.080505</td>\n","      <td>2.058820</td>\n","      <td>...</td>\n","      <td>1.137544</td>\n","      <td>1.137544</td>\n","      <td>1.137544</td>\n","      <td>1.137544</td>\n","      <td>1.137544</td>\n","      <td>1.137544</td>\n","      <td>1.137544</td>\n","      <td>1.137544</td>\n","      <td>1.137544</td>\n","      <td>1.137544</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>13.549956</td>\n","      <td>-0.829171</td>\n","      <td>-2.117776</td>\n","      <td>-0.274666</td>\n","      <td>1.093311</td>\n","      <td>-2.372162</td>\n","      <td>-2.205609</td>\n","      <td>1.307930</td>\n","      <td>2.142208</td>\n","      <td>2.065599</td>\n","      <td>...</td>\n","      <td>1.173489</td>\n","      <td>1.173489</td>\n","      <td>1.173489</td>\n","      <td>1.173489</td>\n","      <td>1.173489</td>\n","      <td>1.173489</td>\n","      <td>1.173489</td>\n","      <td>1.173489</td>\n","      <td>1.173489</td>\n","      <td>1.173489</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>14.052283</td>\n","      <td>-0.465339</td>\n","      <td>-2.066869</td>\n","      <td>-0.695588</td>\n","      <td>0.863206</td>\n","      <td>-2.096478</td>\n","      <td>-2.213705</td>\n","      <td>1.289903</td>\n","      <td>2.117284</td>\n","      <td>2.067234</td>\n","      <td>...</td>\n","      <td>1.189505</td>\n","      <td>1.189505</td>\n","      <td>1.189505</td>\n","      <td>1.189505</td>\n","      <td>1.189505</td>\n","      <td>1.189505</td>\n","      <td>1.189505</td>\n","      <td>1.189505</td>\n","      <td>1.189505</td>\n","      <td>1.189505</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>14.533471</td>\n","      <td>-0.021433</td>\n","      <td>-1.886216</td>\n","      <td>-0.966109</td>\n","      <td>0.517291</td>\n","      <td>-1.748452</td>\n","      <td>-2.060690</td>\n","      <td>1.249960</td>\n","      <td>2.014639</td>\n","      <td>1.979736</td>\n","      <td>...</td>\n","      <td>1.148023</td>\n","      <td>1.148023</td>\n","      <td>1.148023</td>\n","      <td>1.148023</td>\n","      <td>1.148023</td>\n","      <td>1.148023</td>\n","      <td>1.148023</td>\n","      <td>1.148023</td>\n","      <td>1.148023</td>\n","      <td>1.148023</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>15.001737</td>\n","      <td>0.396265</td>\n","      <td>-1.717975</td>\n","      <td>-0.936786</td>\n","      <td>0.120868</td>\n","      <td>-1.439233</td>\n","      <td>-1.941935</td>\n","      <td>1.255687</td>\n","      <td>1.999335</td>\n","      <td>1.975363</td>\n","      <td>...</td>\n","      <td>1.154166</td>\n","      <td>1.154166</td>\n","      <td>1.154166</td>\n","      <td>1.154166</td>\n","      <td>1.154166</td>\n","      <td>1.154166</td>\n","      <td>1.154166</td>\n","      <td>1.154166</td>\n","      <td>1.154166</td>\n","      <td>1.154166</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>15.464746</td>\n","      <td>0.766099</td>\n","      <td>-1.600744</td>\n","      <td>-0.746933</td>\n","      <td>-0.290252</td>\n","      <td>-1.148647</td>\n","      <td>-1.948827</td>\n","      <td>1.284989</td>\n","      <td>1.996198</td>\n","      <td>1.963521</td>\n","      <td>...</td>\n","      <td>1.187071</td>\n","      <td>1.187071</td>\n","      <td>1.187071</td>\n","      <td>1.187071</td>\n","      <td>1.187071</td>\n","      <td>1.187071</td>\n","      <td>1.187071</td>\n","      <td>1.187071</td>\n","      <td>1.187071</td>\n","      <td>1.187071</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>15.937218</td>\n","      <td>1.056247</td>\n","      <td>-1.457762</td>\n","      <td>-0.471972</td>\n","      <td>-0.578229</td>\n","      <td>-0.829984</td>\n","      <td>-1.970247</td>\n","      <td>1.312191</td>\n","      <td>2.014191</td>\n","      <td>1.954652</td>\n","      <td>...</td>\n","      <td>1.227566</td>\n","      <td>1.227566</td>\n","      <td>1.227566</td>\n","      <td>1.227566</td>\n","      <td>1.227566</td>\n","      <td>1.227566</td>\n","      <td>1.227566</td>\n","      <td>1.227566</td>\n","      <td>1.227566</td>\n","      <td>1.227566</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>16.436983</td>\n","      <td>1.365839</td>\n","      <td>-1.266414</td>\n","      <td>-0.195082</td>\n","      <td>-0.826373</td>\n","      <td>-0.581953</td>\n","      <td>-1.915703</td>\n","      <td>1.233081</td>\n","      <td>1.981007</td>\n","      <td>1.922105</td>\n","      <td>...</td>\n","      <td>1.167316</td>\n","      <td>1.167316</td>\n","      <td>1.167316</td>\n","      <td>1.167316</td>\n","      <td>1.167316</td>\n","      <td>1.167316</td>\n","      <td>1.167316</td>\n","      <td>1.167316</td>\n","      <td>1.167316</td>\n","      <td>1.167316</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>16.958950</td>\n","      <td>1.585731</td>\n","      <td>-1.004783</td>\n","      <td>0.178392</td>\n","      <td>-0.922430</td>\n","      <td>-0.374385</td>\n","      <td>-1.768340</td>\n","      <td>1.126836</td>\n","      <td>2.016287</td>\n","      <td>1.975924</td>\n","      <td>...</td>\n","      <td>1.054133</td>\n","      <td>1.054133</td>\n","      <td>1.054133</td>\n","      <td>1.054133</td>\n","      <td>1.054133</td>\n","      <td>1.054133</td>\n","      <td>1.054133</td>\n","      <td>1.054133</td>\n","      <td>1.054133</td>\n","      <td>1.054133</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>17.477139</td>\n","      <td>1.780047</td>\n","      <td>-0.640145</td>\n","      <td>0.569267</td>\n","      <td>-0.708304</td>\n","      <td>-0.151685</td>\n","      <td>-1.591688</td>\n","      <td>1.106603</td>\n","      <td>2.054725</td>\n","      <td>2.058574</td>\n","      <td>...</td>\n","      <td>1.016432</td>\n","      <td>1.016432</td>\n","      <td>1.016432</td>\n","      <td>1.016432</td>\n","      <td>1.016432</td>\n","      <td>1.016432</td>\n","      <td>1.016432</td>\n","      <td>1.016432</td>\n","      <td>1.016432</td>\n","      <td>1.016432</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>17.973789</td>\n","      <td>1.901352</td>\n","      <td>-0.230850</td>\n","      <td>0.813695</td>\n","      <td>-0.259706</td>\n","      <td>0.110963</td>\n","      <td>-1.539996</td>\n","      <td>1.129792</td>\n","      <td>2.031977</td>\n","      <td>2.038699</td>\n","      <td>...</td>\n","      <td>1.037239</td>\n","      <td>1.037239</td>\n","      <td>1.037239</td>\n","      <td>1.037239</td>\n","      <td>1.037239</td>\n","      <td>1.037239</td>\n","      <td>1.037239</td>\n","      <td>1.037239</td>\n","      <td>1.037239</td>\n","      <td>1.037239</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>18.446491</td>\n","      <td>1.905706</td>\n","      <td>0.052176</td>\n","      <td>0.836801</td>\n","      <td>0.100380</td>\n","      <td>0.347887</td>\n","      <td>-1.663893</td>\n","      <td>1.096858</td>\n","      <td>1.981099</td>\n","      <td>1.983657</td>\n","      <td>...</td>\n","      <td>1.012342</td>\n","      <td>1.012342</td>\n","      <td>1.012342</td>\n","      <td>1.012342</td>\n","      <td>1.012342</td>\n","      <td>1.012342</td>\n","      <td>1.012342</td>\n","      <td>1.012342</td>\n","      <td>1.012342</td>\n","      <td>1.012342</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>18.911346</td>\n","      <td>1.811662</td>\n","      <td>0.330609</td>\n","      <td>0.686024</td>\n","      <td>0.426084</td>\n","      <td>0.473132</td>\n","      <td>-1.817253</td>\n","      <td>1.065089</td>\n","      <td>1.917113</td>\n","      <td>1.906045</td>\n","      <td>...</td>\n","      <td>0.997039</td>\n","      <td>0.997039</td>\n","      <td>0.997039</td>\n","      <td>0.997039</td>\n","      <td>0.997039</td>\n","      <td>0.997039</td>\n","      <td>0.997039</td>\n","      <td>0.997039</td>\n","      <td>0.997039</td>\n","      <td>0.997039</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>19.393915</td>\n","      <td>1.724316</td>\n","      <td>0.514841</td>\n","      <td>0.401804</td>\n","      <td>0.650435</td>\n","      <td>0.680229</td>\n","      <td>-1.854092</td>\n","      <td>1.041919</td>\n","      <td>1.971945</td>\n","      <td>1.919307</td>\n","      <td>...</td>\n","      <td>0.995124</td>\n","      <td>0.995124</td>\n","      <td>0.995124</td>\n","      <td>0.995124</td>\n","      <td>0.995124</td>\n","      <td>0.995124</td>\n","      <td>0.995124</td>\n","      <td>0.995124</td>\n","      <td>0.995124</td>\n","      <td>0.995124</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>19.892784</td>\n","      <td>1.553593</td>\n","      <td>0.841788</td>\n","      <td>0.052532</td>\n","      <td>0.817920</td>\n","      <td>1.014394</td>\n","      <td>-1.757624</td>\n","      <td>0.990485</td>\n","      <td>2.076530</td>\n","      <td>2.052320</td>\n","      <td>...</td>\n","      <td>0.959452</td>\n","      <td>0.959452</td>\n","      <td>0.959452</td>\n","      <td>0.959452</td>\n","      <td>0.959452</td>\n","      <td>0.959452</td>\n","      <td>0.959452</td>\n","      <td>0.959452</td>\n","      <td>0.959452</td>\n","      <td>0.959452</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>20.391842</td>\n","      <td>1.272578</td>\n","      <td>1.301042</td>\n","      <td>-0.346223</td>\n","      <td>0.820290</td>\n","      <td>1.418135</td>\n","      <td>-1.625464</td>\n","      <td>1.017228</td>\n","      <td>2.242980</td>\n","      <td>2.237311</td>\n","      <td>...</td>\n","      <td>0.939862</td>\n","      <td>0.939862</td>\n","      <td>0.939862</td>\n","      <td>0.939862</td>\n","      <td>0.939862</td>\n","      <td>0.939862</td>\n","      <td>0.939862</td>\n","      <td>0.939862</td>\n","      <td>0.939862</td>\n","      <td>0.939862</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","      <td>...</td>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","      <td>2.199863</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","      <td>...</td>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","      <td>2.242986</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","      <td>...</td>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","      <td>2.293994</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","      <td>...</td>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","      <td>2.322799</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","      <td>...</td>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","      <td>2.325436</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","      <td>...</td>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","      <td>2.310490</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","      <td>...</td>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","      <td>2.294999</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","      <td>...</td>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","      <td>2.271732</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","      <td>...</td>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","      <td>2.247481</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","      <td>...</td>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","      <td>2.237985</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","      <td>...</td>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","      <td>2.228682</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","      <td>...</td>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","      <td>2.218944</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","      <td>...</td>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","      <td>2.200078</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","      <td>...</td>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","      <td>2.168845</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","      <td>...</td>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","      <td>2.150219</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>59 rows × 26 columns</p>\n","</div>"],"text/plain":["           0         1         2         3         4         5         6   \\\n","0    1.437354  1.437354  1.437391  1.437354  1.437354  1.437354  1.437354   \n","1    1.412170  1.412170  1.412170  1.412170  1.412170  1.412170  1.412170   \n","2    1.391906  1.391906  1.391906  1.391906  1.391906  1.391906  1.391906   \n","3    1.366370  1.366354  1.366348  1.366347  1.366355  1.366348  1.366363   \n","4    1.323469  1.321951  1.321842  1.321652  1.321838  1.321779  1.322127   \n","5    1.271234  1.243167  1.242631  1.240222  1.241583  1.240969  1.244538   \n","6    2.286670  1.145433  1.144320  1.115955  1.124038  1.120317  1.162274   \n","7    4.521352  1.146990  1.187843  0.945027  0.935768  0.912765  1.324656   \n","8    3.774994  1.007358  1.450285  0.487091  0.141636  0.032512  1.942989   \n","9    4.412226  0.588430  1.866369  0.349090 -0.716906 -1.021551  2.443130   \n","10   5.019538  0.227126  1.888445  0.791654 -0.740773 -1.389133  2.334280   \n","11   5.358994 -0.157142  1.874660  1.107809 -0.432813 -1.539848  2.180945   \n","12   5.563088 -0.667493  1.897134  1.186427  0.060345 -1.626834  2.064752   \n","13   5.883640 -1.058375  1.881047  1.000066  0.534132 -1.786740  1.954329   \n","14   6.303962 -1.234468  1.797595  0.633750  0.895178 -2.012424  1.785623   \n","15   6.837324 -1.339337  1.672579  0.024722  1.110208 -2.174672  1.599338   \n","16   7.427869 -1.491255  1.433858 -0.485388  1.030484 -2.273295  1.329829   \n","17   7.982532 -1.674590  1.101997 -0.758202  0.738396 -2.346509  0.997491   \n","18   8.481820 -1.786896  0.806491 -0.997023  0.222622 -2.342515  0.705007   \n","19   8.934710 -1.828657  0.440821 -0.973854 -0.217193 -2.368635  0.369906   \n","20   9.377786 -1.917150  0.149085 -0.789178 -0.602083 -2.473271  0.105496   \n","21   9.834937 -2.024716 -0.164430 -0.514289 -0.915306 -2.598916 -0.173065   \n","22  10.318847 -2.092089 -0.485429 -0.121387 -1.087726 -2.689350 -0.458171   \n","23  10.813057 -2.093420 -0.791364  0.356859 -1.016962 -2.706887 -0.730867   \n","24  11.298434 -2.022815 -1.020691  0.780895 -0.666634 -2.682947 -0.932477   \n","25  11.766288 -1.889057 -1.299571  1.017477 -0.159251 -2.673756 -1.185550   \n","26  12.203382 -1.689771 -1.584885  0.984972  0.317080 -2.680233 -1.481935   \n","27  12.628815 -1.409874 -1.850814  0.727735  0.684783 -2.650165 -1.788742   \n","28  13.072464 -1.121183 -2.018804  0.276828  1.009536 -2.555903 -2.028759   \n","29  13.549956 -0.829171 -2.117776 -0.274666  1.093311 -2.372162 -2.205609   \n","30  14.052283 -0.465339 -2.066869 -0.695588  0.863206 -2.096478 -2.213705   \n","31  14.533471 -0.021433 -1.886216 -0.966109  0.517291 -1.748452 -2.060690   \n","32  15.001737  0.396265 -1.717975 -0.936786  0.120868 -1.439233 -1.941935   \n","33  15.464746  0.766099 -1.600744 -0.746933 -0.290252 -1.148647 -1.948827   \n","34  15.937218  1.056247 -1.457762 -0.471972 -0.578229 -0.829984 -1.970247   \n","35  16.436983  1.365839 -1.266414 -0.195082 -0.826373 -0.581953 -1.915703   \n","36  16.958950  1.585731 -1.004783  0.178392 -0.922430 -0.374385 -1.768340   \n","37  17.477139  1.780047 -0.640145  0.569267 -0.708304 -0.151685 -1.591688   \n","38  17.973789  1.901352 -0.230850  0.813695 -0.259706  0.110963 -1.539996   \n","39  18.446491  1.905706  0.052176  0.836801  0.100380  0.347887 -1.663893   \n","40  18.911346  1.811662  0.330609  0.686024  0.426084  0.473132 -1.817253   \n","41  19.393915  1.724316  0.514841  0.401804  0.650435  0.680229 -1.854092   \n","42  19.892784  1.553593  0.841788  0.052532  0.817920  1.014394 -1.757624   \n","43  20.391842  1.272578  1.301042 -0.346223  0.820290  1.418135 -1.625464   \n","44   2.199863  2.199863  2.199863  2.199863  2.199863  2.199863  2.199863   \n","45   2.242986  2.242986  2.242986  2.242986  2.242986  2.242986  2.242986   \n","46   2.293994  2.293994  2.293994  2.293994  2.293994  2.293994  2.293994   \n","47   2.322799  2.322799  2.322799  2.322799  2.322799  2.322799  2.322799   \n","48   2.325436  2.325436  2.325436  2.325436  2.325436  2.325436  2.325436   \n","49   2.310490  2.310490  2.310490  2.310490  2.310490  2.310490  2.310490   \n","50   2.294999  2.294999  2.294999  2.294999  2.294999  2.294999  2.294999   \n","51   2.271732  2.271732  2.271732  2.271732  2.271732  2.271732  2.271732   \n","52   2.247481  2.247481  2.247481  2.247481  2.247481  2.247481  2.247481   \n","53   2.237985  2.237985  2.237985  2.237985  2.237985  2.237985  2.237985   \n","54   2.228682  2.228682  2.228682  2.228682  2.228682  2.228682  2.228682   \n","55   2.218944  2.218944  2.218944  2.218944  2.218944  2.218944  2.218944   \n","56   2.200078  2.200078  2.200078  2.200078  2.200078  2.200078  2.200078   \n","57   2.168845  2.168845  2.168845  2.168845  2.168845  2.168845  2.168845   \n","58   2.150219  2.150219  2.150219  2.150219  2.150219  2.150219  2.150219   \n","\n","          7         8         9   ...        16        17        18        19  \\\n","0   1.437391  1.437354  1.437354  ...  1.437354  1.437354  1.437354  1.437354   \n","1   1.412170  1.412170  1.412170  ...  1.412170  1.412170  1.412170  1.412170   \n","2   1.391906  1.391906  1.391906  ...  1.391906  1.391906  1.391906  1.391906   \n","3   1.366355  1.366354  1.366354  ...  1.366355  1.366355  1.366355  1.366355   \n","4   1.321920  1.321971  1.321971  ...  1.321885  1.321885  1.321885  1.321885   \n","5   1.243003  1.243547  1.243487  ...  1.242753  1.242753  1.242753  1.242753   \n","6   1.145752  1.152200  1.151740  ...  1.142506  1.142506  1.142506  1.142506   \n","7   1.174479  1.234436  1.230192  ...  1.145297  1.145297  1.145297  1.145297   \n","8   1.287989  1.576423  1.552956  ...  1.142628  1.142628  1.142628  1.142628   \n","9   1.454736  1.956791  1.914964  ...  1.168766  1.168766  1.168766  1.168766   \n","10  1.421951  1.906135  1.866119  ...  1.155034  1.155034  1.155034  1.155034   \n","11  1.386842  1.854647  1.822358  ...  1.179546  1.179546  1.179546  1.179546   \n","12  1.384620  1.892544  1.866678  ...  1.185746  1.185746  1.185746  1.185746   \n","13  1.371077  1.956908  1.924489  ...  1.149803  1.149803  1.149803  1.149803   \n","14  1.410010  2.060913  2.010454  ...  1.146170  1.146170  1.146170  1.146170   \n","15  1.481306  2.196213  2.123794  ...  1.168332  1.168332  1.168332  1.168332   \n","16  1.459788  2.227226  2.148189  ...  1.135512  1.135512  1.135512  1.135512   \n","17  1.328974  2.108857  2.046296  ...  1.096514  1.096514  1.096514  1.096514   \n","18  1.272448  1.977945  1.933778  ...  1.072738  1.072738  1.072738  1.072738   \n","19  1.267811  1.945301  1.902387  ...  1.106788  1.106788  1.106788  1.106788   \n","20  1.329790  2.001824  1.946873  ...  1.134605  1.134605  1.134605  1.134605   \n","21  1.454163  2.049280  1.982165  ...  1.147231  1.147231  1.147231  1.147231   \n","22  1.453902  2.018181  1.951495  ...  1.133224  1.133224  1.133224  1.133224   \n","23  1.360321  1.981212  1.920103  ...  1.144190  1.144190  1.144190  1.144190   \n","24  1.300767  2.020786  1.981955  ...  1.082682  1.082682  1.082682  1.082682   \n","25  1.234440  2.158800  2.124010  ...  1.073079  1.073079  1.073079  1.073079   \n","26  1.242981  2.159756  2.100759  ...  1.078025  1.078025  1.078025  1.078025   \n","27  1.300387  2.088112  2.068450  ...  1.103690  1.103690  1.103690  1.103690   \n","28  1.346703  2.080505  2.058820  ...  1.137544  1.137544  1.137544  1.137544   \n","29  1.307930  2.142208  2.065599  ...  1.173489  1.173489  1.173489  1.173489   \n","30  1.289903  2.117284  2.067234  ...  1.189505  1.189505  1.189505  1.189505   \n","31  1.249960  2.014639  1.979736  ...  1.148023  1.148023  1.148023  1.148023   \n","32  1.255687  1.999335  1.975363  ...  1.154166  1.154166  1.154166  1.154166   \n","33  1.284989  1.996198  1.963521  ...  1.187071  1.187071  1.187071  1.187071   \n","34  1.312191  2.014191  1.954652  ...  1.227566  1.227566  1.227566  1.227566   \n","35  1.233081  1.981007  1.922105  ...  1.167316  1.167316  1.167316  1.167316   \n","36  1.126836  2.016287  1.975924  ...  1.054133  1.054133  1.054133  1.054133   \n","37  1.106603  2.054725  2.058574  ...  1.016432  1.016432  1.016432  1.016432   \n","38  1.129792  2.031977  2.038699  ...  1.037239  1.037239  1.037239  1.037239   \n","39  1.096858  1.981099  1.983657  ...  1.012342  1.012342  1.012342  1.012342   \n","40  1.065089  1.917113  1.906045  ...  0.997039  0.997039  0.997039  0.997039   \n","41  1.041919  1.971945  1.919307  ...  0.995124  0.995124  0.995124  0.995124   \n","42  0.990485  2.076530  2.052320  ...  0.959452  0.959452  0.959452  0.959452   \n","43  1.017228  2.242980  2.237311  ...  0.939862  0.939862  0.939862  0.939862   \n","44  2.199863  2.199863  2.199863  ...  2.199863  2.199863  2.199863  2.199863   \n","45  2.242986  2.242986  2.242986  ...  2.242986  2.242986  2.242986  2.242986   \n","46  2.293994  2.293994  2.293994  ...  2.293994  2.293994  2.293994  2.293994   \n","47  2.322799  2.322799  2.322799  ...  2.322799  2.322799  2.322799  2.322799   \n","48  2.325436  2.325436  2.325436  ...  2.325436  2.325436  2.325436  2.325436   \n","49  2.310490  2.310490  2.310490  ...  2.310490  2.310490  2.310490  2.310490   \n","50  2.294999  2.294999  2.294999  ...  2.294999  2.294999  2.294999  2.294999   \n","51  2.271732  2.271732  2.271732  ...  2.271732  2.271732  2.271732  2.271732   \n","52  2.247481  2.247481  2.247481  ...  2.247481  2.247481  2.247481  2.247481   \n","53  2.237985  2.237985  2.237985  ...  2.237985  2.237985  2.237985  2.237985   \n","54  2.228682  2.228682  2.228682  ...  2.228682  2.228682  2.228682  2.228682   \n","55  2.218944  2.218944  2.218944  ...  2.218944  2.218944  2.218944  2.218944   \n","56  2.200078  2.200078  2.200078  ...  2.200078  2.200078  2.200078  2.200078   \n","57  2.168845  2.168845  2.168845  ...  2.168845  2.168845  2.168845  2.168845   \n","58  2.150219  2.150219  2.150219  ...  2.150219  2.150219  2.150219  2.150219   \n","\n","          20        21        22        23        24        25  \n","0   1.437354  1.437354  1.437354  1.437354  1.437354  1.437354  \n","1   1.412170  1.412170  1.412170  1.412170  1.412170  1.412170  \n","2   1.391906  1.391906  1.391906  1.391906  1.391906  1.391906  \n","3   1.366355  1.366355  1.366355  1.366355  1.366355  1.366355  \n","4   1.321885  1.321885  1.321885  1.321885  1.321885  1.321885  \n","5   1.242753  1.242753  1.242753  1.242753  1.242753  1.242753  \n","6   1.142506  1.142506  1.142506  1.142506  1.142506  1.142506  \n","7   1.145297  1.145297  1.145297  1.145297  1.145297  1.145297  \n","8   1.142628  1.142628  1.142628  1.142628  1.142628  1.142628  \n","9   1.168766  1.168766  1.168766  1.168766  1.168766  1.168766  \n","10  1.155034  1.155034  1.155034  1.155034  1.155034  1.155034  \n","11  1.179546  1.179546  1.179546  1.179546  1.179546  1.179546  \n","12  1.185746  1.185746  1.185746  1.185746  1.185746  1.185746  \n","13  1.149803  1.149803  1.149803  1.149803  1.149803  1.149803  \n","14  1.146170  1.146170  1.146170  1.146170  1.146170  1.146170  \n","15  1.168332  1.168332  1.168332  1.168332  1.168332  1.168332  \n","16  1.135512  1.135512  1.135512  1.135512  1.135512  1.135512  \n","17  1.096514  1.096514  1.096514  1.096514  1.096514  1.096514  \n","18  1.072738  1.072738  1.072738  1.072738  1.072738  1.072738  \n","19  1.106788  1.106788  1.106788  1.106788  1.106788  1.106788  \n","20  1.134605  1.134605  1.134605  1.134605  1.134605  1.134605  \n","21  1.147231  1.147231  1.147231  1.147231  1.147231  1.147231  \n","22  1.133224  1.133224  1.133224  1.133224  1.133224  1.133224  \n","23  1.144190  1.144190  1.144190  1.144190  1.144190  1.144190  \n","24  1.082682  1.082682  1.082682  1.082682  1.082682  1.082682  \n","25  1.073079  1.073079  1.073079  1.073079  1.073079  1.073079  \n","26  1.078025  1.078025  1.078025  1.078025  1.078025  1.078025  \n","27  1.103690  1.103690  1.103690  1.103690  1.103690  1.103690  \n","28  1.137544  1.137544  1.137544  1.137544  1.137544  1.137544  \n","29  1.173489  1.173489  1.173489  1.173489  1.173489  1.173489  \n","30  1.189505  1.189505  1.189505  1.189505  1.189505  1.189505  \n","31  1.148023  1.148023  1.148023  1.148023  1.148023  1.148023  \n","32  1.154166  1.154166  1.154166  1.154166  1.154166  1.154166  \n","33  1.187071  1.187071  1.187071  1.187071  1.187071  1.187071  \n","34  1.227566  1.227566  1.227566  1.227566  1.227566  1.227566  \n","35  1.167316  1.167316  1.167316  1.167316  1.167316  1.167316  \n","36  1.054133  1.054133  1.054133  1.054133  1.054133  1.054133  \n","37  1.016432  1.016432  1.016432  1.016432  1.016432  1.016432  \n","38  1.037239  1.037239  1.037239  1.037239  1.037239  1.037239  \n","39  1.012342  1.012342  1.012342  1.012342  1.012342  1.012342  \n","40  0.997039  0.997039  0.997039  0.997039  0.997039  0.997039  \n","41  0.995124  0.995124  0.995124  0.995124  0.995124  0.995124  \n","42  0.959452  0.959452  0.959452  0.959452  0.959452  0.959452  \n","43  0.939862  0.939862  0.939862  0.939862  0.939862  0.939862  \n","44  2.199863  2.199863  2.199863  2.199863  2.199863  2.199863  \n","45  2.242986  2.242986  2.242986  2.242986  2.242986  2.242986  \n","46  2.293994  2.293994  2.293994  2.293994  2.293994  2.293994  \n","47  2.322799  2.322799  2.322799  2.322799  2.322799  2.322799  \n","48  2.325436  2.325436  2.325436  2.325436  2.325436  2.325436  \n","49  2.310490  2.310490  2.310490  2.310490  2.310490  2.310490  \n","50  2.294999  2.294999  2.294999  2.294999  2.294999  2.294999  \n","51  2.271732  2.271732  2.271732  2.271732  2.271732  2.271732  \n","52  2.247481  2.247481  2.247481  2.247481  2.247481  2.247481  \n","53  2.237985  2.237985  2.237985  2.237985  2.237985  2.237985  \n","54  2.228682  2.228682  2.228682  2.228682  2.228682  2.228682  \n","55  2.218944  2.218944  2.218944  2.218944  2.218944  2.218944  \n","56  2.200078  2.200078  2.200078  2.200078  2.200078  2.200078  \n","57  2.168845  2.168845  2.168845  2.168845  2.168845  2.168845  \n","58  2.150219  2.150219  2.150219  2.150219  2.150219  2.150219  \n","\n","[59 rows x 26 columns]"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["df_pred"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>1.560060</td>\n","      <td>-0.854437</td>\n","      <td>0.720639</td>\n","      <td>0.691729</td>\n","      <td>0.944008</td>\n","      <td>2.700632</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.465116</td>\n","      <td>1.689858</td>\n","      <td>-0.514359</td>\n","      <td>0.333295</td>\n","      <td>0.942289</td>\n","      <td>0.681604</td>\n","      <td>2.785811</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.930233</td>\n","      <td>1.753589</td>\n","      <td>-0.154209</td>\n","      <td>-0.124995</td>\n","      <td>0.992368</td>\n","      <td>0.412951</td>\n","      <td>2.845461</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.395349</td>\n","      <td>1.748068</td>\n","      <td>0.212022</td>\n","      <td>-0.556775</td>\n","      <td>0.831727</td>\n","      <td>0.140540</td>\n","      <td>2.879232</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.860465</td>\n","      <td>1.673573</td>\n","      <td>0.569904</td>\n","      <td>-0.870579</td>\n","      <td>0.494812</td>\n","      <td>-0.133144</td>\n","      <td>2.887018</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2.325581</td>\n","      <td>1.533811</td>\n","      <td>0.905607</td>\n","      <td>-1.000151</td>\n","      <td>0.053178</td>\n","      <td>-0.405638</td>\n","      <td>2.868949</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2.790698</td>\n","      <td>1.335526</td>\n","      <td>1.206832</td>\n","      <td>-0.918192</td>\n","      <td>-0.399688</td>\n","      <td>-0.674530</td>\n","      <td>2.825381</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>3.255814</td>\n","      <td>1.087824</td>\n","      <td>1.463513</td>\n","      <td>-0.641980</td>\n","      <td>-0.767958</td>\n","      <td>-0.937474</td>\n","      <td>2.756889</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>3.720930</td>\n","      <td>0.801365</td>\n","      <td>1.668203</td>\n","      <td>-0.229808</td>\n","      <td>-0.973515</td>\n","      <td>-1.192211</td>\n","      <td>2.664256</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>4.186047</td>\n","      <td>0.487549</td>\n","      <td>1.816168</td>\n","      <td>0.231092</td>\n","      <td>-0.972449</td>\n","      <td>-1.436585</td>\n","      <td>2.548458</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>4.651163</td>\n","      <td>0.157822</td>\n","      <td>1.905225</td>\n","      <td>0.642811</td>\n","      <td>-0.764575</td>\n","      <td>-1.668554</td>\n","      <td>2.410650</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>5.116279</td>\n","      <td>-0.176870</td>\n","      <td>1.935436</td>\n","      <td>0.917559</td>\n","      <td>-0.393801</td>\n","      <td>-1.886207</td>\n","      <td>2.252158</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>5.581395</td>\n","      <td>-0.506443</td>\n","      <td>1.908720</td>\n","      <td>0.996574</td>\n","      <td>0.061000</td>\n","      <td>-2.087774</td>\n","      <td>2.074456</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>6.046512</td>\n","      <td>-0.821885</td>\n","      <td>1.828463</td>\n","      <td>0.862921</td>\n","      <td>0.502832</td>\n","      <td>-2.271633</td>\n","      <td>1.879159</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>6.511628</td>\n","      <td>-1.115352</td>\n","      <td>1.699162</td>\n","      <td>0.545212</td>\n","      <td>0.837490</td>\n","      <td>-2.436322</td>\n","      <td>1.668004</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>6.976744</td>\n","      <td>-1.380170</td>\n","      <td>1.526119</td>\n","      <td>0.111347</td>\n","      <td>0.993853</td>\n","      <td>-2.580544</td>\n","      <td>1.442838</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>7.441860</td>\n","      <td>-1.610795</td>\n","      <td>1.315203</td>\n","      <td>-0.346239</td>\n","      <td>0.939015</td>\n","      <td>-2.703172</td>\n","      <td>1.205599</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>7.906977</td>\n","      <td>-1.802741</td>\n","      <td>1.072663</td>\n","      <td>-0.730422</td>\n","      <td>0.685027</td>\n","      <td>-2.803255</td>\n","      <td>0.958309</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>8.372093</td>\n","      <td>-1.952503</td>\n","      <td>0.804994</td>\n","      <td>-0.959957</td>\n","      <td>0.286020</td>\n","      <td>-2.880023</td>\n","      <td>0.703051</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>8.837209</td>\n","      <td>-2.057485</td>\n","      <td>0.518845</td>\n","      <td>-0.986439</td>\n","      <td>-0.173427</td>\n","      <td>-2.932885</td>\n","      <td>0.441961</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>9.302326</td>\n","      <td>-2.115938</td>\n","      <td>0.220954</td>\n","      <td>-0.804305</td>\n","      <td>-0.596103</td>\n","      <td>-2.961438</td>\n","      <td>0.177211</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>9.767442</td>\n","      <td>-2.126912</td>\n","      <td>-0.081893</td>\n","      <td>-0.451965</td>\n","      <td>-0.892498</td>\n","      <td>-2.965464</td>\n","      <td>-0.089004</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>10.232558</td>\n","      <td>-2.090237</td>\n","      <td>-0.382893</td>\n","      <td>-0.003872</td>\n","      <td>-0.999585</td>\n","      <td>-2.944932</td>\n","      <td>-0.354477</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>10.697674</td>\n","      <td>-2.006509</td>\n","      <td>-0.675252</td>\n","      <td>0.444975</td>\n","      <td>-0.894252</td>\n","      <td>-2.899997</td>\n","      <td>-0.617010</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>11.162791</td>\n","      <td>-1.877113</td>\n","      <td>-0.952210</td>\n","      <td>0.799049</td>\n","      <td>-0.598498</td>\n","      <td>-2.831004</td>\n","      <td>-0.874419</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>11.627907</td>\n","      <td>-1.704255</td>\n","      <td>-1.207074</td>\n","      <td>0.982727</td>\n","      <td>-0.175061</td>\n","      <td>-2.738478</td>\n","      <td>-1.124556</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>12.093023</td>\n","      <td>-1.491017</td>\n","      <td>-1.433278</td>\n","      <td>0.956673</td>\n","      <td>0.285841</td>\n","      <td>-2.623132</td>\n","      <td>-1.365315</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>12.558140</td>\n","      <td>-1.241423</td>\n","      <td>-1.624455</td>\n","      <td>0.726448</td>\n","      <td>0.685888</td>\n","      <td>-2.485858</td>\n","      <td>-1.594656</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>13.023256</td>\n","      <td>-0.960518</td>\n","      <td>-1.774551</td>\n","      <td>0.341306</td>\n","      <td>0.939879</td>\n","      <td>-2.327724</td>\n","      <td>-1.810607</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>13.488372</td>\n","      <td>-0.654444</td>\n","      <td>-1.877989</td>\n","      <td>-0.116555</td>\n","      <td>0.994010</td>\n","      <td>-2.149972</td>\n","      <td>-2.011292</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>13.953488</td>\n","      <td>-0.330492</td>\n","      <td>-1.929876</td>\n","      <td>-0.549761</td>\n","      <td>0.837182</td>\n","      <td>-1.954013</td>\n","      <td>-2.194933</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>14.418605</td>\n","      <td>0.002871</td>\n","      <td>-1.926281</td>\n","      <td>-0.866538</td>\n","      <td>0.503039</td>\n","      <td>-1.741415</td>\n","      <td>-2.359874</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>14.883721</td>\n","      <td>0.336052</td>\n","      <td>-1.864565</td>\n","      <td>-1.000002</td>\n","      <td>0.062565</td>\n","      <td>-1.513904</td>\n","      <td>-2.504591</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>15.348837</td>\n","      <td>0.658485</td>\n","      <td>-1.743757</td>\n","      <td>-0.922043</td>\n","      <td>-0.390993</td>\n","      <td>-1.273345</td>\n","      <td>-2.627708</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>15.813953</td>\n","      <td>0.958914</td>\n","      <td>-1.564952</td>\n","      <td>-0.649103</td>\n","      <td>-0.761665</td>\n","      <td>-1.021741</td>\n","      <td>-2.728011</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>16.279070</td>\n","      <td>1.225853</td>\n","      <td>-1.331663</td>\n","      <td>-0.238785</td>\n","      <td>-0.970834</td>\n","      <td>-0.761211</td>\n","      <td>-2.804462</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>16.744186</td>\n","      <td>1.448211</td>\n","      <td>-1.050060</td>\n","      <td>0.222075</td>\n","      <td>-0.973830</td>\n","      <td>-0.493987</td>\n","      <td>-2.856214</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>17.209302</td>\n","      <td>1.616059</td>\n","      <td>-0.729007</td>\n","      <td>0.635585</td>\n","      <td>-0.769602</td>\n","      <td>-0.222387</td>\n","      <td>-2.882622</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>17.674419</td>\n","      <td>1.721456</td>\n","      <td>-0.379819</td>\n","      <td>0.913575</td>\n","      <td>-0.401274</td>\n","      <td>0.051193</td>\n","      <td>-2.883258</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>18.139535</td>\n","      <td>1.759203</td>\n","      <td>-0.015710</td>\n","      <td>0.996587</td>\n","      <td>0.052806</td>\n","      <td>0.324305</td>\n","      <td>-2.857916</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>18.604651</td>\n","      <td>1.727385</td>\n","      <td>0.349052</td>\n","      <td>0.866830</td>\n","      <td>0.495796</td>\n","      <td>0.594465</td>\n","      <td>-2.806627</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>19.069767</td>\n","      <td>1.627587</td>\n","      <td>0.700142</td>\n","      <td>0.552081</td>\n","      <td>0.833240</td>\n","      <td>0.859181</td>\n","      <td>-2.729662</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>19.534884</td>\n","      <td>1.464716</td>\n","      <td>1.024199</td>\n","      <td>0.119611</td>\n","      <td>0.993425</td>\n","      <td>1.115970</td>\n","      <td>-2.627536</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>20.000000</td>\n","      <td>1.246492</td>\n","      <td>1.309683</td>\n","      <td>-0.338443</td>\n","      <td>0.942632</td>\n","      <td>1.362393</td>\n","      <td>-2.501014</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>59 rows × 26 columns</p>\n","</div>"],"text/plain":["           0         1         2         3         4         5         6   \\\n","0    0.000000  1.560060 -0.854437  0.720639  0.691729  0.944008  2.700632   \n","1    0.465116  1.689858 -0.514359  0.333295  0.942289  0.681604  2.785811   \n","2    0.930233  1.753589 -0.154209 -0.124995  0.992368  0.412951  2.845461   \n","3    1.395349  1.748068  0.212022 -0.556775  0.831727  0.140540  2.879232   \n","4    1.860465  1.673573  0.569904 -0.870579  0.494812 -0.133144  2.887018   \n","5    2.325581  1.533811  0.905607 -1.000151  0.053178 -0.405638  2.868949   \n","6    2.790698  1.335526  1.206832 -0.918192 -0.399688 -0.674530  2.825381   \n","7    3.255814  1.087824  1.463513 -0.641980 -0.767958 -0.937474  2.756889   \n","8    3.720930  0.801365  1.668203 -0.229808 -0.973515 -1.192211  2.664256   \n","9    4.186047  0.487549  1.816168  0.231092 -0.972449 -1.436585  2.548458   \n","10   4.651163  0.157822  1.905225  0.642811 -0.764575 -1.668554  2.410650   \n","11   5.116279 -0.176870  1.935436  0.917559 -0.393801 -1.886207  2.252158   \n","12   5.581395 -0.506443  1.908720  0.996574  0.061000 -2.087774  2.074456   \n","13   6.046512 -0.821885  1.828463  0.862921  0.502832 -2.271633  1.879159   \n","14   6.511628 -1.115352  1.699162  0.545212  0.837490 -2.436322  1.668004   \n","15   6.976744 -1.380170  1.526119  0.111347  0.993853 -2.580544  1.442838   \n","16   7.441860 -1.610795  1.315203 -0.346239  0.939015 -2.703172  1.205599   \n","17   7.906977 -1.802741  1.072663 -0.730422  0.685027 -2.803255  0.958309   \n","18   8.372093 -1.952503  0.804994 -0.959957  0.286020 -2.880023  0.703051   \n","19   8.837209 -2.057485  0.518845 -0.986439 -0.173427 -2.932885  0.441961   \n","20   9.302326 -2.115938  0.220954 -0.804305 -0.596103 -2.961438  0.177211   \n","21   9.767442 -2.126912 -0.081893 -0.451965 -0.892498 -2.965464 -0.089004   \n","22  10.232558 -2.090237 -0.382893 -0.003872 -0.999585 -2.944932 -0.354477   \n","23  10.697674 -2.006509 -0.675252  0.444975 -0.894252 -2.899997 -0.617010   \n","24  11.162791 -1.877113 -0.952210  0.799049 -0.598498 -2.831004 -0.874419   \n","25  11.627907 -1.704255 -1.207074  0.982727 -0.175061 -2.738478 -1.124556   \n","26  12.093023 -1.491017 -1.433278  0.956673  0.285841 -2.623132 -1.365315   \n","27  12.558140 -1.241423 -1.624455  0.726448  0.685888 -2.485858 -1.594656   \n","28  13.023256 -0.960518 -1.774551  0.341306  0.939879 -2.327724 -1.810607   \n","29  13.488372 -0.654444 -1.877989 -0.116555  0.994010 -2.149972 -2.011292   \n","30  13.953488 -0.330492 -1.929876 -0.549761  0.837182 -1.954013 -2.194933   \n","31  14.418605  0.002871 -1.926281 -0.866538  0.503039 -1.741415 -2.359874   \n","32  14.883721  0.336052 -1.864565 -1.000002  0.062565 -1.513904 -2.504591   \n","33  15.348837  0.658485 -1.743757 -0.922043 -0.390993 -1.273345 -2.627708   \n","34  15.813953  0.958914 -1.564952 -0.649103 -0.761665 -1.021741 -2.728011   \n","35  16.279070  1.225853 -1.331663 -0.238785 -0.970834 -0.761211 -2.804462   \n","36  16.744186  1.448211 -1.050060  0.222075 -0.973830 -0.493987 -2.856214   \n","37  17.209302  1.616059 -0.729007  0.635585 -0.769602 -0.222387 -2.882622   \n","38  17.674419  1.721456 -0.379819  0.913575 -0.401274  0.051193 -2.883258   \n","39  18.139535  1.759203 -0.015710  0.996587  0.052806  0.324305 -2.857916   \n","40  18.604651  1.727385  0.349052  0.866830  0.495796  0.594465 -2.806627   \n","41  19.069767  1.627587  0.700142  0.552081  0.833240  0.859181 -2.729662   \n","42  19.534884  1.464716  1.024199  0.119611  0.993425  1.115970 -2.627536   \n","43  20.000000  1.246492  1.309683 -0.338443  0.942632  1.362393 -2.501014   \n","44        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","45        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","46        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","47        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","48        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","49        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","50        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","51        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","52        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","53        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","54        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","55        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","56        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","57        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","58        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","\n","          7         8         9   ...  16  17  18  19  20  21  22  23  24  25  \n","0   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","1   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","2   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","3   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","4   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","5   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","6   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","7   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","8   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","9   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","10  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","11  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","12  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","13  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","14  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","15  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","16  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","17  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","18  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","19  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","20  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","21  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","22  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","23  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","24  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","25  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","26  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","27  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","28  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","29  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","30  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","31  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","32  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","33  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","34  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","35  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","36  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","37  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","38  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","39  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","40  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","41  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","42  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","43  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","44       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","45       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","46       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","47       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","48       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","49       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","50       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","51       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","52       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","53       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","54       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","55       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","56       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","57       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","58       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","\n","[59 rows x 26 columns]"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["df_actual"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_name = f\"big_train_{dt.now()}_\"\n","\n","current_dir = os.getcwd()\n","\n","if not os.path.exists(\"./pre_trained_models/\"):\n","    os.makedirs(\"./pre_trained_models/\")\n","\n","path = os.path.join(current_dir, \"./pre_trained_models/\")\n","\n","\n","ckpt_dir = f\"./pre_trained_models/{model_name}\"\n","\n","# checkpoints.save_checkpoint(\n","#     ckpt_dir=path, target=state, step=batch_count, overwrite=True, prefix=model_name\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vz702qVy_AKz"},"outputs":[],"source":["def jax_array_memory_usage(array):\n","    \"\"\"Calculate the memory usage of a JAX array in bytes.\"\"\"\n","    # Get the number of bytes per element based on the data type\n","    bytes_per_element = array.dtype.itemsize\n","    # Calculate the total number of elements in the array\n","    total_elements = np.prod(array.shape)\n","    # Calculate total memory usage\n","    memory_usage_bytes = bytes_per_element * total_elements\n","    return memory_usage_bytes\n","\n","\n","cat_memory_usage = jax_array_memory_usage(batch.categorical)\n","num_memory_usage = jax_array_memory_usage(batch.numeric)\n","memory_usage = cat_memory_usage + num_memory_usage\n","memory_usage_gb = memory_usage / 1024 / 1024 / 1024\n","print(f\"Memory usage: {memory_usage} bytes\")\n","print(f\"Memory usage: {memory_usage_gb} gb\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[[[False False False]\n","   [ True  True  True]\n","   [ True  True  True]\n","   [ True  True  True]\n","   [ True  True  True]]\n","\n","  [[False False False]\n","   [False False False]\n","   [ True  True  True]\n","   [ True  True  True]\n","   [ True  True  True]]\n","\n","  [[False False False]\n","   [False False False]\n","   [False False False]\n","   [ True  True  True]\n","   [ True  True  True]]\n","\n","  [[False False False]\n","   [False False False]\n","   [False False False]\n","   [False False False]\n","   [ True  True  True]]\n","\n","  [[False False False]\n","   [False False False]\n","   [False False False]\n","   [False False False]\n","   [False False False]]]\n","\n","\n"," [[[False False False]\n","   [ True  True  True]\n","   [ True  True  True]\n","   [ True  True  True]\n","   [ True  True  True]]\n","\n","  [[False False False]\n","   [False False False]\n","   [ True  True  True]\n","   [ True  True  True]\n","   [ True  True  True]]\n","\n","  [[False False False]\n","   [False False False]\n","   [False False False]\n","   [ True  True  True]\n","   [ True  True  True]]\n","\n","  [[False False False]\n","   [False False False]\n","   [False False False]\n","   [False False False]\n","   [ True  True  True]]\n","\n","  [[False False False]\n","   [False False False]\n","   [False False False]\n","   [False False False]\n","   [False False False]]]]\n"]}],"source":["import jax\n","import jax.numpy as jnp\n","\n","\n","def create_future_mask(seq_len):\n","    # Create a basic mask for future sequences\n","    mask = jnp.triu(jnp.ones((seq_len, seq_len)), k=1).astype(bool)\n","    return mask\n","\n","\n","def create_custom_mask(batch_size, seq_len, n_columns):\n","    # Create the future sequence mask\n","    future_mask = create_future_mask(seq_len)\n","\n","    # Expand the mask to match the required dimensions\n","    mask = jnp.expand_dims(future_mask, axis=0)  # Shape: (1, seq_len, seq_len)\n","    mask = jnp.expand_dims(mask, axis=-1)  # Shape: (1, seq_len, seq_len, 1)\n","    mask = jnp.tile(\n","        mask, (batch_size, 1, 1, n_columns)\n","    )  # Shape: (batch_size, seq_len, seq_len, n_columns)\n","\n","    return mask\n","\n","\n","# Example usage:\n","batch_size = 2\n","seq_len = 5\n","n_columns = 3\n","embedding_dim = 4\n","\n","mask = create_custom_mask(batch_size, seq_len, n_columns)\n","\n","print(mask)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import jax\n","import jax.numpy as jnp\n","from jax import random\n","\n","\n","# Create the custom mask function\n","def create_future_mask(seq_len):\n","    # Create a basic mask for future sequences\n","    mask = jnp.triu(jnp.ones((seq_len, seq_len)), k=1).astype(bool)\n","    return mask\n","\n","\n","def create_custom_mask(batch_size, seq_len, n_columns):\n","    # Create the future sequence mask\n","    future_mask = create_future_mask(seq_len)\n","\n","    # Expand the mask to match the required dimensions\n","    mask = jnp.expand_dims(future_mask, axis=0)  # Shape: (1, seq_len, seq_len)\n","    mask = jnp.expand_dims(mask, axis=-1)  # Shape: (1, seq_len, seq_len, 1)\n","    mask = jnp.tile(\n","        mask, (batch_size, 1, 1, n_columns)\n","    )  # Shape: (batch_size, seq_len, seq_len, n_columns)\n","\n","    return mask\n","\n","\n","# Generate random input data\n","key = random.PRNGKey(0)\n","batch_size = 2\n","seq_len = 5\n","n_columns = 3\n","embedding_dim = 4\n","\n","input_data = random.normal(key, (batch_size, seq_len, n_columns, embedding_dim))\n","print(\"Input Data:\")\n","print(input_data)\n","\n","# Create the mask\n","mask = create_custom_mask(batch_size, seq_len, n_columns)\n","print(\"Mask:\")\n","print(mask)\n","\n","# Apply the mask to the input data (for demonstration purposes, we'll just print the mask applied to some dummy attention scores)\n","dummy_attention_scores = random.normal(key, (batch_size, seq_len, seq_len, n_columns))\n","masked_attention_scores = jnp.where(mask, -jnp.inf, dummy_attention_scores)\n","\n","print(\"Dummy Attention Scores:\")\n","print(dummy_attention_scores)\n","\n","print(\"Masked Attention Scores:\")\n","print(masked_attention_scores)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0;31mInit signature:\u001b[0m\n","\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mnum_heads\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupportsDType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mparam_dtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupportsDType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'jax.numpy.float32'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mqkv_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mout_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mbroadcast_dropout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdropout_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mprecision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNoneType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mkernel_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mvariance_scaling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x139d34160\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mbias_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mzeros\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x11908cee0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mattention_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mdot_product_attention\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x139d4b400\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdecode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mnormalize_qk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mqkv_dot_general\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mout_dot_general\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mqkv_dot_general_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mout_dot_general_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mparent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Sentinel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Sentinel\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x139cfebc0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDocstring:\u001b[0m     \n","Multi-head dot-product attention.\n","Alias for ``MultiHeadDotProductAttention``.\n","\n","**NOTE**: ``MultiHeadAttention`` is a wrapper of ``MultiHeadDotProductAttention``,\n","and so their implementations are identical. However ``MultiHeadAttention`` layers\n","will, by default, be named ``MultiHeadAttention_{index}``, whereas ``MultiHeadDotProductAttention``\n","will be named ``MultiHeadDotProductAttention_{index}``. Therefore, this could affect\n","checkpointing, param collection names and RNG threading (since the layer name is\n","used when generating new RNG's) within the module.\n","\n","Example usage::\n","\n","  >>> import flax.linen as nn\n","  >>> import jax\n","\n","  >>> layer = nn.MultiHeadAttention(num_heads=8, qkv_features=16)\n","  >>> key1, key2, key3, key4, key5, key6 = jax.random.split(jax.random.key(0), 6)\n","  >>> shape = (4, 3, 2, 5)\n","  >>> q, k, v = jax.random.uniform(key1, shape), jax.random.uniform(key2, shape), jax.random.uniform(key3, shape)\n","  >>> variables = layer.init(jax.random.key(0), q)\n","\n","  >>> # different inputs for inputs_q, inputs_k and inputs_v\n","  >>> out = layer.apply(variables, q, k, v)\n","  >>> # equivalent to layer.apply(variables, inputs_q=q, inputs_k=k, inputs_v=k)\n","  >>> out = layer.apply(variables, q, k)\n","  >>> # equivalent to layer.apply(variables, inputs_q=q, inputs_k=q) and layer.apply(variables, inputs_q=q, inputs_k=q, inputs_v=q)\n","  >>> out = layer.apply(variables, q)\n","\n","  >>> attention_kwargs = dict(\n","  ...     num_heads=8,\n","  ...     qkv_features=16,\n","  ...     kernel_init=nn.initializers.ones,\n","  ...     bias_init=nn.initializers.zeros,\n","  ...     dropout_rate=0.5,\n","  ...     deterministic=False,\n","  ...     )\n","  >>> class Module(nn.Module):\n","  ...   attention_kwargs: dict\n","  ...\n","  ...   @nn.compact\n","  ...   def __call__(self, x, dropout_rng=None):\n","  ...     out1 = nn.MultiHeadAttention(**self.attention_kwargs)(x, dropout_rng=dropout_rng)\n","  ...     out2 = nn.MultiHeadAttention(**self.attention_kwargs)(x, dropout_rng=dropout_rng)\n","  ...     return out1, out2\n","  >>> module = Module(attention_kwargs)\n","  >>> variables = module.init({'params': key1, 'dropout': key2}, q)\n","\n","  >>> # out1 and out2 are different.\n","  >>> out1, out2 = module.apply(variables, q, rngs={'dropout': key3})\n","  >>> # out3 and out4 are different.\n","  >>> # out1 and out3 are different. out2 and out4 are different.\n","  >>> out3, out4 = module.apply(variables, q, rngs={'dropout': key4})\n","  >>> # out1 and out2 are the same.\n","  >>> out1, out2 = module.apply(variables, q, dropout_rng=key5)\n","  >>> # out1 and out2 are the same as out3 and out4.\n","  >>> # providing a `dropout_rng` arg will take precedence over the `rngs` arg in `.apply`\n","  >>> out3, out4 = module.apply(variables, q, rngs={'dropout': key6}, dropout_rng=key5)\n","\n","Attributes:\n","  num_heads: number of attention heads. Features (i.e. inputs_q.shape[-1])\n","    should be divisible by the number of heads.\n","  dtype: the dtype of the computation (default: infer from inputs and params)\n","  param_dtype: the dtype passed to parameter initializers (default: float32)\n","  qkv_features: dimension of the key, query, and value.\n","  out_features: dimension of the last projection\n","  broadcast_dropout: bool: use a broadcasted dropout along batch dims.\n","  dropout_rate: dropout rate\n","  deterministic: if false, the attention weight is masked randomly using\n","    dropout, whereas if true, the attention weights are deterministic.\n","  precision: numerical precision of the computation see ``jax.lax.Precision``\n","    for details.\n","  kernel_init: initializer for the kernel of the Dense layers.\n","  bias_init: initializer for the bias of the Dense layers.\n","  use_bias: bool: whether pointwise QKVO dense transforms use bias.\n","  attention_fn: dot_product_attention or compatible function. Accepts query,\n","    key, value, and returns output of shape ``[bs, dim1, dim2, ..., dimN,,\n","    num_heads, value_channels]``\n","  decode: whether to prepare and use an autoregressive cache.\n","  normalize_qk: should QK normalization be applied (arxiv.org/abs/2302.05442).\n","\u001b[0;31mFile:\u001b[0m           ~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py\n","\u001b[0;31mType:\u001b[0m           type\n","\u001b[0;31mSubclasses:\u001b[0m     "]}],"source":["from flax import linen as nn\n","\n","?nn.MultiHeadAttention"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input Data:\n","[[[[-5.47799706e-01 -1.17179680e+00  1.45061789e-02  2.34819144e-01\n","     3.00504017e+00  1.99740767e-01 -6.31268919e-01 -2.68845528e-01\n","    -5.21487474e-01 -1.72483146e+00 -2.67246771e+00 -1.67209733e+00\n","    -1.23799145e-01 -7.75377810e-01  7.31753230e-01 -4.72956657e-01]\n","   [-7.59660363e-01 -1.43394160e+00  9.79405999e-01  2.70364374e-01\n","    -7.26617202e-02  1.33200324e+00  1.11467469e+00  5.46375573e-01\n","    -3.29602033e-01 -6.35213614e-01  8.06641698e-01  1.48840249e+00\n","     6.16176844e-01 -4.41124678e-01  5.02394319e-01 -4.20634806e-01]\n","   [-2.35249791e-02 -1.27048820e-01 -7.40331471e-01 -1.69789433e+00\n","    -5.50638080e-01  3.32435369e-01 -6.02500677e-01 -7.93417037e-01\n","    -1.91679239e+00  4.30762082e-01  2.93178469e-01  3.57544348e-02\n","     2.85551995e-01  1.14142168e+00 -6.61235869e-01  1.30349076e+00]]\n","\n","  [[ 7.65341893e-02 -5.83034575e-01  3.56375240e-02 -1.11470902e+00\n","     6.09569371e-01 -1.52801716e+00  1.40855372e+00 -1.50325167e+00\n","    -1.90268196e-02  1.37721896e+00  1.01575285e-01  7.35573828e-01\n","    -7.44182229e-01 -6.10773265e-01 -1.37155894e-02  7.49079645e-01]\n","   [ 1.16729426e+00  9.47832465e-01 -8.75902995e-02  1.43540037e+00\n","     2.03567296e-01 -6.49375856e-01 -1.14499569e+00  5.55262744e-01\n","    -6.96689665e-01 -6.59611166e-01  1.26914454e+00 -2.01910585e-01\n","     1.20939435e-02 -8.60836446e-01 -6.29101872e-01 -1.26618731e+00]\n","   [-2.29058251e-01  2.14313731e-01 -9.98690546e-01  3.07292295e+00\n","     8.91804874e-01 -8.39625478e-01 -2.47639561e+00  1.25158325e-01\n","    -1.26054823e+00  2.00455189e-02  1.84192151e-01  1.27778888e+00\n","    -4.27830726e-01  1.40636250e-01 -3.03655230e-02  9.94564891e-01]]\n","\n","  [[-4.26839054e-01 -1.31841564e+00  2.12921351e-02 -1.13130033e+00\n","    -8.87557119e-02 -1.60422766e+00 -1.29964411e+00 -2.36143488e-02\n","    -8.03564370e-01 -1.38650641e-01 -2.79372156e-01  1.14396751e+00\n","     1.53220305e-02 -5.14247715e-01 -1.96691751e+00 -1.34604469e-01]\n","   [ 4.32796031e-01 -2.88100958e-01 -9.03109431e-01  5.05412340e-01\n","    -2.29584381e-01  8.81282806e-01  1.62935090e+00  6.68148637e-01\n","     1.38726163e+00  1.37816763e+00  5.73498487e-01 -1.76900423e+00\n","     5.86095035e-01 -2.48563290e+00  9.43803370e-01 -9.23078775e-01]\n","   [ 7.28776217e-01  6.11684382e-01 -8.76088321e-01 -2.35505253e-01\n","     5.72645701e-02 -8.46090689e-02  1.09683156e+00 -3.53859104e-02\n","    -1.28413618e+00 -5.97052932e-01  1.14283180e+00 -4.17192310e-01\n","    -2.70076931e-01 -1.86461449e+00  3.05952460e-01 -4.18166190e-01]]\n","\n","  [[ 1.44522059e+00  1.64912164e+00 -7.56208837e-01  5.34757972e-01\n","    -2.73642004e-01 -8.95813257e-02 -1.04655921e+00 -5.23219824e-01\n","    -1.22036362e+00  1.34282291e+00 -2.11059317e-01 -1.98725924e-01\n","    -4.71270829e-02 -9.41687942e-01  5.07678270e-01  9.22263920e-01]\n","   [-1.02429318e+00 -1.09060490e+00 -5.73331773e-01 -2.11118296e-01\n","    -3.39075446e-01 -5.28774381e-01 -2.20028353e+00  2.52022833e-01\n","     2.83047974e-01 -1.32400799e+00 -1.34488463e-03  2.74970587e-02\n","     9.03180122e-01  4.39890772e-01 -1.04267824e+00  3.93869549e-01]\n","   [ 1.03338212e-02 -8.50608766e-01  1.10901201e+00 -2.46981367e-01\n","    -6.83523893e-01  5.87789357e-01 -9.90404487e-02 -3.85973677e-02\n","     1.02486563e+00  1.41465291e-01 -7.02347100e-01  5.33523560e-01\n","    -7.11001575e-01  1.38313800e-01  4.06129807e-01  3.89108121e-01]]\n","\n","  [[ 1.51913390e-01  4.76801693e-01 -1.01600420e+00 -8.82864177e-01\n","     1.72873759e+00  6.46490276e-01 -5.12281835e-01 -1.61252022e-01\n","    -9.83342111e-01  1.15742540e+00  1.37689519e+00 -1.10410190e+00\n","    -2.04914427e+00 -1.95826247e-01  1.91862571e+00 -1.90753222e-01]\n","   [-1.02089787e+00  1.98495656e-01  1.28144848e+00 -7.18914986e-01\n","     7.41300404e-01  1.10979998e+00  1.65606886e-01  2.62002647e-01\n","    -1.63593483e+00  1.71451676e+00  5.42055726e-01 -2.21169099e-01\n","     1.66384590e+00  8.35620105e-01 -1.07018888e+00  5.10219753e-01]\n","   [ 1.67486739e+00  6.18863344e-01 -1.04402602e+00  7.32049704e-01\n","    -1.30217361e+00 -4.35524791e-01 -2.62460202e-01  1.00086892e+00\n","    -2.18341812e-01 -4.92205441e-01  2.60721356e-01 -5.64928949e-01\n","     1.16850710e+00 -8.29534948e-01  2.40424323e+00 -1.90497309e-01]]]\n","\n","\n"," [[[-1.17544866e+00 -5.48851155e-02  4.22603786e-02 -9.39402223e-01\n","     1.19028628e+00  6.26563072e-01 -6.09275997e-01 -5.55905521e-01\n","     3.07423204e-01 -1.43776655e+00 -1.11876059e+00 -6.92955732e-01\n","    -3.66843373e-01 -5.77220023e-01 -1.76732528e+00 -5.36239088e-01]\n","   [-1.52670097e+00  2.10521555e+00  2.44599628e+00  9.18931007e-01\n","    -2.62253940e-01 -5.68299472e-01 -5.81631541e-01  6.20614350e-01\n","     5.50635517e-01  2.74599284e-01  4.67253476e-01 -1.10525206e-01\n","     1.07619023e+00  1.48619711e+00 -4.15436402e-02 -4.93129492e-01]\n","   [-1.15018487e+00  1.39044428e+00  1.21777141e+00 -1.12656450e+00\n","    -2.26464343e+00 -2.13983357e-01 -1.28403401e+00  8.43076110e-01\n","    -1.39442325e-01  4.79474306e-01  2.97692697e-02  1.01177029e-01\n","    -5.35952747e-01  1.29347610e+00 -2.72579998e-01  1.19361842e+00]]\n","\n","  [[ 1.78025946e-01 -1.24072433e+00 -1.24420023e+00 -7.88696587e-01\n","    -1.24152648e+00  8.79596531e-01 -8.59584987e-01  5.26383221e-01\n","    -1.23334563e+00  2.34329653e+00  1.34136105e+00 -1.19314837e+00\n","    -5.59304714e-01 -1.01250517e+00 -1.15559116e-01  2.71430075e-01]\n","   [ 8.51941943e-01 -2.61823058e+00  2.40780830e-01 -6.89047337e-01\n","    -4.41132605e-01 -6.86932445e-01  3.18634421e-01  1.11395156e+00\n","    -8.35723430e-03  1.87544599e-01  3.90780754e-02 -1.57513559e+00\n","    -1.47010815e+00  5.99129558e-01  7.97059953e-01  3.26195434e-02]\n","   [-2.98961550e-01  1.48740637e+00 -3.82947892e-01  1.14175880e+00\n","    -3.67538333e-01 -1.28722620e+00  8.72689664e-01  6.97225928e-01\n","    -1.00847840e+00  1.60017836e+00  7.27395475e-01  3.37876618e-01\n","     4.25743312e-01 -4.18708235e-01  2.54765630e-01 -1.64596891e+00]]\n","\n","  [[ 1.17007220e+00 -2.85560250e-01  8.03896785e-02  1.06136692e+00\n","     6.25034332e-01  6.90889597e-01 -3.52775425e-01  1.00555420e+00\n","    -7.11774349e-01  3.81468028e-01 -9.88594592e-01  5.26896000e-01\n","     1.23935473e+00  2.66446471e+00  1.42381716e+00  1.09026179e-01]\n","   [-3.75006080e-01 -8.10260057e-01 -1.37713313e+00 -1.14200628e+00\n","    -2.31188595e-01 -7.40914106e-01  1.16216815e+00  1.53247893e+00\n","     2.25940440e-03 -1.72462237e+00  2.02629972e+00 -2.57482409e-01\n","    -5.32267630e-01 -6.84871256e-01 -1.75036514e+00 -2.82968432e-02]\n","   [ 1.96031988e+00 -1.53299510e+00  6.99366212e-01 -1.46556711e+00\n","     5.78366339e-01 -8.32008839e-01  1.45459580e+00  1.24557531e+00\n","    -2.77160287e-01 -2.00246349e-01  1.55093566e-01  2.98580498e-01\n","    -1.18777287e+00 -8.52076173e-01 -2.16620350e+00  2.31011868e+00]]\n","\n","  [[-2.05782633e-02 -1.59692302e-01 -4.24988836e-01  9.43649888e-01\n","     1.41065109e+00 -2.37810683e+00 -6.81668758e-01  1.50250053e+00\n","     1.03178585e+00  6.43335998e-01 -1.02595067e+00 -1.47315252e+00\n","     6.29622936e-01 -1.09131232e-01  7.66469061e-01  1.53373218e+00]\n","   [-6.64848804e-01  4.23595160e-01  1.78632402e+00 -1.13268232e+00\n","    -6.82671487e-01  1.91094875e+00 -1.32285452e+00  2.75485635e-01\n","    -2.42395353e+00 -9.20824051e-01  3.77879590e-01 -5.66891693e-02\n","     7.68425688e-02 -1.57620870e-02 -2.06549072e+00  1.88852146e-01]\n","   [ 4.37522344e-02 -6.57358587e-01 -4.76909764e-02  6.15530252e-01\n","     6.57291710e-01  8.83487642e-01 -2.92197406e-01  5.73906779e-01\n","     1.74268353e+00  3.74617696e-01 -3.24656463e+00  1.78566039e-01\n","     1.55177820e+00  3.41335297e-01  2.62867391e-01  5.86450934e-01]]\n","\n","  [[-6.82086170e-01  1.48331121e-01  1.01442468e+00  1.09393620e+00\n","     1.71937191e+00 -3.05081576e-01  8.12685311e-01  6.35167778e-01\n","     6.74835026e-01 -1.08194244e+00 -4.07744199e-01 -5.79516403e-02\n","    -6.18456125e-01  8.59635890e-01  1.58947989e-01 -1.56896442e-01]\n","   [-1.05287802e+00  9.88370299e-01 -9.71435189e-01  8.59063506e-01\n","    -1.86965708e-02 -9.40730609e-03 -1.60416067e+00  5.51001310e-01\n","     7.77331412e-01 -9.69766319e-01  2.54476130e-01  3.56859773e-01\n","     1.22686468e-01 -9.85618770e-01  6.24602139e-01 -6.67222321e-01]\n","   [-5.12909532e-01 -8.08211267e-01  1.81276202e+00  2.86694139e-01\n","    -6.36618197e-01 -9.69957590e-01 -3.89256239e-01 -1.40181601e+00\n","     1.18410575e+00  6.15966260e-01  1.01148725e+00  8.69927526e-01\n","     9.67692792e-01 -8.70647803e-02 -1.00636196e+00 -1.79708213e-01]]]]\n","Mask:\n","[[[[False  True  True  True  True]\n","   [False False  True  True  True]\n","   [False False False  True  True]\n","   [False False False False  True]\n","   [False False False False False]]\n","\n","  [[False  True  True  True  True]\n","   [False False  True  True  True]\n","   [False False False  True  True]\n","   [False False False False  True]\n","   [False False False False False]]\n","\n","  [[False  True  True  True  True]\n","   [False False  True  True  True]\n","   [False False False  True  True]\n","   [False False False False  True]\n","   [False False False False False]]\n","\n","  [[False  True  True  True  True]\n","   [False False  True  True  True]\n","   [False False False  True  True]\n","   [False False False False  True]\n","   [False False False False False]]]\n","\n","\n"," [[[False  True  True  True  True]\n","   [False False  True  True  True]\n","   [False False False  True  True]\n","   [False False False False  True]\n","   [False False False False False]]\n","\n","  [[False  True  True  True  True]\n","   [False False  True  True  True]\n","   [False False False  True  True]\n","   [False False False False  True]\n","   [False False False False False]]\n","\n","  [[False  True  True  True  True]\n","   [False False  True  True  True]\n","   [False False False  True  True]\n","   [False False False False  True]\n","   [False False False False False]]\n","\n","  [[False  True  True  True  True]\n","   [False False  True  True  True]\n","   [False False False  True  True]\n","   [False False False False  True]\n","   [False False False False False]]]]\n"]},{"ename":"ValueError","evalue":"Incompatible shapes for broadcasting: shapes=[(2, 4, 5, 5), (), (2, 4, 15, 15)]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/util.py:287\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 287\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/util.py:280\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 280\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:155\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 155\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(2, 4, 5, 5), (), (2, 4, 15, 15)]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Initialize and apply the model\u001b[39;00m\n\u001b[1;32m     82\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(n_heads\u001b[38;5;241m=\u001b[39mnum_heads, embedding_dim\u001b[38;5;241m=\u001b[39membedding_dim)\n\u001b[0;32m---> 83\u001b[0m variables \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(variables, input_data, mask)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n","Cell \u001b[0;32mIn[27], line 28\u001b[0m, in \u001b[0;36mMyModel.__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     25\u001b[0m attn_logits \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbthd,bThd->bhtT\u001b[39m\u001b[38;5;124m\"\u001b[39m, q, k)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Apply mask: broadcast mask to match attention logits shape\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m attn_logits \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Compute attention weights\u001b[39;00m\n\u001b[1;32m     31\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39msoftmax(attn_logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:1141\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(acondition, if_true, if_false, size, fill_value, condition, x, y)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize and fill_value arguments cannot be used in three-term where function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_where\u001b[49m\u001b[43m(\u001b[49m\u001b[43macondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_false\u001b[49m\u001b[43m)\u001b[49m\n","    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:448\u001b[0m, in \u001b[0;36m_where\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    446\u001b[0m   condition \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mne(condition, lax\u001b[38;5;241m.\u001b[39m_zero(condition))\n\u001b[1;32m    447\u001b[0m x, y \u001b[38;5;241m=\u001b[39m promote_dtypes(x, y)\n\u001b[0;32m--> 448\u001b[0m condition_arr, x_arr, y_arr \u001b[38;5;241m=\u001b[39m \u001b[43m_broadcast_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m   is_always_empty \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mis_empty_shape(x_arr\u001b[38;5;241m.\u001b[39mshape)\n","    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:407\u001b[0m, in \u001b[0;36m_broadcast_arrays\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shapes \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(core\u001b[38;5;241m.\u001b[39mdefinitely_equal_shape(shapes[\u001b[38;5;241m0\u001b[39m], s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m shapes):\n\u001b[1;32m    406\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [lax\u001b[38;5;241m.\u001b[39masarray(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 407\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, result_shape) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    169\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(2, 4, 5, 5), (), (2, 4, 15, 15)]"]}],"source":["import jax\n","import jax.numpy as jnp\n","from jax import random\n","from flax import linen as nn\n","\n","\n","class MyModel(nn.Module):\n","    n_heads: int\n","    embedding_dim: int\n","\n","    @nn.compact\n","    def __call__(self, x, mask):\n","        batch_size, seq_len, n_columns, embedding_dim = x.shape\n","\n","        # Merge seq_len and n_columns dimensions\n","        x_reshaped = x.reshape(batch_size, seq_len * n_columns, embedding_dim)\n","\n","        # Project the inputs to query, key, and value\n","        qkv_features = self.embedding_dim // self.n_heads\n","        q = nn.DenseGeneral(features=(self.n_heads, qkv_features))(x_reshaped)\n","        k = nn.DenseGeneral(features=(self.n_heads, qkv_features))(x_reshaped)\n","        v = nn.DenseGeneral(features=(self.n_heads, qkv_features))(x_reshaped)\n","\n","        # Compute attention logits\n","        attn_logits = jnp.einsum(\"bthd,bThd->bhtT\", q, k)\n","\n","        # Apply mask: broadcast mask to match attention logits shape\n","        attn_logits = jnp.where(mask, -jnp.inf, attn_logits)\n","\n","        # Compute attention weights\n","        attn_weights = nn.softmax(attn_logits, axis=-1)\n","\n","        # Compute the attention output\n","        attn_output = jnp.einsum(\"bhtT,bThd->bthd\", attn_weights, v)\n","\n","        # Combine heads and reshape back to original dimensions\n","        attn_output = attn_output.reshape(\n","            batch_size, seq_len, n_columns, self.embedding_dim\n","        )\n","\n","        return attn_output\n","\n","\n","def create_future_mask(seq_len):\n","    # Create a basic mask for future sequences\n","    mask = jnp.triu(jnp.ones((seq_len, seq_len)), k=1).astype(bool)\n","    return mask\n","\n","\n","def create_custom_mask(batch_size, seq_len, n_columns, num_heads):\n","    # Create the future sequence mask\n","    future_mask = create_future_mask(seq_len)\n","\n","    # Expand the mask to match the required dimensions for attention\n","    mask = jnp.expand_dims(future_mask, axis=0)  # Shape: (1, seq_len, seq_len)\n","    mask = jnp.expand_dims(mask, axis=1)  # Shape: (1, 1, seq_len, seq_len)\n","    mask = jnp.tile(\n","        mask, (batch_size, num_heads, 1, 1)\n","    )  # Shape: (batch_size, num_heads, seq_len, seq_len)\n","\n","    return mask\n","\n","\n","# Generate random input data\n","key = random.PRNGKey(0)\n","batch_size = 2\n","seq_len = 5\n","n_columns = 3\n","embedding_dim = 16\n","num_heads = 4\n","\n","input_data = random.normal(key, (batch_size, seq_len, n_columns, embedding_dim))\n","print(\"Input Data:\")\n","print(input_data)\n","\n","# Create the mask\n","mask = create_custom_mask(batch_size, seq_len, n_columns, num_heads)\n","print(\"Mask:\")\n","print(mask)\n","\n","# Initialize and apply the model\n","model = MyModel(n_heads=num_heads, embedding_dim=embedding_dim)\n","variables = model.init(key, input_data, mask)\n","output = model.apply(variables, input_data, mask)\n","\n","print(\"Output:\")\n","print(output)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Incompatible shapes for broadcasting: shapes=[(2, 4, 5, 5), (2, 5, 4, 3, 3), ()]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/util.py:287\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 287\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/util.py:280\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 280\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:155\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 155\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(2, 4, 5, 5), (2, 5, 4, 3, 3), ()]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 58\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# print(\"Mask:\")\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# print(mask)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Initialize and apply the model\u001b[39;00m\n\u001b[1;32m     57\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(n_heads\u001b[38;5;241m=\u001b[39mnum_heads)\n\u001b[0;32m---> 58\u001b[0m variables \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(variables, input_data, mask)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n","Cell \u001b[0;32mIn[31], line 13\u001b[0m, in \u001b[0;36mMyModel.__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Apply multi-head attention with the custom mask\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiHeadAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqkv_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn\n","    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:546\u001b[0m, in \u001b[0;36mMultiHeadDotProductAttention.__call__\u001b[0;34m(self, inputs_q, inputs_k, inputs_v, inputs_kv, mask, deterministic, dropout_rng, sow_weights)\u001b[0m\n\u001b[1;32m    532\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_fn(\n\u001b[1;32m    533\u001b[0m     query,\n\u001b[1;32m    534\u001b[0m     key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    544\u001b[0m   )  \u001b[38;5;66;03m# pytype: disable=wrong-keyword-args\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 546\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm_deterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# back to the original inputs dimensions\u001b[39;00m\n\u001b[1;32m    559\u001b[0m out \u001b[38;5;241m=\u001b[39m DenseGeneral(\n\u001b[1;32m    560\u001b[0m   features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    561\u001b[0m   axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    570\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    571\u001b[0m )(x)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:198\u001b[0m, in \u001b[0;36mdot_product_attention\u001b[0;34m(query, key, value, bias, mask, broadcast_dropout, dropout_rng, dropout_rate, deterministic, dtype, precision, module)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m key\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk, v lengths must match.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# compute attention weights\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mdot_product_attention_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m  \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m  \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m  \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# return weighted sum over values for each query position\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[1;32m    214\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...hqk,...khd->...qhd\u001b[39m\u001b[38;5;124m'\u001b[39m, attn_weights, value, precision\u001b[38;5;241m=\u001b[39mprecision\n\u001b[1;32m    215\u001b[0m )\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:112\u001b[0m, in \u001b[0;36mdot_product_attention_weights\u001b[0;34m(query, key, bias, mask, broadcast_dropout, dropout_rng, dropout_rate, deterministic, dtype, precision, module)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m   big_neg \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mfinfo(dtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[0;32m--> 112\u001b[0m   attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbig_neg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# normalize the attention weights\u001b[39;00m\n\u001b[1;32m    115\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(attn_weights)\u001b[38;5;241m.\u001b[39mastype(dtype)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:1141\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(acondition, if_true, if_false, size, fill_value, condition, x, y)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize and fill_value arguments cannot be used in three-term where function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_where\u001b[49m\u001b[43m(\u001b[49m\u001b[43macondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_false\u001b[49m\u001b[43m)\u001b[49m\n","    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:448\u001b[0m, in \u001b[0;36m_where\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    446\u001b[0m   condition \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mne(condition, lax\u001b[38;5;241m.\u001b[39m_zero(condition))\n\u001b[1;32m    447\u001b[0m x, y \u001b[38;5;241m=\u001b[39m promote_dtypes(x, y)\n\u001b[0;32m--> 448\u001b[0m condition_arr, x_arr, y_arr \u001b[38;5;241m=\u001b[39m \u001b[43m_broadcast_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m   is_always_empty \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mis_empty_shape(x_arr\u001b[38;5;241m.\u001b[39mshape)\n","    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:407\u001b[0m, in \u001b[0;36m_broadcast_arrays\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shapes \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(core\u001b[38;5;241m.\u001b[39mdefinitely_equal_shape(shapes[\u001b[38;5;241m0\u001b[39m], s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m shapes):\n\u001b[1;32m    406\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [lax\u001b[38;5;241m.\u001b[39masarray(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 407\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, result_shape) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    169\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(2, 4, 5, 5), (2, 5, 4, 3, 3), ()]"]}],"source":["import jax\n","import jax.numpy as jnp\n","from jax import random\n","from flax import linen as nn\n","\n","\n","class MyModel(nn.Module):\n","    n_heads: int\n","\n","    @nn.compact\n","    def __call__(self, x, mask):\n","        # Apply multi-head attention with the custom mask\n","        attn = nn.MultiHeadAttention(\n","            num_heads=self.n_heads, qkv_features=16, use_bias=False\n","        )(x, mask=mask)\n","        return attn\n","\n","\n","def create_future_mask(seq_len):\n","    # Create a basic mask for future sequences\n","    mask = jnp.triu(jnp.ones((seq_len, seq_len)), k=1).astype(bool)\n","    return mask\n","\n","\n","def create_custom_mask(batch_size, seq_len, num_heads):\n","    # Create the future sequence mask\n","    future_mask = create_future_mask(seq_len)\n","\n","    # Expand the mask to match the required dimensions for attention\n","    mask = jnp.expand_dims(future_mask, axis=0)  # Shape: (1, seq_len, seq_len)\n","    mask = jnp.expand_dims(mask, axis=1)  # Shape: (1, 1, seq_len, seq_len)\n","    mask = jnp.tile(\n","        mask, (batch_size, num_heads, 1, 1)\n","    )  # Shape: (batch_size, num_heads, seq_len, seq_len)\n","\n","    return mask\n","\n","\n","# Generate random input data\n","key = random.PRNGKey(0)\n","batch_size = 2\n","seq_len = 5\n","n_columns = 3\n","embedding_dim = 16\n","num_heads = 4\n","\n","input_data = random.normal(key, (batch_size, seq_len, n_columns, embedding_dim))\n","# print(\"Input Data:\")\n","# print(input_data)\n","\n","# Create the mask\n","mask = create_custom_mask(batch_size, seq_len, num_heads)\n","# print(\"Mask:\")\n","# print(mask)\n","\n","# Initialize and apply the model\n","model = MyModel(n_heads=num_heads)\n","variables = model.init(key, input_data, mask)\n","output = model.apply(variables, input_data, mask)\n","\n","print(\"Output:\")\n","print(output)"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(2, 15, 1, 16, 16)\n"]},{"ename":"TypeError","evalue":"cannot reshape array of shape (2, 15, 1, 16, 16) (size 7680) into shape (2, 5, 3, 5, 3) (size 450)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Initialize and apply the model\u001b[39;00m\n\u001b[1;32m     45\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(n_heads\u001b[38;5;241m=\u001b[39mnum_heads)\n\u001b[0;32m---> 46\u001b[0m variables \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(variables, input_data, mask)\n","    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n","Cell \u001b[0;32mIn[46], line 20\u001b[0m, in \u001b[0;36mMyModel.__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# ValueError: Incompatible shapes for broadcasting: shapes=[(2, 15, 1, 16, 16),\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#                                                           (2, 4, 15, 15), ()]\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Apply multi-head attention with the custom mask\u001b[39;00m\n\u001b[1;32m     22\u001b[0m attn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMultiHeadAttention(\n\u001b[1;32m     23\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads, qkv_features\u001b[38;5;241m=\u001b[39membedding_dim, use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     24\u001b[0m )(x, mask\u001b[38;5;241m=\u001b[39mmask)\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:136\u001b[0m, in \u001b[0;36m_compute_newshape\u001b[0;34m(a, newshape)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(d, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mshape(a), \u001b[38;5;241m*\u001b[39mnewshape)) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    135\u001b[0m       np\u001b[38;5;241m.\u001b[39msize(a) \u001b[38;5;241m!=\u001b[39m math\u001b[38;5;241m.\u001b[39mprod(newshape)):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot reshape array of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mshape(a)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39msize(a)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minto shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morig_newshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmath\u001b[38;5;241m.\u001b[39mprod(newshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;241m-\u001b[39mcore\u001b[38;5;241m.\u001b[39mdivide_shape_sizes(np\u001b[38;5;241m.\u001b[39mshape(a), newshape)\n\u001b[1;32m    139\u001b[0m              \u001b[38;5;28;01mif\u001b[39;00m core\u001b[38;5;241m.\u001b[39mdefinitely_equal(d, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m newshape)\n","\u001b[0;31mTypeError\u001b[0m: cannot reshape array of shape (2, 15, 1, 16, 16) (size 7680) into shape (2, 5, 3, 5, 3) (size 450)"]}],"source":["import jax\n","import jax.numpy as jnp\n","from jax import random\n","from flax import linen as nn\n","\n","\n","class MyModel(nn.Module):\n","    n_heads: int\n","\n","    @nn.compact\n","    def __call__(self, x, mask):\n","        # Reshape input to combine seq_len and n_columns\n","        batch_size, seq_len, n_columns, embedding_dim = x.shape\n","        x = x.reshape(batch_size, seq_len * n_columns, embedding_dim)\n","\n","        mask = nn.make_causal_mask(x)\n","        print(mask.shape)  #                                        (2, 15, 1, 16, 16)\n","        # ValueError: Incompatible shapes for broadcasting: shapes=[(2, 15, 1, 16, 16),\n","        #                                                           (2, 4, 15, 15), ()]\n","        mask = mask.reshape(batch_size, seq_len, n_columns, seq_len, n_columns)\n","        # Apply multi-head attention with the custom mask\n","        attn = nn.MultiHeadAttention(\n","            num_heads=self.n_heads, qkv_features=embedding_dim, use_bias=False\n","        )(x, mask=mask)\n","        print(attn.shape)\n","        # Reshape the output back to the original shape\n","        attn = attn.reshape(batch_size, seq_len, n_columns, embedding_dim)\n","        return attn\n","\n","\n","# Generate random input data\n","key = random.PRNGKey(0)\n","batch_size = 2\n","seq_len = 5\n","n_columns = 3\n","embedding_dim = 16\n","num_heads = 4\n","\n","input_data = random.normal(key, (batch_size, seq_len, n_columns, embedding_dim))\n","\n","# Create the mask\n","mask = create_custom_mask(batch_size, seq_len, num_heads, n_columns)\n","\n","# Initialize and apply the model\n","model = MyModel(n_heads=num_heads)\n","variables = model.init(key, input_data, mask)\n","output = model.apply(variables, input_data, mask)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["(2, 5, 3, 1, 16, 16)"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["nn.make_causal_mask(jnp.ones((2, 5, 3, 16))).shape"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"\"Decoder\" object has no attribute \"token_embedding\". If \"token_embedding\" is defined in '.setup()', remember these fields are only accessible from inside 'init' or 'apply'.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[47], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m    107\u001b[0m deterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/module.py:692\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 692\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_wrapped_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/module.py:1224\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1223\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m-> 1224\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mrun_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1226\u001b[0m   y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","Cell \u001b[0;32mIn[47], line 69\u001b[0m, in \u001b[0;36mDecoder.__call__\u001b[0;34m(self, x, deterministic)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# Embedding and positional encoding\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     seq_len \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding\u001b[49m(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(jnp\u001b[38;5;241m.\u001b[39marange(seq_len))\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Create the causal mask\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mmake_causal_mask(x)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/module.py:1317\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m   msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m If \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is defined in \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m.setup()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m, remember these fields \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mare only accessible from inside \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1316\u001b[0m   )\n\u001b[0;32m-> 1317\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(msg)\n","\u001b[0;31mAttributeError\u001b[0m: \"Decoder\" object has no attribute \"token_embedding\". If \"token_embedding\" is defined in '.setup()', remember these fields are only accessible from inside 'init' or 'apply'."]}],"source":["import jax.numpy as jnp\n","import flax.linen as nn\n","from flax.linen import partitioning\n","from typing import Any\n","\n","\n","class DecoderBlock(nn.Module):\n","    num_heads: int\n","    qkv_dim: int\n","    mlp_dim: int\n","    dropout_rate: float\n","\n","    def setup(self):\n","        self.self_attention = nn.MultiHeadDotProductAttention(\n","            num_heads=self.num_heads,\n","            qkv_features=self.qkv_dim,\n","            kernel_init=nn.initializers.xavier_uniform(),\n","            bias_init=nn.initializers.normal(stddev=1e-6),\n","            dropout_rate=self.dropout_rate,\n","        )\n","        self.mlp = nn.Sequential(\n","            [nn.Dense(self.mlp_dim), nn.relu, nn.Dense(self.qkv_dim)]\n","        )\n","        self.layer_norm1 = nn.LayerNorm()\n","        self.layer_norm2 = nn.LayerNorm()\n","        self.dropout = nn.Dropout(rate=self.dropout_rate)\n","\n","    def __call__(self, x, causal_mask, deterministic):\n","        # Self-attention block\n","        x = self.layer_norm1(x)\n","        attn_out = self.self_attention(\n","            query=x, key=x, value=x, mask=causal_mask, deterministic=deterministic\n","        )\n","        x = x + self.dropout(attn_out, deterministic=deterministic)\n","\n","        # Feed-forward block\n","        x = self.layer_norm2(x)\n","        mlp_out = self.mlp(x)\n","        x = x + self.dropout(mlp_out, deterministic=deterministic)\n","\n","        return x\n","\n","\n","class Decoder(nn.Module):\n","    vocab_size: int\n","    num_layers: int\n","    num_heads: int\n","    qkv_dim: int\n","    mlp_dim: int\n","    max_len: int\n","    dropout_rate: float\n","\n","    def setup(self):\n","        self.token_embedding = nn.Embed(self.vocab_size, self.qkv_dim)\n","        self.position_embedding = nn.Embed(self.max_len, self.qkv_dim)\n","        self.decoder_blocks = [\n","            DecoderBlock(\n","                num_heads=self.num_heads,\n","                qkv_dim=self.qkv_dim,\n","                mlp_dim=self.mlp_dim,\n","                dropout_rate=self.dropout_rate,\n","            )\n","            for _ in range(self.num_layers)\n","        ]\n","\n","    def __call__(self, x, deterministic=True):\n","        # Embedding and positional encoding\n","        seq_len = x.shape[1]\n","        x = self.token_embedding(x) + self.position_embedding(jnp.arange(seq_len))\n","\n","        # Create the causal mask\n","        causal_mask = nn.make_causal_mask(x)\n","\n","        # Apply decoder blocks\n","        for block in self.decoder_blocks:\n","            x = block(x, causal_mask, deterministic)\n","\n","        return x\n","\n","\n","# Example usage\n","vocab_size = 32000\n","num_layers = 6\n","num_heads = 8\n","qkv_dim = 512\n","mlp_dim = 2048\n","max_len = 512\n","dropout_rate = 0.1\n","\n","decoder = Decoder(\n","    vocab_size=vocab_size,\n","    num_layers=num_layers,\n","    num_heads=num_heads,\n","    qkv_dim=qkv_dim,\n","    mlp_dim=mlp_dim,\n","    max_len=max_len,\n","    dropout_rate=dropout_rate,\n",")\n","\n","# Create a random input sequence of token IDs\n","import jax.random as random\n","\n","key = random.PRNGKey(0)\n","x = random.randint(key, (1, max_len), 0, vocab_size)\n","\n","# Forward pass\n","deterministic = True\n","output = decoder(x, deterministic=deterministic)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Incompatible shapes for broadcasting: shapes=[(1, 10, 1, 64, 64), (1, 8, 10, 10), ()]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/util.py:287\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 287\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/util.py:280\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 280\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:155\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 155\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(1, 10, 1, 64, 64), (1, 8, 10, 10), ()]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[48], line 59\u001b[0m\n\u001b[1;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\n\u001b[1;32m     55\u001b[0m     key, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     56\u001b[0m )  \u001b[38;5;66;03m# Example input: batch size 1, sequence length 10, embedding size 64\u001b[39;00m\n\u001b[1;32m     58\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleDecoder(num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, mlp_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(params, x)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n","    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n","Cell \u001b[0;32mIn[48], line 47\u001b[0m, in \u001b[0;36mSimpleDecoder.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m mask \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mmake_causal_mask(x)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m---> 47\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mDecoderBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n","Cell \u001b[0;32mIn[48], line 17\u001b[0m, in \u001b[0;36mDecoderBlock.__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Self-attention block\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm()(x)\n\u001b[0;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqkv_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxavier_uniform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.1\u001b[39m)(x, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m     residual \u001b[38;5;241m=\u001b[39m x\n","    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:701\u001b[0m, in \u001b[0;36mSelfAttention.__call__\u001b[0;34m(self, inputs_q, mask, deterministic, dropout_rng, sow_weights)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Applies multi-head dot product self-attention on the input data.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \n\u001b[1;32m    680\u001b[0m \u001b[38;5;124;03mProjects the inputs into multi-headed query, key, and value vectors,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;124;03m  output of shape ``[batch_sizes..., length, features]``.\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    694\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    695\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSelfAttention will be deprecated soon. Use \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    696\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`MultiHeadDotProductAttention.__call__(inputs_q)` instead. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    699\u001b[0m   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    700\u001b[0m )\n\u001b[0;32m--> 701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m  \u001b[49m\u001b[43minputs_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msow_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msow_weights\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:546\u001b[0m, in \u001b[0;36mMultiHeadDotProductAttention.__call__\u001b[0;34m(self, inputs_q, inputs_k, inputs_v, inputs_kv, mask, deterministic, dropout_rng, sow_weights)\u001b[0m\n\u001b[1;32m    532\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_fn(\n\u001b[1;32m    533\u001b[0m     query,\n\u001b[1;32m    534\u001b[0m     key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    544\u001b[0m   )  \u001b[38;5;66;03m# pytype: disable=wrong-keyword-args\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 546\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm_deterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# back to the original inputs dimensions\u001b[39;00m\n\u001b[1;32m    559\u001b[0m out \u001b[38;5;241m=\u001b[39m DenseGeneral(\n\u001b[1;32m    560\u001b[0m   features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    561\u001b[0m   axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    570\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    571\u001b[0m )(x)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:198\u001b[0m, in \u001b[0;36mdot_product_attention\u001b[0;34m(query, key, value, bias, mask, broadcast_dropout, dropout_rng, dropout_rate, deterministic, dtype, precision, module)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m key\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk, v lengths must match.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# compute attention weights\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mdot_product_attention_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m  \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m  \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m  \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# return weighted sum over values for each query position\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[1;32m    214\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...hqk,...khd->...qhd\u001b[39m\u001b[38;5;124m'\u001b[39m, attn_weights, value, precision\u001b[38;5;241m=\u001b[39mprecision\n\u001b[1;32m    215\u001b[0m )\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:112\u001b[0m, in \u001b[0;36mdot_product_attention_weights\u001b[0;34m(query, key, bias, mask, broadcast_dropout, dropout_rng, dropout_rate, deterministic, dtype, precision, module)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m   big_neg \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mfinfo(dtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[0;32m--> 112\u001b[0m   attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbig_neg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# normalize the attention weights\u001b[39;00m\n\u001b[1;32m    115\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(attn_weights)\u001b[38;5;241m.\u001b[39mastype(dtype)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:1141\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(acondition, if_true, if_false, size, fill_value, condition, x, y)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize and fill_value arguments cannot be used in three-term where function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_where\u001b[49m\u001b[43m(\u001b[49m\u001b[43macondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_false\u001b[49m\u001b[43m)\u001b[49m\n","    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:448\u001b[0m, in \u001b[0;36m_where\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    446\u001b[0m   condition \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mne(condition, lax\u001b[38;5;241m.\u001b[39m_zero(condition))\n\u001b[1;32m    447\u001b[0m x, y \u001b[38;5;241m=\u001b[39m promote_dtypes(x, y)\n\u001b[0;32m--> 448\u001b[0m condition_arr, x_arr, y_arr \u001b[38;5;241m=\u001b[39m \u001b[43m_broadcast_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m   is_always_empty \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mis_empty_shape(x_arr\u001b[38;5;241m.\u001b[39mshape)\n","    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:407\u001b[0m, in \u001b[0;36m_broadcast_arrays\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shapes \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(core\u001b[38;5;241m.\u001b[39mdefinitely_equal_shape(shapes[\u001b[38;5;241m0\u001b[39m], s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m shapes):\n\u001b[1;32m    406\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [lax\u001b[38;5;241m.\u001b[39masarray(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 407\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, result_shape) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    169\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(1, 10, 1, 64, 64), (1, 8, 10, 10), ()]"]}],"source":["import jax\n","import jax.numpy as jnp\n","from flax import linen as nn\n","from flax.training import train_state\n","from typing import Any\n","\n","\n","class DecoderBlock(nn.Module):\n","    num_heads: int\n","    embed_dim: int\n","    mlp_dim: int\n","\n","    @nn.compact\n","    def __call__(self, x, mask=None):\n","        # Self-attention block\n","        x = nn.LayerNorm()(x)\n","        x = nn.SelfAttention(\n","            num_heads=self.num_heads,\n","            qkv_features=self.embed_dim,\n","            kernel_init=nn.initializers.xavier_uniform(),\n","        )(x, mask=mask)\n","        x = nn.Dropout(0.1)(x, deterministic=True)\n","        residual = x\n","\n","        # Feed-forward block\n","        x = nn.LayerNorm()(x)\n","        x = nn.Dense(self.mlp_dim, kernel_init=nn.initializers.xavier_uniform())(x)\n","        x = nn.relu(x)\n","        x = nn.Dense(self.embed_dim, kernel_init=nn.initializers.xavier_uniform())(x)\n","        x = nn.Dropout(0.1)(x, deterministic=True)\n","\n","        return residual + x\n","\n","\n","class SimpleDecoder(nn.Module):\n","    num_layers: int\n","    num_heads: int\n","    embed_dim: int\n","    mlp_dim: int\n","\n","    @nn.compact\n","    def __call__(self, x):\n","        # Create a causal mask\n","        mask = nn.make_causal_mask(x)\n","\n","        for _ in range(self.num_layers):\n","            x = DecoderBlock(self.num_heads, self.embed_dim, self.mlp_dim)(x, mask)\n","\n","        return x\n","\n","\n","# Example usage\n","key = jax.random.PRNGKey(0)\n","x = jax.random.normal(\n","    key, (1, 10, 64)\n",")  # Example input: batch size 1, sequence length 10, embedding size 64\n","\n","model = SimpleDecoder(num_layers=2, num_heads=8, embed_dim=64, mlp_dim=256)\n","params = model.init(key, x)\n","y = model.apply(params, x)\n","\n","print(y)"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Incompatible shapes for broadcasting: shapes=[(1, 10, 1, 64, 64), (1, 8, 10, 10), ()]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/util.py:287\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 287\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/util.py:280\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 280\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:155\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 155\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(1, 10, 1, 64, 64), (1, 8, 10, 10), ()]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[49], line 59\u001b[0m\n\u001b[1;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\n\u001b[1;32m     55\u001b[0m     key, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     56\u001b[0m )  \u001b[38;5;66;03m# Example input: batch size 1, sequence length 10, embedding size 64\u001b[39;00m\n\u001b[1;32m     58\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleDecoder(num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, mlp_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(params, x)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n","    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n","Cell \u001b[0;32mIn[49], line 47\u001b[0m, in \u001b[0;36mSimpleDecoder.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m mask \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mmake_causal_mask(x)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m---> 47\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mDecoderBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n","Cell \u001b[0;32mIn[49], line 17\u001b[0m, in \u001b[0;36mDecoderBlock.__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Self-attention block\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm()(x)\n\u001b[0;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqkv_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxavier_uniform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.1\u001b[39m)(x, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m     residual \u001b[38;5;241m=\u001b[39m x\n","    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:701\u001b[0m, in \u001b[0;36mSelfAttention.__call__\u001b[0;34m(self, inputs_q, mask, deterministic, dropout_rng, sow_weights)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Applies multi-head dot product self-attention on the input data.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \n\u001b[1;32m    680\u001b[0m \u001b[38;5;124;03mProjects the inputs into multi-headed query, key, and value vectors,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;124;03m  output of shape ``[batch_sizes..., length, features]``.\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    694\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    695\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSelfAttention will be deprecated soon. Use \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    696\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`MultiHeadDotProductAttention.__call__(inputs_q)` instead. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    699\u001b[0m   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    700\u001b[0m )\n\u001b[0;32m--> 701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m  \u001b[49m\u001b[43minputs_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msow_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msow_weights\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:546\u001b[0m, in \u001b[0;36mMultiHeadDotProductAttention.__call__\u001b[0;34m(self, inputs_q, inputs_k, inputs_v, inputs_kv, mask, deterministic, dropout_rng, sow_weights)\u001b[0m\n\u001b[1;32m    532\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_fn(\n\u001b[1;32m    533\u001b[0m     query,\n\u001b[1;32m    534\u001b[0m     key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    544\u001b[0m   )  \u001b[38;5;66;03m# pytype: disable=wrong-keyword-args\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 546\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm_deterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# back to the original inputs dimensions\u001b[39;00m\n\u001b[1;32m    559\u001b[0m out \u001b[38;5;241m=\u001b[39m DenseGeneral(\n\u001b[1;32m    560\u001b[0m   features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    561\u001b[0m   axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    570\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    571\u001b[0m )(x)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:198\u001b[0m, in \u001b[0;36mdot_product_attention\u001b[0;34m(query, key, value, bias, mask, broadcast_dropout, dropout_rng, dropout_rate, deterministic, dtype, precision, module)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m key\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk, v lengths must match.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# compute attention weights\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mdot_product_attention_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m  \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m  \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m  \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# return weighted sum over values for each query position\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[1;32m    214\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...hqk,...khd->...qhd\u001b[39m\u001b[38;5;124m'\u001b[39m, attn_weights, value, precision\u001b[38;5;241m=\u001b[39mprecision\n\u001b[1;32m    215\u001b[0m )\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:112\u001b[0m, in \u001b[0;36mdot_product_attention_weights\u001b[0;34m(query, key, bias, mask, broadcast_dropout, dropout_rng, dropout_rate, deterministic, dtype, precision, module)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m   big_neg \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mfinfo(dtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[0;32m--> 112\u001b[0m   attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbig_neg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# normalize the attention weights\u001b[39;00m\n\u001b[1;32m    115\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(attn_weights)\u001b[38;5;241m.\u001b[39mastype(dtype)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:1141\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(acondition, if_true, if_false, size, fill_value, condition, x, y)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize and fill_value arguments cannot be used in three-term where function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_where\u001b[49m\u001b[43m(\u001b[49m\u001b[43macondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_false\u001b[49m\u001b[43m)\u001b[49m\n","    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:448\u001b[0m, in \u001b[0;36m_where\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    446\u001b[0m   condition \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mne(condition, lax\u001b[38;5;241m.\u001b[39m_zero(condition))\n\u001b[1;32m    447\u001b[0m x, y \u001b[38;5;241m=\u001b[39m promote_dtypes(x, y)\n\u001b[0;32m--> 448\u001b[0m condition_arr, x_arr, y_arr \u001b[38;5;241m=\u001b[39m \u001b[43m_broadcast_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m   is_always_empty \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mis_empty_shape(x_arr\u001b[38;5;241m.\u001b[39mshape)\n","    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:407\u001b[0m, in \u001b[0;36m_broadcast_arrays\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shapes \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(core\u001b[38;5;241m.\u001b[39mdefinitely_equal_shape(shapes[\u001b[38;5;241m0\u001b[39m], s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m shapes):\n\u001b[1;32m    406\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [lax\u001b[38;5;241m.\u001b[39masarray(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 407\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, result_shape) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    169\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(1, 10, 1, 64, 64), (1, 8, 10, 10), ()]"]}],"source":["import jax\n","import jax.numpy as jnp\n","from flax import linen as nn\n","from flax.training import train_state\n","from typing import Any\n","\n","\n","class DecoderBlock(nn.Module):\n","    num_heads: int\n","    embed_dim: int\n","    mlp_dim: int\n","\n","    @nn.compact\n","    def __call__(self, x, mask=None):\n","        # Self-attention block\n","        x = nn.LayerNorm()(x)\n","        x = nn.SelfAttention(\n","            num_heads=self.num_heads,\n","            qkv_features=self.embed_dim,\n","            kernel_init=nn.initializers.xavier_uniform(),\n","        )(x, mask=mask)\n","        x = nn.Dropout(0.1)(x, deterministic=True)\n","        residual = x\n","\n","        # Feed-forward block\n","        x = nn.LayerNorm()(x)\n","        x = nn.Dense(self.mlp_dim, kernel_init=nn.initializers.xavier_uniform())(x)\n","        x = nn.relu(x)\n","        x = nn.Dense(self.embed_dim, kernel_init=nn.initializers.xavier_uniform())(x)\n","        x = nn.Dropout(0.1)(x, deterministic=True)\n","\n","        return residual + x\n","\n","\n","class SimpleDecoder(nn.Module):\n","    num_layers: int\n","    num_heads: int\n","    embed_dim: int\n","    mlp_dim: int\n","\n","    @nn.compact\n","    def __call__(self, x):\n","        # Create a causal mask\n","        mask = nn.make_causal_mask(x)\n","\n","        for _ in range(self.num_layers):\n","            x = DecoderBlock(self.num_heads, self.embed_dim, self.mlp_dim)(x, mask)\n","\n","        return x\n","\n","\n","# Example usage\n","key = jax.random.PRNGKey(0)\n","x = jax.random.normal(\n","    key, (1, 10, 64)\n",")  # Example input: batch size 1, sequence length 10, embedding size 64\n","\n","model = SimpleDecoder(num_layers=2, num_heads=8, embed_dim=64, mlp_dim=256)\n","params = model.init(key, x)\n","y = model.apply(params, x)\n","\n","print(y)"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Exception encountered when calling SimpleDecoder.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: True (of type <class 'bool'>)\u001b[0m\n\nArguments received by SimpleDecoder.call():\n  • x=tf.Tensor(shape=(1, 10, 64), dtype=float32)\n  • training=True\n  • mask=tf.Tensor(shape=(1, 1, 10, 10), dtype=float32)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[50], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m causal_mask[tf\u001b[38;5;241m.\u001b[39mnewaxis, tf\u001b[38;5;241m.\u001b[39mnewaxis, :, :]  \u001b[38;5;66;03m# (1, 1, seq_len, seq_len)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Apply the decoder\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","Cell \u001b[0;32mIn[50], line 52\u001b[0m, in \u001b[0;36mSimpleDecoder.call\u001b[0;34m(self, x, training, mask)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, training, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m---> 52\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling SimpleDecoder.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: True (of type <class 'bool'>)\u001b[0m\n\nArguments received by SimpleDecoder.call():\n  • x=tf.Tensor(shape=(1, 10, 64), dtype=float32)\n  • training=True\n  • mask=tf.Tensor(shape=(1, 1, 10, 10), dtype=float32)"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import (\n","    Layer,\n","    LayerNormalization,\n","    Dense,\n","    Dropout,\n","    MultiHeadAttention,\n",")\n","\n","\n","class DecoderBlock(Layer):\n","    def __init__(self, num_heads, embed_dim, mlp_dim, dropout_rate=0.1):\n","        super(DecoderBlock, self).__init__()\n","        self.num_heads = num_heads\n","        self.embed_dim = embed_dim\n","        self.mlp_dim = mlp_dim\n","        self.dropout_rate = dropout_rate\n","\n","        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = tf.keras.Sequential(\n","            [Dense(mlp_dim, activation=\"relu\"), Dense(embed_dim)]\n","        )\n","        self.layernorm1 = LayerNormalization()\n","        self.layernorm2 = LayerNormalization()\n","        self.dropout1 = Dropout(dropout_rate)\n","        self.dropout2 = Dropout(dropout_rate)\n","\n","    def call(self, x, training, mask=None):\n","        attn_output = self.attention(x, x, attention_mask=mask)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)\n","\n","        ffn_output = self.dense_proj(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","\n","class SimpleDecoder(Layer):\n","    def __init__(self, num_layers, num_heads, embed_dim, mlp_dim, dropout_rate=0.1):\n","        super(SimpleDecoder, self).__init__()\n","        self.num_layers = num_layers\n","        self.embed_dim = embed_dim\n","        self.mlp_dim = mlp_dim\n","\n","        self.dec_layers = [\n","            DecoderBlock(num_heads, embed_dim, mlp_dim, dropout_rate)\n","            for _ in range(num_layers)\n","        ]\n","\n","    def call(self, x, training, mask=None):\n","        for i in range(self.num_layers):\n","            x = self.dec_layers[i](x, training, mask)\n","        return x\n","\n","\n","def create_causal_mask(seq_len):\n","    mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","    return mask\n","\n","\n","# Example usage\n","batch_size = 1\n","seq_len = 10\n","embed_dim = 64\n","\n","# Input tensor\n","x = tf.random.normal((batch_size, seq_len, embed_dim))\n","\n","# Create the model\n","num_layers = 2\n","num_heads = 8\n","mlp_dim = 256\n","dropout_rate = 0.1\n","\n","decoder = SimpleDecoder(num_layers, num_heads, embed_dim, mlp_dim, dropout_rate)\n","\n","# Create a causal mask\n","causal_mask = create_causal_mask(seq_len)\n","causal_mask = causal_mask[tf.newaxis, tf.newaxis, :, :]  # (1, 1, seq_len, seq_len)\n","\n","# Apply the decoder\n","output = decoder(x, training=True, mask=causal_mask)\n","\n","print(output)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP1swsXnq2jqt/Hz4IBTZm2","gpuType":"V100","machine_shape":"hm","mount_file_id":"1zHmvVqlKJh0x9vjCRSkYKkko5buvMMEd","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"01ca2dcb14e647dfb1e68750ca6de39c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a51aa7b827db4f768dce6315dbebf379","IPY_MODEL_634935ff0a6b4d65b72f87359d9f82fc","IPY_MODEL_09b63c98bee543dfb557a007d50c41d1"],"layout":"IPY_MODEL_dc4a8a3b0a8b4ef49dcd6269a1b32e14"}},"0481743e80634f7a8e718fb528950e54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"055681c8159b4b6c8104d4e06279803c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09b63c98bee543dfb557a007d50c41d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fb36a08ca1540228919af722727572c","placeholder":"​","style":"IPY_MODEL_3bd47d84fac442d4bcf49dceeb699d45","value":" 807/807 [04:29&lt;00:00,  3.10it/s]"}},"0e26d28846fb449789510e4748c01c6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25ce5408138f4cb28e163f1fdffdee5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b35a70a3b9e4f3b87cf2a7280439ac6","placeholder":"​","style":"IPY_MODEL_0e26d28846fb449789510e4748c01c6a","value":"100%"}},"2ac0cceac38e43adb00a10b778fad2df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddb1791795134ac0a754748f9793c214","placeholder":"​","style":"IPY_MODEL_d95aea2b915b44f38b5090ee186abafd","value":" 807/807 [04:31&lt;00:00,  2.80it/s]"}},"2fb36a08ca1540228919af722727572c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31d37adf891a4da68675945509051210":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_055681c8159b4b6c8104d4e06279803c","placeholder":"​","style":"IPY_MODEL_9f5e5843559b4f9e8f3440ebd85743f8","value":" 807/807 [04:52&lt;00:00,  3.03it/s]"}},"3bd47d84fac442d4bcf49dceeb699d45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4151c7353de54909aa9deecbe8c1e1e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"462d9e93a94f44868fec1ece82f0a241":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47793d3155614c8cbd7d3337dcb2f895":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8093c8ddeec4c55ac6cc5f8f7df30bf","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4151c7353de54909aa9deecbe8c1e1e1","value":3}},"4bc81f6d94394d7f90e070d8b11a7059":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4da9993585914618a35d8d5382fef850":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"4dd64f565fcf4b259ef24944493477bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_25ce5408138f4cb28e163f1fdffdee5f","IPY_MODEL_47793d3155614c8cbd7d3337dcb2f895","IPY_MODEL_f9ec11073e484927a4cc1ee2e3da132f"],"layout":"IPY_MODEL_62e750769d364ab2b80d5b9646b10ea6"}},"5e15315a77864badafe48c2baf31b030":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dfd9dd713862479bba73b6c0a12a1902","IPY_MODEL_88be535ce5a945f4979fe72d9f086365","IPY_MODEL_31d37adf891a4da68675945509051210"],"layout":"IPY_MODEL_4da9993585914618a35d8d5382fef850"}},"5edd903b07594d128ded9b4844835159":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bc81f6d94394d7f90e070d8b11a7059","max":807,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d3bc58d04d5f42a5a24bd467a9569e01","value":807}},"5f1fb4d40e154e27baa0d4f330df540e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62e750769d364ab2b80d5b9646b10ea6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"634935ff0a6b4d65b72f87359d9f82fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4de75777bba4d72b61936baf3d84cbb","max":807,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0481743e80634f7a8e718fb528950e54","value":807}},"6f5ca39406174a4fb1bee9eb1e35fccb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70c7b641da5c4b82bbd5b2e1750f3b39":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"851b29291123491a821b1ce8088ca785":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"88be535ce5a945f4979fe72d9f086365":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8382d8bbb7f42f6a0f4d4028203615f","max":807,"min":0,"orientation":"horizontal","style":"IPY_MODEL_851b29291123491a821b1ce8088ca785","value":807}},"8b35a70a3b9e4f3b87cf2a7280439ac6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"948787e6434f419a86d9d7da831e2572":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ced4485d5c641eda90ab0634e0691d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f5e5843559b4f9e8f3440ebd85743f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a317a54aa1a349f490e388aacb2d1e4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c470a61e692a4459988f63f0024f7eac","IPY_MODEL_5edd903b07594d128ded9b4844835159","IPY_MODEL_2ac0cceac38e43adb00a10b778fad2df"],"layout":"IPY_MODEL_c7f61bdee3ca4ca0a3f7d021aa558deb"}},"a51aa7b827db4f768dce6315dbebf379":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f1fb4d40e154e27baa0d4f330df540e","placeholder":"​","style":"IPY_MODEL_948787e6434f419a86d9d7da831e2572","value":"100%"}},"acee2c1ed91e44188d5ca40bddace4b1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c470a61e692a4459988f63f0024f7eac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70c7b641da5c4b82bbd5b2e1750f3b39","placeholder":"​","style":"IPY_MODEL_462d9e93a94f44868fec1ece82f0a241","value":"100%"}},"c7f61bdee3ca4ca0a3f7d021aa558deb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"cc57eeb92d32434ca82b7c3f669865d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3bc58d04d5f42a5a24bd467a9569e01":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d95aea2b915b44f38b5090ee186abafd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc4a8a3b0a8b4ef49dcd6269a1b32e14":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"ddb1791795134ac0a754748f9793c214":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfd9dd713862479bba73b6c0a12a1902":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acee2c1ed91e44188d5ca40bddace4b1","placeholder":"​","style":"IPY_MODEL_cc57eeb92d32434ca82b7c3f669865d1","value":"100%"}},"e4de75777bba4d72b61936baf3d84cbb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8093c8ddeec4c55ac6cc5f8f7df30bf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8382d8bbb7f42f6a0f4d4028203615f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9ec11073e484927a4cc1ee2e3da132f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ced4485d5c641eda90ab0634e0691d3","placeholder":"​","style":"IPY_MODEL_6f5ca39406174a4fb1bee9eb1e35fccb","value":" 3/3 [13:53&lt;00:00, 275.36s/it]"}}}}},"nbformat":4,"nbformat_minor":0}
