{"cells":[{"cell_type":"markdown","metadata":{"id":"trbcfMV6vked"},"source":["<https://github.com/PolymathicAI/xVal>\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"yKsx1R26dLGC"},"outputs":[],"source":["import os\n","\n","os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.95\""]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2102,"status":"ok","timestamp":1708903394514,"user":{"displayName":"Kai Lukowiak","userId":"12340107642472090190"},"user_tz":420},"id":"RYm4WOeS1B8x","outputId":"a5c71de2-9597-4fcb-a9a7-b743f1a29feb"},"outputs":[{"data":{"text/plain":["{cuda(id=0)}"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import jax.numpy as jnp  # Oddly works in colab to set gpu\n","\n","arr = jnp.array([1, 2, 3])\n","arr.devices()"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Jqmm_5s9KXkI"},"outputs":[],"source":["import icecream\n","from icecream import ic\n","\n","icecream.install()\n","ic_disable = True\n","if ic_disable:\n","    ic.disable()\n","ic.configureOutput(includeContext=True, contextAbsPath=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18045,"status":"ok","timestamp":1708903436762,"user":{"displayName":"Kai Lukowiak","userId":"12340107642472090190"},"user_tz":420},"id":"oSKniUdxtiTd","outputId":"49871258-6ef7-4956-ad3b-90af827f7253"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-20 00:46:39.845518: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["import os\n","import ast\n","\n","from datetime import datetime as dt\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","import hephaestus as hp\n","import jax\n","import jax.numpy as jnp\n","import numpy as np\n","import optax\n","import pandas as pd\n","from flax.training import train_state\n","from icecream import ic\n","from jax import random\n","from flax import struct\n","from jax.tree_util import tree_flatten\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm.notebook import tqdm, trange\n","\n","pd.options.mode.copy_on_write = True"]},{"cell_type":"markdown","metadata":{},"source":["\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def line2df(line, idx):\n","    data_rows = []\n","    line = ast.literal_eval(line)\n","    for i, time_step in enumerate(line[\"data\"]):\n","        row = {\"time_step\": i}\n","        # Add position data for each planet\n","        for j, position in enumerate(time_step):\n","            row[f\"planet{j}_x\"] = position[0]\n","            row[f\"planet{j}_y\"] = position[1]\n","        data_rows.append(row)\n","\n","    df = pd.DataFrame(data_rows)\n","    description = line.pop(\"description\")\n","    step_size = description.pop(\"stepsize\")\n","    for k, v in description.items():\n","        for k_prop, v_prop in v.items():\n","            df[f\"{k}_{k_prop}\"] = v_prop\n","    df[\"time_step\"] = df[\"time_step\"] * step_size\n","    df.insert(0, \"idx\", idx)\n","\n","    return df"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["files = os.listdir(\"data\")\n","if \"planets.parquet\" not in files:\n","    with open(\"data/planets.data\") as f:\n","        data = f.read().splitlines()\n","\n","        dfs = []\n","        for idx, line in enumerate(tqdm(data)):\n","            dfs.append(line2df(line, idx))\n","        print(\"Concatenating dfs...\")\n","        df = pd.concat(dfs)\n","    df.to_parquet(\"data/planets.parquet\")\n","else:\n","    df = pd.read_parquet(\"data/planets.parquet\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["min     30.000000\n","mean    44.511656\n","max     59.000000\n","Name: time_step, dtype: float64"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Get min, mean, and max number of time steps\n","df.groupby(\"idx\").count().time_step.agg([\"min\", \"mean\", \"max\"])"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class SimpleDS(Dataset):\n","    def __init__(self, df):\n","        # Add nan padding to make sure all sequences are the same length\n","        # use the idx column to group by\n","        self.max_seq_len = df.groupby(\"idx\").count().time_step.max()\n","\n","        self.df = df\n","        self.batch_size = self.max_seq_len\n","\n","        self.special_tokens = [\"[PAD]\", \"[NUMERIC_MASK]\", \"[MASK]\"]\n","        self.cat_mask = \"[MASK]\"\n","        self.numeric_mask = \"[NUMERIC_MASK]\"\n","\n","        self.col_tokens = [col_name for col_name in df.columns if col_name != \"idx\"]\n","\n","        self.tokens = self.special_tokens + self.col_tokens\n","\n","        self.token_dict = {token: i for i, token in enumerate(self.tokens)}\n","        self.token_decoder_dict = {i: token for i, token in enumerate(self.tokens)}\n","        self.n_tokens = len(self.tokens)\n","        self.numeric_indices = jnp.array(\n","            [self.tokens.index(i) for i in self.col_tokens]\n","        )\n","\n","        self.numeric_mask_token = self.tokens.index(self.numeric_mask)\n","\n","    def __len__(self):\n","        return df.idx.max() + 1  # probably should be max idx + 1 thanks\n","\n","    def __getitem__(self, set_idx):\n","        batch = self.df.loc[\n","            df.idx == set_idx, [col for col in self.df.columns if col != \"idx\"]\n","        ]\n","        batch = np.array(batch.values)\n","        # Add padding\n","        batch_len, n_cols = batch.shape\n","        pad_len = self.max_seq_len - batch_len\n","        padding = np.full((pad_len, n_cols), jnp.nan)\n","        batch = np.concatenate([batch, padding], axis=0)\n","        return batch"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["array([[nan, nan, nan],\n","       [nan, nan, nan],\n","       [nan, nan, nan]])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["np.full((3, 3), np.nan)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"4RZof2SNKXkK"},"outputs":[],"source":["train_ds = SimpleDS(df)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"Y4b7IkMWKXkK"},"outputs":[],"source":["time_series_regressor = hp.simple_time_series.SimplePred(train_ds, d_model=64 * 4)"]},{"cell_type":"markdown","metadata":{},"source":["\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["256"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["time_series_regressor.d_model"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["array([[ 0.        ,  1.56006022, -0.85443699, ...,         nan,\n","                nan,         nan],\n","       [ 0.46511628,  1.68985799, -0.5143588 , ...,         nan,\n","                nan,         nan],\n","       [ 0.93023256,  1.75358875, -0.15420858, ...,         nan,\n","                nan,         nan],\n","       ...,\n","       [        nan,         nan,         nan, ...,         nan,\n","                nan,         nan],\n","       [        nan,         nan,         nan, ...,         nan,\n","                nan,         nan],\n","       [        nan,         nan,         nan, ...,         nan,\n","                nan,         nan]])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["train_ds[0]"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def make_batch(ds: SimpleDS, start: int, length: int):\n","    data = []\n","    for i in range(start, length + start):\n","        data.append(ds[i])\n","\n","    return jnp.array(data)\n","\n","\n","batch = make_batch(train_ds, 0, 4)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["ic| simple_time_series.py:188 in __call__()\n","    col_embeddings.shape: (26, 256)\n","    numeric_inputs.shape: (4, 59, 26)\n","ic| simple_time_series.py:192 in __call__()\n","    repeated_numeric_indices.shape: (59, 26)\n","ic| simple_time_series.py:202 in __call__()\n","    numeric_inputs.shape: (4, 59, 26)\n","    numeric_col_embeddings.shape: (59, 26, 256)\n","ic| simple_time_series.py:206 in __call__()\n","    \"here!!!!!!\": 'here!!!!!!'\n","    numeric_inputs.shape: (4, 59, 26)\n","    numeric_col_embeddings.shape: (59, 26, 256)\n","    nan_mask.shape: (4, 59, 26)\n","    numeric_mat_mull.shape: (4, 59, 26, 256)\n","ic| simple_time_series.py:220 in __call__()\n","    f\"Nan values in out: {jnp.isnan(out).any()}\": 'Nan values in out: False'\n","ic| simple_time_series.py:225 in __call__()\n","    out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:226 in __call__()\n","    f\"Nan values in out positional: {jnp.isnan(out).any()}\": 'Nan values in out positional: False'\n","ic| simple_time_series.py:227 in __call__()- 'Starting Attention'\n","ic| simple_time_series.py:228 in __call__()\n","    out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:35 in __call__()\n","    q.shape: (4, 59, 26, 272)\n","    k.shape: (4, 59, 26, 272)\n","    v.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:94 in scaled_dot_product_attention()\n","    f\"Before Masking matmul_qk: {matmul_qk.shape=}\": 'Before Masking matmul_qk: matmul_qk.shape=(4, 59, 26, 4, 4)'\n","ic| simple_time_series.py:71 in __call__()\n","    attention_out.shape: (4, 59, 26, 4, 68)\n","ic| simple_time_series.py:82 in __call__()\n","    attention_out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:236 in __call__()\n","    out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:237 in __call__()\n","    f\"Nan values in out 1st mha: {jnp.isnan(out).any()}\": 'Nan values in out 1st mha: False'\n","ic| simple_time_series.py:35 in __call__()\n","    q.shape: (4, 59, 26, 272)\n","    k.shape: (4, 59, 26, 272)\n","    v.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:94 in scaled_dot_product_attention()\n","    f\"Before Masking matmul_qk: {matmul_qk.shape=}\": 'Before Masking matmul_qk: matmul_qk.shape=(4, 59, 26, 4, 4)'\n","ic| simple_time_series.py:71 in __call__()\n","    attention_out.shape: (4, 59, 26, 4, 68)\n","ic| simple_time_series.py:82 in __call__()\n","    attention_out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:246 in __call__()\n","    f\"Nan values in in out 2nd mha: {jnp.isnan(out).any()}\": 'Nan values in in out 2nd mha: False'\n","ic| simple_time_series.py:265 in __call__()\n","    out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:266 in __call__()\n","    f\"Nan values in simplePred out 1: {jnp.isnan(out).any()}\": 'Nan values in simplePred out 1: False'\n","ic| simple_time_series.py:275 in __call__()\n","    f\"Nan values in simplePred out after seq: {jnp.isnan(out).any()}\": 'Nan values in simplePred out after seq: False'\n","ic| simple_time_series.py:277 in __call__()\n","    f\"Penultimate Simple OUT: {out.shape}\": 'Penultimate Simple OUT: (4, 59, 26, 1)'\n","ic| simple_time_series.py:279 in __call__()\n","    f\"Nan values in simplePred out after seq: {jnp.isnan(out).any()}\": 'Nan values in simplePred out after seq: False'\n","ic| simple_time_series.py:282 in __call__()\n","    f\"FInal Simple OUT: {out.shape}\": 'FInal Simple OUT: (4, 59, 26)'\n","ic| simple_time_series.py:188 in __call__()\n","    col_embeddings.shape: (26, 256)\n","    numeric_inputs.shape: (4, 59, 26)\n","ic| simple_time_series.py:192 in __call__()\n","    repeated_numeric_indices.shape: (59, 26)\n","ic| simple_time_series.py:202 in __call__()\n","    numeric_inputs.shape: (4, 59, 26)\n","    numeric_col_embeddings.shape: (59, 26, 256)\n","ic| simple_time_series.py:206 in __call__()\n","    \"here!!!!!!\": 'here!!!!!!'\n","    numeric_inputs.shape: (4, 59, 26)\n","    numeric_col_embeddings.shape: (59, 26, 256)\n","    nan_mask.shape: (4, 59, 26)\n","    numeric_mat_mull.shape: (4, 59, 26, 256)\n","ic| simple_time_series.py:220 in __call__()\n","    f\"Nan values in out: {jnp.isnan(out).any()}\": 'Nan values in out: False'\n","ic| simple_time_series.py:225 in __call__()\n","    out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:226 in __call__()\n","    f\"Nan values in out positional: {jnp.isnan(out).any()}\": 'Nan values in out positional: False'\n","ic| simple_time_series.py:227 in __call__()- 'Starting Attention'\n","ic| simple_time_series.py:228 in __call__()\n","    out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:35 in __call__()\n","    q.shape: (4, 59, 26, 272)\n","    k.shape: (4, 59, 26, 272)\n","    v.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:94 in scaled_dot_product_attention()\n","    f\"Before Masking matmul_qk: {matmul_qk.shape=}\": 'Before Masking matmul_qk: matmul_qk.shape=(4, 59, 26, 4, 4)'\n","ic| simple_time_series.py:71 in __call__()\n","    attention_out.shape: (4, 59, 26, 4, 68)\n","ic| simple_time_series.py:82 in __call__()\n","    attention_out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:236 in __call__()\n","    out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:237 in __call__()\n","    f\"Nan values in out 1st mha: {jnp.isnan(out).any()}\": 'Nan values in out 1st mha: False'\n","ic| simple_time_series.py:35 in __call__()\n","    q.shape: (4, 59, 26, 272)\n","    k.shape: (4, 59, 26, 272)\n","    v.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:94 in scaled_dot_product_attention()\n","    f\"Before Masking matmul_qk: {matmul_qk.shape=}\": 'Before Masking matmul_qk: matmul_qk.shape=(4, 59, 26, 4, 4)'\n","ic| simple_time_series.py:71 in __call__()\n","    attention_out.shape: (4, 59, 26, 4, 68)\n","ic| simple_time_series.py:82 in __call__()\n","    attention_out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:246 in __call__()\n","    f\"Nan values in in out 2nd mha: {jnp.isnan(out).any()}\": 'Nan values in in out 2nd mha: False'\n","ic| simple_time_series.py:265 in __call__()\n","    out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:266 in __call__()\n","    f\"Nan values in simplePred out 1: {jnp.isnan(out).any()}\": 'Nan values in simplePred out 1: False'\n","ic| simple_time_series.py:275 in __call__()\n","    f\"Nan values in simplePred out after seq: {jnp.isnan(out).any()}\": 'Nan values in simplePred out after seq: False'\n","ic| simple_time_series.py:277 in __call__()\n","    f\"Penultimate Simple OUT: {out.shape}\": 'Penultimate Simple OUT: (4, 59, 26, 1)'\n","ic| simple_time_series.py:279 in __call__()\n","    f\"Nan values in simplePred out after seq: {jnp.isnan(out).any()}\": 'Nan values in simplePred out after seq: False'\n","ic| simple_time_series.py:282 in __call__()\n","    f\"FInal Simple OUT: {out.shape}\": 'FInal Simple OUT: (4, 59, 26)'\n"]}],"source":["vars = time_series_regressor.init(random.PRNGKey(0), batch)\n","\n","x = time_series_regressor.apply(vars, batch)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"ename":"ZeroDivisionError","evalue":"division by zero","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n","\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"]}],"source":["1 / 0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["Array([[[-0.99787635, -1.0830878 , -1.0205737 , ..., -1.1147583 ,\n","         -1.1147583 , -1.1147583 ],\n","        [-0.6347571 , -1.0432668 , -0.9717052 , ..., -0.9100214 ,\n","         -0.9100214 , -0.9100214 ],\n","        [-0.43598816, -1.0155857 , -0.74097013, ..., -0.92857766,\n","         -0.92857766, -0.92857766],\n","        ...,\n","        [-1.5675948 , -1.5675948 , -1.5675948 , ..., -1.5675948 ,\n","         -1.5675948 , -1.5675948 ],\n","        [-1.5146824 , -1.5146824 , -1.5146824 , ..., -1.5146824 ,\n","         -1.5146824 , -1.5146824 ],\n","        [-1.324331  , -1.324331  , -1.324331  , ..., -1.324331  ,\n","         -1.324331  , -1.324331  ]],\n","\n","       [[-0.99787635, -0.91905284, -0.6940967 , ..., -1.1147583 ,\n","         -1.1147583 , -1.1147583 ],\n","        [-0.656036  , -0.6604141 , -0.38698754, ..., -0.9100214 ,\n","         -0.9100214 , -0.9100214 ],\n","        [-0.47413844, -0.59235746, -0.11756715, ..., -0.92857766,\n","         -0.92857766, -0.92857766],\n","        ...,\n","        [-1.5675948 , -1.5675948 , -1.5675948 , ..., -1.5675948 ,\n","         -1.5675948 , -1.5675948 ],\n","        [-1.5146824 , -1.5146824 , -1.5146824 , ..., -1.5146824 ,\n","         -1.5146824 , -1.5146824 ],\n","        [-1.324331  , -1.324331  , -1.324331  , ..., -1.324331  ,\n","         -1.324331  , -1.324331  ]],\n","\n","       [[-0.99787635, -0.4435499 , -1.0232308 , ..., -1.1147583 ,\n","         -1.1147583 , -1.1147583 ],\n","        [-0.6203122 , -0.44943848, -1.1541004 , ..., -0.9100214 ,\n","         -0.9100214 , -0.9100214 ],\n","        [-0.4075574 , -0.56579536, -1.0147563 , ..., -0.92857766,\n","         -0.92857766, -0.92857766],\n","        ...,\n","        [-1.5675948 , -1.5675948 , -1.5675948 , ..., -1.5675948 ,\n","         -1.5675948 , -1.5675948 ],\n","        [-1.5146824 , -1.5146824 , -1.5146824 , ..., -1.5146824 ,\n","         -1.5146824 , -1.5146824 ],\n","        [-1.324331  , -1.324331  , -1.324331  , ..., -1.324331  ,\n","         -1.324331  , -1.324331  ]],\n","\n","       [[-0.99787635, -1.0375865 , -0.99787635, ..., -0.5446441 ,\n","         -0.7943155 , -0.640522  ],\n","        [-0.564858  , -0.97870517, -0.61102295, ..., -0.57332766,\n","         -0.6477485 , -0.46796298],\n","        [-0.31553438, -0.79816306, -0.4200492 , ..., -0.465229  ,\n","         -0.52249795, -0.35326314],\n","        ...,\n","        [-1.5675948 , -1.5675948 , -1.5675948 , ..., -1.5675948 ,\n","         -1.5675948 , -1.5675948 ],\n","        [-1.5146824 , -1.5146824 , -1.5146824 , ..., -1.5146824 ,\n","         -1.5146824 , -1.5146824 ],\n","        [-1.324331  , -1.324331  , -1.324331  , ..., -1.324331  ,\n","         -1.324331  , -1.324331  ]]], dtype=float32)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["(4, 59, 26)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["batch.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pMS-gjmiVGsn"},"outputs":[],"source":["def calculate_memory_footprint(params):\n","    \"\"\"Calculate total memory footprint of JAX model parameters.\"\"\"\n","    total_bytes = 0\n","    # Flatten the parameter tree structure into a list of arrays\n","    flat_params, _ = tree_flatten(params)\n","    for param in flat_params:\n","        # Calculate bytes: number of elements * size of each element\n","        bytes_per_param = param.size * param.dtype.itemsize\n","        total_bytes += bytes_per_param\n","    return total_bytes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_causal_mask(tensor: jnp.ndarray):\n","    \"\"\"Create a causal mask to mask out future values.\"\"\"\n","    mask = jnp.tril(jnp.ones((tensor.shape[0], tensor.shape[1])))\n","    return mask\n","\n","\n","def create_padding_mask(tensor: jnp.ndarray):\n","    \"\"\"Create a padding mask to mask out padded values.\"\"\"\n","    mask = jnp.isnan(tensor)\n","    return mask\n","\n","\n","def mask_array(tensor: jnp.array):\n","    \"\"\"Create a mask for the tensor\"\"\"\n","    causal_mask = create_causal_mask(tensor)\n","    padding_mask = create_padding_mask(tensor)\n","    mask = jnp.logical_or(causal_mask, padding_mask)\n","    return mask"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["(59, 26)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["jnp.array(train_ds[0]).shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["4dd64f565fcf4b259ef24944493477bf","25ce5408138f4cb28e163f1fdffdee5f","47793d3155614c8cbd7d3337dcb2f895","f9ec11073e484927a4cc1ee2e3da132f","62e750769d364ab2b80d5b9646b10ea6","8b35a70a3b9e4f3b87cf2a7280439ac6","0e26d28846fb449789510e4748c01c6a","f8093c8ddeec4c55ac6cc5f8f7df30bf","4151c7353de54909aa9deecbe8c1e1e1","9ced4485d5c641eda90ab0634e0691d3","6f5ca39406174a4fb1bee9eb1e35fccb","5e15315a77864badafe48c2baf31b030","dfd9dd713862479bba73b6c0a12a1902","88be535ce5a945f4979fe72d9f086365","31d37adf891a4da68675945509051210","4da9993585914618a35d8d5382fef850","acee2c1ed91e44188d5ca40bddace4b1","cc57eeb92d32434ca82b7c3f669865d1","f8382d8bbb7f42f6a0f4d4028203615f","851b29291123491a821b1ce8088ca785","055681c8159b4b6c8104d4e06279803c","9f5e5843559b4f9e8f3440ebd85743f8","a317a54aa1a349f490e388aacb2d1e4d","c470a61e692a4459988f63f0024f7eac","5edd903b07594d128ded9b4844835159","2ac0cceac38e43adb00a10b778fad2df","c7f61bdee3ca4ca0a3f7d021aa558deb","70c7b641da5c4b82bbd5b2e1750f3b39","462d9e93a94f44868fec1ece82f0a241","4bc81f6d94394d7f90e070d8b11a7059","d3bc58d04d5f42a5a24bd467a9569e01","ddb1791795134ac0a754748f9793c214","d95aea2b915b44f38b5090ee186abafd","01ca2dcb14e647dfb1e68750ca6de39c","a51aa7b827db4f768dce6315dbebf379","634935ff0a6b4d65b72f87359d9f82fc","09b63c98bee543dfb557a007d50c41d1","dc4a8a3b0a8b4ef49dcd6269a1b32e14","5f1fb4d40e154e27baa0d4f330df540e","948787e6434f419a86d9d7da831e2572","e4de75777bba4d72b61936baf3d84cbb","0481743e80634f7a8e718fb528950e54","2fb36a08ca1540228919af722727572c","3bd47d84fac442d4bcf49dceeb699d45"]},"executionInfo":{"elapsed":841688,"status":"ok","timestamp":1708904334771,"user":{"displayName":"Kai Lukowiak","userId":"12340107642472090190"},"user_tz":420},"id":"RIXu3GkdYzNA","outputId":"dee6137d-c096-4df6-fbb4-baa3764d359e"},"outputs":[{"name":"stderr","output_type":"stream","text":["ic| simple_time_series.py:188 in __call__()\n","    col_embeddings.shape: (26, 256)\n","    numeric_inputs.shape: (4, 59, 26)\n","ic| simple_time_series.py:192 in __call__()\n","    repeated_numeric_indices.shape: (59, 26)\n","ic| simple_time_series.py:202 in __call__()\n","    numeric_inputs.shape: (4, 59, 26)\n","    numeric_col_embeddings.shape: (59, 26, 256)\n","ic| simple_time_series.py:206 in __call__()\n","    numeric_inputs.shape: (4, 59, 26)\n","    numeric_col_embeddings.shape: (59, 26, 256)\n","    nan_mask.shape: (4, 59, 26)\n","ic| simple_time_series.py:214 in __call__()\n","    f\"Nan values in out: {jnp.isnan(out).any()}\": 'Nan values in out: False'\n","ic| simple_time_series.py:219 in __call__()\n","    out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:220 in __call__()\n","    f\"Nan values in out positional: {jnp.isnan(out).any()}\": 'Nan values in out positional: False'\n","ic| simple_time_series.py:221 in __call__()- 'Starting Attention'\n","ic| simple_time_series.py:222 in __call__()\n","    out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:35 in __call__()\n","    q.shape: (4, 59, 26, 272)\n","    k.shape: (4, 59, 26, 272)\n","    v.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:94 in scaled_dot_product_attention()\n","    f\"Before Masking matmul_qk: {matmul_qk.shape=}\": 'Before Masking matmul_qk: matmul_qk.shape=(4, 59, 26, 4, 4)'\n","ic| simple_time_series.py:71 in __call__()\n","    attention_out.shape: (4, 59, 26, 4, 68)\n","ic| simple_time_series.py:82 in __call__()\n","    attention_out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:230 in __call__()\n","    out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:231 in __call__()\n","    f\"Nan values in out 1st mha: {jnp.isnan(out).any()}\": 'Nan values in out 1st mha: False'\n","ic| simple_time_series.py:35 in __call__()\n","    q.shape: (4, 59, 26, 272)\n","    k.shape: (4, 59, 26, 272)\n","    v.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:94 in scaled_dot_product_attention()\n","    f\"Before Masking matmul_qk: {matmul_qk.shape=}\": 'Before Masking matmul_qk: matmul_qk.shape=(4, 59, 26, 4, 4)'\n","ic| simple_time_series.py:71 in __call__()\n","    attention_out.shape: (4, 59, 26, 4, 68)\n","ic| simple_time_series.py:82 in __call__()\n","    attention_out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:240 in __call__()\n","    f\"Nan values in in out 2nd mha: {jnp.isnan(out).any()}\": 'Nan values in in out 2nd mha: False'\n","ic| simple_time_series.py:259 in __call__()\n","    out.shape: (4, 59, 26, 272)\n","ic| simple_time_series.py:260 in __call__()\n","    f\"Nan values in simplePred out 1: {jnp.isnan(out).any()}\": 'Nan values in simplePred out 1: False'\n","ic| simple_time_series.py:269 in __call__()\n","    f\"Nan values in simplePred out after seq: {jnp.isnan(out).any()}\": 'Nan values in simplePred out after seq: False'\n","ic| simple_time_series.py:271 in __call__()\n","    f\"Penultimate Simple OUT: {out.shape}\": 'Penultimate Simple OUT: (4, 59, 26, 1)'\n","ic| simple_time_series.py:273 in __call__()\n","    f\"Nan values in simplePred out after seq: {jnp.isnan(out).any()}\": 'Nan values in simplePred out after seq: False'\n","ic| simple_time_series.py:276 in __call__()\n","    f\"FInal Simple OUT: {out.shape}\": 'FInal Simple OUT: (4, 59, 26)'\n"]}],"source":["mts_root_key = random.PRNGKey(44)\n","mts_main_key, ts_params_key, ts_data_key = random.split(mts_root_key, 3)\n","\n","\n","def clip_gradients(gradients, max_norm):\n","    total_norm = jnp.sqrt(sum(jnp.sum(jnp.square(grad)) for grad in gradients.values()))\n","    scale = max_norm / (total_norm + 1e-6)\n","    clipped_gradients = jax.tree_map(\n","        lambda grad: jnp.where(total_norm > max_norm, grad * scale, grad), gradients\n","    )\n","    return clipped_gradients\n","\n","\n","# @jax.jit\n","def calculate_loss(params, state, inputs, dataset: SimpleDS):\n","    out = state.apply_fn(\n","        {\"params\": params},\n","        hp.mask_tensor(inputs, dataset, prng_key=ts_data_key),\n","    )\n","\n","    # Create mask for nan inputs\n","    nan_mask = jnp.isnan(inputs)\n","    inputs = jnp.where(nan_mask, jnp.zeros_like(inputs), inputs)\n","    out = jnp.where(nan_mask, jnp.zeros_like(out), out)\n","\n","    raw_loss = optax.squared_error(out, inputs)\n","    masked_loss = jnp.where(nan_mask, 0.0, raw_loss)\n","    loss = masked_loss.sum() / (~nan_mask).sum()\n","\n","    return loss\n","\n","\n","@jax.jit\n","def eval_step(state: train_state.TrainState, batch):\n","    def loss_fn(params):\n","        return calculate_loss(params, state, batch, train_ds)\n","\n","    # (loss, individual_losses), grad = grad_fn(state.params)\n","    loss, loss_dict = loss_fn(state.params)\n","    return loss, loss_dict\n","\n","\n","@jax.jit\n","def train_step(state: train_state.TrainState, batch):\n","    def loss_fn(params):\n","        return calculate_loss(params, state, batch, train_ds)\n","\n","    grad_fn = jax.value_and_grad(loss_fn)\n","\n","    # (loss, individual_losses), grad = grad_fn(state.params)\n","    loss, grad = grad_fn(state.params)\n","    # grad = replace_nans(grad)\n","    # grad = clip_gradients(grad, 1.0)\n","    state = state.apply_gradients(grads=grad)\n","\n","    return state, loss\n","\n","\n","def create_train_state(model, prng, batch, lr):\n","    params = model.init(prng, batch)\n","    # optimizer = optax.chain(optax.adam(lr))\n","    optimizer = optax.chain(optax.clip_by_global_norm(0.4), optax.adam(lr))\n","    # optimizer_state = optimizer.init(params)\n","    return train_state.TrainState.create(\n","        apply_fn=model.apply,\n","        params=params[\"params\"],\n","        tx=optimizer,\n","        # tx_state=optimizer_state,\n","    )\n","\n","\n","batch_size = 2\n","# batch = train_ds[0]\n","\n","state = create_train_state(time_series_regressor, mts_main_key, batch, 0.0001)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf15f1e6eec740918c3e6b80b48cbc3e","version_major":2,"version_minor":0},"text/plain":["epochs for runs/2024-05-20T00:40:31_wow_nn_Attention_Same_Params_train:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"018473bd4d5d4e6db112d52b5fbe37bc","version_major":2,"version_minor":0},"text/plain":["batches:   0%|          | 0/245 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m batch_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m2\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_summary_writer\u001b[38;5;241m.\u001b[39mlog_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# arrs = train_data_loader()\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(train_data_loader, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatches\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m# for i in trange(len(pre_train) // batch_size, leave=False):\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;66;03m# for i in trange(len(pre_train) // batch_size //10, leave=False):\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m# batch = make_batch(train_ds, i[0], 4)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m         state, loss \u001b[38;5;241m=\u001b[39m train_step(state, jnp\u001b[38;5;241m.\u001b[39marray(i))\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39misnan(loss):\n","File \u001b[0;32m~/environment/Hephaestus/.venv/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n","File \u001b[0;32m~/environment/Hephaestus/.venv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m~/environment/Hephaestus/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m~/environment/Hephaestus/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m~/environment/Hephaestus/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m~/environment/Hephaestus/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[8], line 32\u001b[0m, in \u001b[0;36mSimpleDS.__getitem__\u001b[0;34m(self, set_idx)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, set_idx):\n\u001b[1;32m     31\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mloc[\n\u001b[0;32m---> 32\u001b[0m         \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mset_idx\u001b[49m, [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     33\u001b[0m     ]\n\u001b[1;32m     34\u001b[0m     batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(batch\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Add padding\u001b[39;00m\n","File \u001b[0;32m~/environment/Hephaestus/.venv/lib/python3.10/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/environment/Hephaestus/.venv/lib/python3.10/site-packages/pandas/core/arraylike.py:40\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__eq__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meq\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/environment/Hephaestus/.venv/lib/python3.10/site-packages/pandas/core/series.py:6110\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6107\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   6108\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 6110\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n","File \u001b[0;32m~/environment/Hephaestus/.venv/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:347\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    344\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_cmp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n","File \u001b[0;32m~/environment/Hephaestus/.venv/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:218\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    215\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(expressions\u001b[38;5;241m.\u001b[39mevaluate, op)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    221\u001b[0m         left\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    222\u001b[0m     ):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n","File \u001b[0;32m~/environment/Hephaestus/.venv/lib/python3.10/site-packages/pandas/core/computation/expressions.py:242\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op_str \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n","File \u001b[0;32m~/environment/Hephaestus/.venv/lib/python3.10/site-packages/pandas/core/computation/expressions.py:73\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _TEST_MODE:\n\u001b[1;32m     72\u001b[0m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["writer_name = \"nn_Attention_Same_Params\"\n","\n","writer_time = dt.now().strftime(\"%Y-%m-%dT%H:%M:%S\")\n","\n","train_summary_writer = SummaryWriter(\n","    \"runs/\" + writer_time + \"_wow_\" + writer_name + \"_train\"\n",")\n","\n","\n","test_set_key = random.PRNGKey(4454)\n","\n","train_data_loader = DataLoader(train_ds, batch_size=512, shuffle=True)\n","batch_count = 0\n","for j in trange(2, desc=f\"epochs for {train_summary_writer.log_dir}\"):\n","    # arrs = train_data_loader()\n","    for i in tqdm(train_data_loader, leave=False, desc=\"batches\"):\n","        # for i in trange(len(pre_train) // batch_size, leave=False):\n","        # for i in trange(len(pre_train) // batch_size //10, leave=False):\n","        # batch = make_batch(train_ds, i[0], 4)\n","\n","        state, loss = train_step(state, jnp.array(i))\n","        if jnp.isnan(loss):\n","            raise ValueError(\"Nan Value in loss, stopping\")\n","        batch_count += 1\n","\n","        if batch_count % 1 == 0:\n","            train_summary_writer.add_scalar(\n","                \"loss/loss\", np.array(loss.item()), batch_count\n","            )\n","train_summary_writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0;31mInit signature:\u001b[0m\n","\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mnum_heads\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupportsDType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mparam_dtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupportsDType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'jax.numpy.float32'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mqkv_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mout_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mbroadcast_dropout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdropout_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mprecision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNoneType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mkernel_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mvariance_scaling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7a494413cee0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mbias_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mzeros\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7a4a1f6e0ee0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mattention_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mdot_product_attention\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7a4944180280\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdecode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mnormalize_qk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mqkv_dot_general\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mout_dot_general\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mqkv_dot_general_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mout_dot_general_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mparent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Sentinel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Sentinel\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7a494412a560\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDocstring:\u001b[0m     \n","Multi-head dot-product attention.\n","Alias for ``MultiHeadDotProductAttention``.\n","\n","**NOTE**: ``MultiHeadAttention`` is a wrapper of ``MultiHeadDotProductAttention``,\n","and so their implementations are identical. However ``MultiHeadAttention`` layers\n","will, by default, be named ``MultiHeadAttention_{index}``, whereas ``MultiHeadDotProductAttention``\n","will be named ``MultiHeadDotProductAttention_{index}``. Therefore, this could affect\n","checkpointing, param collection names and RNG threading (since the layer name is\n","used when generating new RNG's) within the module.\n","\n","Example usage::\n","\n","  >>> import flax.linen as nn\n","  >>> import jax\n","\n","  >>> layer = nn.MultiHeadAttention(num_heads=8, qkv_features=16)\n","  >>> key1, key2, key3, key4, key5, key6 = jax.random.split(jax.random.key(0), 6)\n","  >>> shape = (4, 3, 2, 5)\n","  >>> q, k, v = jax.random.uniform(key1, shape), jax.random.uniform(key2, shape), jax.random.uniform(key3, shape)\n","  >>> variables = layer.init(jax.random.key(0), q)\n","\n","  >>> # different inputs for inputs_q, inputs_k and inputs_v\n","  >>> out = layer.apply(variables, q, k, v)\n","  >>> # equivalent to layer.apply(variables, inputs_q=q, inputs_k=k, inputs_v=k)\n","  >>> out = layer.apply(variables, q, k)\n","  >>> # equivalent to layer.apply(variables, inputs_q=q, inputs_k=q) and layer.apply(variables, inputs_q=q, inputs_k=q, inputs_v=q)\n","  >>> out = layer.apply(variables, q)\n","\n","  >>> attention_kwargs = dict(\n","  ...     num_heads=8,\n","  ...     qkv_features=16,\n","  ...     kernel_init=nn.initializers.ones,\n","  ...     bias_init=nn.initializers.zeros,\n","  ...     dropout_rate=0.5,\n","  ...     deterministic=False,\n","  ...     )\n","  >>> class Module(nn.Module):\n","  ...   attention_kwargs: dict\n","  ...\n","  ...   @nn.compact\n","  ...   def __call__(self, x, dropout_rng=None):\n","  ...     out1 = nn.MultiHeadAttention(**self.attention_kwargs)(x, dropout_rng=dropout_rng)\n","  ...     out2 = nn.MultiHeadAttention(**self.attention_kwargs)(x, dropout_rng=dropout_rng)\n","  ...     return out1, out2\n","  >>> module = Module(attention_kwargs)\n","  >>> variables = module.init({'params': key1, 'dropout': key2}, q)\n","\n","  >>> # out1 and out2 are different.\n","  >>> out1, out2 = module.apply(variables, q, rngs={'dropout': key3})\n","  >>> # out3 and out4 are different.\n","  >>> # out1 and out3 are different. out2 and out4 are different.\n","  >>> out3, out4 = module.apply(variables, q, rngs={'dropout': key4})\n","  >>> # out1 and out2 are the same.\n","  >>> out1, out2 = module.apply(variables, q, dropout_rng=key5)\n","  >>> # out1 and out2 are the same as out3 and out4.\n","  >>> # providing a `dropout_rng` arg will take precedence over the `rngs` arg in `.apply`\n","  >>> out3, out4 = module.apply(variables, q, rngs={'dropout': key6}, dropout_rng=key5)\n","\n","Attributes:\n","  num_heads: number of attention heads. Features (i.e. inputs_q.shape[-1])\n","    should be divisible by the number of heads.\n","  dtype: the dtype of the computation (default: infer from inputs and params)\n","  param_dtype: the dtype passed to parameter initializers (default: float32)\n","  qkv_features: dimension of the key, query, and value.\n","  out_features: dimension of the last projection\n","  broadcast_dropout: bool: use a broadcasted dropout along batch dims.\n","  dropout_rate: dropout rate\n","  deterministic: if false, the attention weight is masked randomly using\n","    dropout, whereas if true, the attention weights are deterministic.\n","  precision: numerical precision of the computation see ``jax.lax.Precision``\n","    for details.\n","  kernel_init: initializer for the kernel of the Dense layers.\n","  bias_init: initializer for the bias of the Dense layers.\n","  use_bias: bool: whether pointwise QKVO dense transforms use bias.\n","  attention_fn: dot_product_attention or compatible function. Accepts query,\n","    key, value, and returns output of shape ``[bs, dim1, dim2, ..., dimN,,\n","    num_heads, value_channels]``\n","  decode: whether to prepare and use an autoregressive cache.\n","  normalize_qk: should QK normalization be applied (arxiv.org/abs/2302.05442).\n","\u001b[0;31mFile:\u001b[0m           ~/environment/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py\n","\u001b[0;31mType:\u001b[0m           type\n","\u001b[0;31mSubclasses:\u001b[0m     "]}],"source":["import flax.linen as nn\n","\n","?nn.MultiHeadAttention"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from jax import random\n","\n","test_key = jax.random.key(12)\n","att = random.normal(test_key, (2, 10, 6, 16))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["Array([[1.1705537, 1.1705537, 1.1705537, ..., 1.1705537, 1.1705537,\n","        1.1705537],\n","       [1.2930098, 1.293119 , 1.2930987, ..., 1.293119 , 1.293119 ,\n","        1.293119 ],\n","       [1.3365014, 1.3365445, 1.336489 , ..., 1.3365445, 1.3365445,\n","        1.3365445],\n","       ...,\n","       [2.2498384, 2.2498384, 2.2498384, ..., 2.2498384, 2.2498384,\n","        2.2498384],\n","       [2.252554 , 2.252554 , 2.252554 , ..., 2.252554 , 2.252554 ,\n","        2.252554 ],\n","       [2.2437117, 2.2437117, 2.2437117, ..., 2.2437117, 2.2437117,\n","        2.2437117]], dtype=float32)"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["test_result = jnp.squeeze(\n","    state.apply_fn({\"params\": state.params}, jnp.array([train_ds[0]]))\n",")\n","test_result"]},{"cell_type":"code","execution_count":null,"metadata":{"notebookRunGroups":{"groupValue":""}},"outputs":[{"data":{"text/plain":["((59, 26), (59, 26))"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["jnp.array(train_ds[0]).shape, test_result.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["(59, 26)"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["jnp.squeeze(test_result).shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_pred = pd.DataFrame(test_result)\n","df_actual = pd.DataFrame(train_ds[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","      <td>...</td>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","      <td>1.170554</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.293010</td>\n","      <td>1.293119</td>\n","      <td>1.293099</td>\n","      <td>1.293119</td>\n","      <td>1.293119</td>\n","      <td>1.293119</td>\n","      <td>1.293119</td>\n","      <td>1.293119</td>\n","      <td>1.293119</td>\n","      <td>1.293119</td>\n","      <td>...</td>\n","      <td>1.293119</td>\n","      <td>1.293119</td>\n","      <td>1.293119</td>\n","      <td>1.293119</td>\n","      <td>1.293119</td>\n","      <td>1.293119</td>\n","      <td>1.293119</td>\n","      <td>1.293119</td>\n","      <td>1.293119</td>\n","      <td>1.293119</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.336501</td>\n","      <td>1.336545</td>\n","      <td>1.336489</td>\n","      <td>1.336489</td>\n","      <td>1.336545</td>\n","      <td>1.336550</td>\n","      <td>1.336524</td>\n","      <td>1.336545</td>\n","      <td>1.336550</td>\n","      <td>1.336550</td>\n","      <td>...</td>\n","      <td>1.336545</td>\n","      <td>1.336545</td>\n","      <td>1.336545</td>\n","      <td>1.336545</td>\n","      <td>1.336545</td>\n","      <td>1.336545</td>\n","      <td>1.336545</td>\n","      <td>1.336545</td>\n","      <td>1.336545</td>\n","      <td>1.336545</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.343069</td>\n","      <td>1.343064</td>\n","      <td>1.342984</td>\n","      <td>1.343044</td>\n","      <td>1.343052</td>\n","      <td>1.342984</td>\n","      <td>1.343002</td>\n","      <td>1.343064</td>\n","      <td>1.343072</td>\n","      <td>1.343072</td>\n","      <td>...</td>\n","      <td>1.343087</td>\n","      <td>1.343087</td>\n","      <td>1.343087</td>\n","      <td>1.343087</td>\n","      <td>1.343087</td>\n","      <td>1.343087</td>\n","      <td>1.343087</td>\n","      <td>1.343087</td>\n","      <td>1.343087</td>\n","      <td>1.343087</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.375174</td>\n","      <td>1.375121</td>\n","      <td>1.375313</td>\n","      <td>1.375036</td>\n","      <td>1.375317</td>\n","      <td>1.375072</td>\n","      <td>1.375220</td>\n","      <td>1.375094</td>\n","      <td>1.375217</td>\n","      <td>1.375252</td>\n","      <td>...</td>\n","      <td>1.375198</td>\n","      <td>1.375198</td>\n","      <td>1.375198</td>\n","      <td>1.375198</td>\n","      <td>1.375198</td>\n","      <td>1.375198</td>\n","      <td>1.375198</td>\n","      <td>1.375198</td>\n","      <td>1.375198</td>\n","      <td>1.375198</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1.402991</td>\n","      <td>1.429855</td>\n","      <td>1.429755</td>\n","      <td>1.429083</td>\n","      <td>1.429697</td>\n","      <td>1.429249</td>\n","      <td>1.430241</td>\n","      <td>1.429848</td>\n","      <td>1.429969</td>\n","      <td>1.430094</td>\n","      <td>...</td>\n","      <td>1.429824</td>\n","      <td>1.429824</td>\n","      <td>1.429824</td>\n","      <td>1.429824</td>\n","      <td>1.429824</td>\n","      <td>1.429824</td>\n","      <td>1.429824</td>\n","      <td>1.429824</td>\n","      <td>1.429824</td>\n","      <td>1.429824</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.383199</td>\n","      <td>1.477023</td>\n","      <td>1.476209</td>\n","      <td>1.465274</td>\n","      <td>1.468350</td>\n","      <td>1.466837</td>\n","      <td>1.482053</td>\n","      <td>1.477890</td>\n","      <td>1.479513</td>\n","      <td>1.479953</td>\n","      <td>...</td>\n","      <td>1.476856</td>\n","      <td>1.476856</td>\n","      <td>1.476856</td>\n","      <td>1.476856</td>\n","      <td>1.476856</td>\n","      <td>1.476856</td>\n","      <td>1.476856</td>\n","      <td>1.476856</td>\n","      <td>1.476856</td>\n","      <td>1.476856</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2.554446</td>\n","      <td>1.451056</td>\n","      <td>1.465172</td>\n","      <td>1.350047</td>\n","      <td>1.338477</td>\n","      <td>1.325754</td>\n","      <td>1.501209</td>\n","      <td>1.470335</td>\n","      <td>1.484572</td>\n","      <td>1.486199</td>\n","      <td>...</td>\n","      <td>1.459361</td>\n","      <td>1.459361</td>\n","      <td>1.459361</td>\n","      <td>1.459361</td>\n","      <td>1.459361</td>\n","      <td>1.459361</td>\n","      <td>1.459361</td>\n","      <td>1.459361</td>\n","      <td>1.459361</td>\n","      <td>1.459361</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>3.329627</td>\n","      <td>1.222258</td>\n","      <td>1.371737</td>\n","      <td>0.955009</td>\n","      <td>0.704728</td>\n","      <td>0.637018</td>\n","      <td>1.490662</td>\n","      <td>1.362946</td>\n","      <td>1.428571</td>\n","      <td>1.433815</td>\n","      <td>...</td>\n","      <td>1.313279</td>\n","      <td>1.313279</td>\n","      <td>1.313279</td>\n","      <td>1.313279</td>\n","      <td>1.313279</td>\n","      <td>1.313279</td>\n","      <td>1.313279</td>\n","      <td>1.313279</td>\n","      <td>1.313279</td>\n","      <td>1.313279</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>3.917570</td>\n","      <td>0.776517</td>\n","      <td>1.319047</td>\n","      <td>0.659090</td>\n","      <td>0.084370</td>\n","      <td>-0.128863</td>\n","      <td>1.511409</td>\n","      <td>1.249411</td>\n","      <td>1.401388</td>\n","      <td>1.413216</td>\n","      <td>...</td>\n","      <td>1.131709</td>\n","      <td>1.131709</td>\n","      <td>1.131709</td>\n","      <td>1.131709</td>\n","      <td>1.131709</td>\n","      <td>1.131709</td>\n","      <td>1.131709</td>\n","      <td>1.131709</td>\n","      <td>1.131709</td>\n","      <td>1.131709</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>4.441944</td>\n","      <td>0.332891</td>\n","      <td>1.365953</td>\n","      <td>0.627930</td>\n","      <td>-0.239914</td>\n","      <td>-0.712836</td>\n","      <td>1.583809</td>\n","      <td>1.210077</td>\n","      <td>1.453936</td>\n","      <td>1.470719</td>\n","      <td>...</td>\n","      <td>1.022225</td>\n","      <td>1.022225</td>\n","      <td>1.022225</td>\n","      <td>1.022225</td>\n","      <td>1.022225</td>\n","      <td>1.022225</td>\n","      <td>1.022225</td>\n","      <td>1.022225</td>\n","      <td>1.022225</td>\n","      <td>1.022225</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>4.941816</td>\n","      <td>-0.083558</td>\n","      <td>1.469703</td>\n","      <td>0.711816</td>\n","      <td>-0.249664</td>\n","      <td>-0.995767</td>\n","      <td>1.675934</td>\n","      <td>1.232261</td>\n","      <td>1.573056</td>\n","      <td>1.590914</td>\n","      <td>...</td>\n","      <td>0.993613</td>\n","      <td>0.993613</td>\n","      <td>0.993613</td>\n","      <td>0.993613</td>\n","      <td>0.993613</td>\n","      <td>0.993613</td>\n","      <td>0.993613</td>\n","      <td>0.993613</td>\n","      <td>0.993613</td>\n","      <td>0.993613</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>5.458483</td>\n","      <td>-0.439908</td>\n","      <td>1.593119</td>\n","      <td>0.763294</td>\n","      <td>-0.014556</td>\n","      <td>-1.173434</td>\n","      <td>1.755806</td>\n","      <td>1.293082</td>\n","      <td>1.747336</td>\n","      <td>1.764788</td>\n","      <td>...</td>\n","      <td>1.003666</td>\n","      <td>1.003666</td>\n","      <td>1.003666</td>\n","      <td>1.003666</td>\n","      <td>1.003666</td>\n","      <td>1.003666</td>\n","      <td>1.003666</td>\n","      <td>1.003666</td>\n","      <td>1.003666</td>\n","      <td>1.003666</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>5.979119</td>\n","      <td>-0.755277</td>\n","      <td>1.655974</td>\n","      <td>0.670480</td>\n","      <td>0.336401</td>\n","      <td>-1.270998</td>\n","      <td>1.749752</td>\n","      <td>1.350975</td>\n","      <td>1.921169</td>\n","      <td>1.937463</td>\n","      <td>...</td>\n","      <td>1.038651</td>\n","      <td>1.038651</td>\n","      <td>1.038651</td>\n","      <td>1.038651</td>\n","      <td>1.038651</td>\n","      <td>1.038651</td>\n","      <td>1.038651</td>\n","      <td>1.038651</td>\n","      <td>1.038651</td>\n","      <td>1.038651</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>6.490006</td>\n","      <td>-1.006512</td>\n","      <td>1.580911</td>\n","      <td>0.352295</td>\n","      <td>0.661160</td>\n","      <td>-1.319030</td>\n","      <td>1.582327</td>\n","      <td>1.392717</td>\n","      <td>2.070171</td>\n","      <td>2.081344</td>\n","      <td>...</td>\n","      <td>1.059593</td>\n","      <td>1.059593</td>\n","      <td>1.059593</td>\n","      <td>1.059593</td>\n","      <td>1.059593</td>\n","      <td>1.059593</td>\n","      <td>1.059593</td>\n","      <td>1.059593</td>\n","      <td>1.059593</td>\n","      <td>1.059593</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>6.968133</td>\n","      <td>-1.222418</td>\n","      <td>1.335957</td>\n","      <td>-0.072701</td>\n","      <td>0.788273</td>\n","      <td>-1.485516</td>\n","      <td>1.267325</td>\n","      <td>1.390456</td>\n","      <td>2.157733</td>\n","      <td>2.162774</td>\n","      <td>...</td>\n","      <td>1.039972</td>\n","      <td>1.039972</td>\n","      <td>1.039972</td>\n","      <td>1.039972</td>\n","      <td>1.039972</td>\n","      <td>1.039972</td>\n","      <td>1.039972</td>\n","      <td>1.039972</td>\n","      <td>1.039972</td>\n","      <td>1.039972</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>7.411851</td>\n","      <td>-1.349008</td>\n","      <td>1.055357</td>\n","      <td>-0.517487</td>\n","      <td>0.694644</td>\n","      <td>-1.647017</td>\n","      <td>0.971635</td>\n","      <td>1.381395</td>\n","      <td>2.187262</td>\n","      <td>2.177594</td>\n","      <td>...</td>\n","      <td>1.019893</td>\n","      <td>1.019893</td>\n","      <td>1.019893</td>\n","      <td>1.019893</td>\n","      <td>1.019893</td>\n","      <td>1.019893</td>\n","      <td>1.019893</td>\n","      <td>1.019893</td>\n","      <td>1.019893</td>\n","      <td>1.019893</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>7.823130</td>\n","      <td>-1.385589</td>\n","      <td>0.781906</td>\n","      <td>-0.906479</td>\n","      <td>0.415009</td>\n","      <td>-1.739556</td>\n","      <td>0.679451</td>\n","      <td>1.347985</td>\n","      <td>2.175492</td>\n","      <td>2.145929</td>\n","      <td>...</td>\n","      <td>1.022656</td>\n","      <td>1.022656</td>\n","      <td>1.022656</td>\n","      <td>1.022656</td>\n","      <td>1.022656</td>\n","      <td>1.022656</td>\n","      <td>1.022656</td>\n","      <td>1.022656</td>\n","      <td>1.022656</td>\n","      <td>1.022656</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>8.241763</td>\n","      <td>-1.523832</td>\n","      <td>0.495968</td>\n","      <td>-1.149477</td>\n","      <td>0.028220</td>\n","      <td>-1.793647</td>\n","      <td>0.410195</td>\n","      <td>1.307204</td>\n","      <td>2.104525</td>\n","      <td>2.053375</td>\n","      <td>...</td>\n","      <td>1.014515</td>\n","      <td>1.014515</td>\n","      <td>1.014515</td>\n","      <td>1.014515</td>\n","      <td>1.014515</td>\n","      <td>1.014515</td>\n","      <td>1.014515</td>\n","      <td>1.014515</td>\n","      <td>1.014515</td>\n","      <td>1.014515</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>8.704298</td>\n","      <td>-1.664597</td>\n","      <td>0.240144</td>\n","      <td>-1.190564</td>\n","      <td>-0.428132</td>\n","      <td>-1.815625</td>\n","      <td>0.189352</td>\n","      <td>1.301259</td>\n","      <td>2.032043</td>\n","      <td>1.969257</td>\n","      <td>...</td>\n","      <td>1.011796</td>\n","      <td>1.011796</td>\n","      <td>1.011796</td>\n","      <td>1.011796</td>\n","      <td>1.011796</td>\n","      <td>1.011796</td>\n","      <td>1.011796</td>\n","      <td>1.011796</td>\n","      <td>1.011796</td>\n","      <td>1.011796</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>9.170307</td>\n","      <td>-1.748594</td>\n","      <td>0.005344</td>\n","      <td>-1.094429</td>\n","      <td>-0.922364</td>\n","      <td>-1.825535</td>\n","      <td>-0.034357</td>\n","      <td>1.312565</td>\n","      <td>2.004714</td>\n","      <td>1.945456</td>\n","      <td>...</td>\n","      <td>1.005286</td>\n","      <td>1.005286</td>\n","      <td>1.005286</td>\n","      <td>1.005286</td>\n","      <td>1.005286</td>\n","      <td>1.005286</td>\n","      <td>1.005286</td>\n","      <td>1.005286</td>\n","      <td>1.005286</td>\n","      <td>1.005286</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>9.626358</td>\n","      <td>-1.793868</td>\n","      <td>-0.333325</td>\n","      <td>-0.781577</td>\n","      <td>-1.170820</td>\n","      <td>-1.839692</td>\n","      <td>-0.345917</td>\n","      <td>1.317422</td>\n","      <td>1.973236</td>\n","      <td>1.934422</td>\n","      <td>...</td>\n","      <td>1.000456</td>\n","      <td>1.000456</td>\n","      <td>1.000456</td>\n","      <td>1.000456</td>\n","      <td>1.000456</td>\n","      <td>1.000456</td>\n","      <td>1.000456</td>\n","      <td>1.000456</td>\n","      <td>1.000456</td>\n","      <td>1.000456</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>10.083169</td>\n","      <td>-1.818708</td>\n","      <td>-0.690314</td>\n","      <td>-0.235138</td>\n","      <td>-1.214800</td>\n","      <td>-1.850785</td>\n","      <td>-0.665024</td>\n","      <td>1.314112</td>\n","      <td>1.983895</td>\n","      <td>1.946602</td>\n","      <td>...</td>\n","      <td>0.996543</td>\n","      <td>0.996543</td>\n","      <td>0.996543</td>\n","      <td>0.996543</td>\n","      <td>0.996543</td>\n","      <td>0.996543</td>\n","      <td>0.996543</td>\n","      <td>0.996543</td>\n","      <td>0.996543</td>\n","      <td>0.996543</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>10.543955</td>\n","      <td>-1.832876</td>\n","      <td>-0.960239</td>\n","      <td>0.254929</td>\n","      <td>-1.147520</td>\n","      <td>-1.863052</td>\n","      <td>-0.905731</td>\n","      <td>1.306029</td>\n","      <td>1.975133</td>\n","      <td>1.943418</td>\n","      <td>...</td>\n","      <td>0.992995</td>\n","      <td>0.992995</td>\n","      <td>0.992995</td>\n","      <td>0.992995</td>\n","      <td>0.992995</td>\n","      <td>0.992995</td>\n","      <td>0.992995</td>\n","      <td>0.992995</td>\n","      <td>0.992995</td>\n","      <td>0.992995</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>11.012310</td>\n","      <td>-1.830307</td>\n","      <td>-1.161041</td>\n","      <td>0.604902</td>\n","      <td>-0.897317</td>\n","      <td>-1.873938</td>\n","      <td>-1.123061</td>\n","      <td>1.301821</td>\n","      <td>1.951219</td>\n","      <td>1.918502</td>\n","      <td>...</td>\n","      <td>0.987020</td>\n","      <td>0.987020</td>\n","      <td>0.987020</td>\n","      <td>0.987020</td>\n","      <td>0.987020</td>\n","      <td>0.987020</td>\n","      <td>0.987020</td>\n","      <td>0.987020</td>\n","      <td>0.987020</td>\n","      <td>0.987020</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>11.490853</td>\n","      <td>-1.812771</td>\n","      <td>-1.360878</td>\n","      <td>0.803771</td>\n","      <td>-0.462119</td>\n","      <td>-1.887318</td>\n","      <td>-1.266454</td>\n","      <td>1.323995</td>\n","      <td>1.949044</td>\n","      <td>1.924143</td>\n","      <td>...</td>\n","      <td>1.002380</td>\n","      <td>1.002380</td>\n","      <td>1.002380</td>\n","      <td>1.002380</td>\n","      <td>1.002380</td>\n","      <td>1.002380</td>\n","      <td>1.002380</td>\n","      <td>1.002380</td>\n","      <td>1.002380</td>\n","      <td>1.002380</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>11.972882</td>\n","      <td>-1.738017</td>\n","      <td>-1.717036</td>\n","      <td>0.809980</td>\n","      <td>0.136249</td>\n","      <td>-1.907433</td>\n","      <td>-1.653563</td>\n","      <td>1.342226</td>\n","      <td>1.953604</td>\n","      <td>1.949266</td>\n","      <td>...</td>\n","      <td>1.021721</td>\n","      <td>1.021721</td>\n","      <td>1.021721</td>\n","      <td>1.021721</td>\n","      <td>1.021721</td>\n","      <td>1.021721</td>\n","      <td>1.021721</td>\n","      <td>1.021721</td>\n","      <td>1.021721</td>\n","      <td>1.021721</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>12.453620</td>\n","      <td>-1.543148</td>\n","      <td>-1.891551</td>\n","      <td>0.597400</td>\n","      <td>0.567655</td>\n","      <td>-1.933579</td>\n","      <td>-1.877466</td>\n","      <td>1.350266</td>\n","      <td>1.956860</td>\n","      <td>1.966073</td>\n","      <td>...</td>\n","      <td>1.044224</td>\n","      <td>1.044224</td>\n","      <td>1.044224</td>\n","      <td>1.044224</td>\n","      <td>1.044224</td>\n","      <td>1.044224</td>\n","      <td>1.044224</td>\n","      <td>1.044224</td>\n","      <td>1.044224</td>\n","      <td>1.044224</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>12.918081</td>\n","      <td>-1.215436</td>\n","      <td>-1.961745</td>\n","      <td>0.191233</td>\n","      <td>0.834121</td>\n","      <td>-1.971790</td>\n","      <td>-1.962726</td>\n","      <td>1.346412</td>\n","      <td>1.944442</td>\n","      <td>1.964050</td>\n","      <td>...</td>\n","      <td>1.038683</td>\n","      <td>1.038683</td>\n","      <td>1.038683</td>\n","      <td>1.038683</td>\n","      <td>1.038683</td>\n","      <td>1.038683</td>\n","      <td>1.038683</td>\n","      <td>1.038683</td>\n","      <td>1.038683</td>\n","      <td>1.038683</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>13.374146</td>\n","      <td>-1.030815</td>\n","      <td>-2.007030</td>\n","      <td>-0.413870</td>\n","      <td>0.888772</td>\n","      <td>-2.011466</td>\n","      <td>-2.007923</td>\n","      <td>1.341111</td>\n","      <td>1.958741</td>\n","      <td>1.973480</td>\n","      <td>...</td>\n","      <td>1.026558</td>\n","      <td>1.026558</td>\n","      <td>1.026558</td>\n","      <td>1.026558</td>\n","      <td>1.026558</td>\n","      <td>1.026558</td>\n","      <td>1.026558</td>\n","      <td>1.026558</td>\n","      <td>1.026558</td>\n","      <td>1.026558</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>13.842186</td>\n","      <td>-0.666116</td>\n","      <td>-2.043381</td>\n","      <td>-0.936920</td>\n","      <td>0.713072</td>\n","      <td>-2.043640</td>\n","      <td>-2.044856</td>\n","      <td>1.333787</td>\n","      <td>1.975123</td>\n","      <td>1.978959</td>\n","      <td>...</td>\n","      <td>1.022614</td>\n","      <td>1.022614</td>\n","      <td>1.022614</td>\n","      <td>1.022614</td>\n","      <td>1.022614</td>\n","      <td>1.022614</td>\n","      <td>1.022614</td>\n","      <td>1.022614</td>\n","      <td>1.022614</td>\n","      <td>1.022614</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>14.305241</td>\n","      <td>-0.268238</td>\n","      <td>-2.082867</td>\n","      <td>-1.219329</td>\n","      <td>0.361235</td>\n","      <td>-2.072150</td>\n","      <td>-2.082762</td>\n","      <td>1.315732</td>\n","      <td>1.974259</td>\n","      <td>1.966286</td>\n","      <td>...</td>\n","      <td>1.029202</td>\n","      <td>1.029202</td>\n","      <td>1.029202</td>\n","      <td>1.029202</td>\n","      <td>1.029202</td>\n","      <td>1.029202</td>\n","      <td>1.029202</td>\n","      <td>1.029202</td>\n","      <td>1.029202</td>\n","      <td>1.029202</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>14.773447</td>\n","      <td>0.142130</td>\n","      <td>-2.112854</td>\n","      <td>-1.427811</td>\n","      <td>-0.223614</td>\n","      <td>-2.066993</td>\n","      <td>-2.111967</td>\n","      <td>1.317303</td>\n","      <td>1.971722</td>\n","      <td>1.958439</td>\n","      <td>...</td>\n","      <td>1.042325</td>\n","      <td>1.042325</td>\n","      <td>1.042325</td>\n","      <td>1.042325</td>\n","      <td>1.042325</td>\n","      <td>1.042325</td>\n","      <td>1.042325</td>\n","      <td>1.042325</td>\n","      <td>1.042325</td>\n","      <td>1.042325</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>15.241642</td>\n","      <td>0.518060</td>\n","      <td>-2.143642</td>\n","      <td>-1.321691</td>\n","      <td>-0.794478</td>\n","      <td>-1.940420</td>\n","      <td>-2.146140</td>\n","      <td>1.311600</td>\n","      <td>1.967133</td>\n","      <td>1.949926</td>\n","      <td>...</td>\n","      <td>1.055226</td>\n","      <td>1.055226</td>\n","      <td>1.055226</td>\n","      <td>1.055226</td>\n","      <td>1.055226</td>\n","      <td>1.055226</td>\n","      <td>1.055226</td>\n","      <td>1.055226</td>\n","      <td>1.055226</td>\n","      <td>1.055226</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>15.700771</td>\n","      <td>0.861952</td>\n","      <td>-2.145168</td>\n","      <td>-1.064014</td>\n","      <td>-1.180540</td>\n","      <td>-1.530468</td>\n","      <td>-2.170757</td>\n","      <td>1.299288</td>\n","      <td>1.947541</td>\n","      <td>1.930276</td>\n","      <td>...</td>\n","      <td>1.054437</td>\n","      <td>1.054437</td>\n","      <td>1.054437</td>\n","      <td>1.054437</td>\n","      <td>1.054437</td>\n","      <td>1.054437</td>\n","      <td>1.054437</td>\n","      <td>1.054437</td>\n","      <td>1.054437</td>\n","      <td>1.054437</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>16.148867</td>\n","      <td>1.136051</td>\n","      <td>-2.031051</td>\n","      <td>-0.521997</td>\n","      <td>-1.454344</td>\n","      <td>-1.139038</td>\n","      <td>-2.190956</td>\n","      <td>1.268897</td>\n","      <td>1.912685</td>\n","      <td>1.885200</td>\n","      <td>...</td>\n","      <td>1.033006</td>\n","      <td>1.033006</td>\n","      <td>1.033006</td>\n","      <td>1.033006</td>\n","      <td>1.033006</td>\n","      <td>1.033006</td>\n","      <td>1.033006</td>\n","      <td>1.033006</td>\n","      <td>1.033006</td>\n","      <td>1.033006</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>16.594830</td>\n","      <td>1.337177</td>\n","      <td>-1.504064</td>\n","      <td>0.066949</td>\n","      <td>-1.438534</td>\n","      <td>-0.821310</td>\n","      <td>-2.200004</td>\n","      <td>1.241379</td>\n","      <td>1.870603</td>\n","      <td>1.826644</td>\n","      <td>...</td>\n","      <td>1.009450</td>\n","      <td>1.009450</td>\n","      <td>1.009450</td>\n","      <td>1.009450</td>\n","      <td>1.009450</td>\n","      <td>1.009450</td>\n","      <td>1.009450</td>\n","      <td>1.009450</td>\n","      <td>1.009450</td>\n","      <td>1.009450</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>17.050966</td>\n","      <td>1.526245</td>\n","      <td>-1.010042</td>\n","      <td>0.474849</td>\n","      <td>-1.095749</td>\n","      <td>-0.418915</td>\n","      <td>-2.201171</td>\n","      <td>1.221927</td>\n","      <td>1.836219</td>\n","      <td>1.782140</td>\n","      <td>...</td>\n","      <td>0.985361</td>\n","      <td>0.985361</td>\n","      <td>0.985361</td>\n","      <td>0.985361</td>\n","      <td>0.985361</td>\n","      <td>0.985361</td>\n","      <td>0.985361</td>\n","      <td>0.985361</td>\n","      <td>0.985361</td>\n","      <td>0.985361</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>17.518845</td>\n","      <td>1.648091</td>\n","      <td>-0.574141</td>\n","      <td>0.755716</td>\n","      <td>-0.660435</td>\n","      <td>-0.150772</td>\n","      <td>-2.196371</td>\n","      <td>1.207668</td>\n","      <td>1.812740</td>\n","      <td>1.753622</td>\n","      <td>...</td>\n","      <td>0.962587</td>\n","      <td>0.962587</td>\n","      <td>0.962587</td>\n","      <td>0.962587</td>\n","      <td>0.962587</td>\n","      <td>0.962587</td>\n","      <td>0.962587</td>\n","      <td>0.962587</td>\n","      <td>0.962587</td>\n","      <td>0.962587</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>17.986134</td>\n","      <td>1.683339</td>\n","      <td>-0.173734</td>\n","      <td>0.835238</td>\n","      <td>-0.117456</td>\n","      <td>0.129841</td>\n","      <td>-2.188532</td>\n","      <td>1.189313</td>\n","      <td>1.782250</td>\n","      <td>1.740371</td>\n","      <td>...</td>\n","      <td>0.939099</td>\n","      <td>0.939099</td>\n","      <td>0.939099</td>\n","      <td>0.939099</td>\n","      <td>0.939099</td>\n","      <td>0.939099</td>\n","      <td>0.939099</td>\n","      <td>0.939099</td>\n","      <td>0.939099</td>\n","      <td>0.939099</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>18.444338</td>\n","      <td>1.627908</td>\n","      <td>0.165078</td>\n","      <td>0.676528</td>\n","      <td>0.319953</td>\n","      <td>0.385245</td>\n","      <td>-2.172081</td>\n","      <td>1.157489</td>\n","      <td>1.753652</td>\n","      <td>1.741264</td>\n","      <td>...</td>\n","      <td>0.905069</td>\n","      <td>0.905069</td>\n","      <td>0.905069</td>\n","      <td>0.905069</td>\n","      <td>0.905069</td>\n","      <td>0.905069</td>\n","      <td>0.905069</td>\n","      <td>0.905069</td>\n","      <td>0.905069</td>\n","      <td>0.905069</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>18.899122</td>\n","      <td>1.520819</td>\n","      <td>0.449638</td>\n","      <td>0.387301</td>\n","      <td>0.620823</td>\n","      <td>0.622740</td>\n","      <td>-2.161063</td>\n","      <td>1.122325</td>\n","      <td>1.734101</td>\n","      <td>1.752351</td>\n","      <td>...</td>\n","      <td>0.865020</td>\n","      <td>0.865020</td>\n","      <td>0.865020</td>\n","      <td>0.865020</td>\n","      <td>0.865020</td>\n","      <td>0.865020</td>\n","      <td>0.865020</td>\n","      <td>0.865020</td>\n","      <td>0.865020</td>\n","      <td>0.865020</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>19.349394</td>\n","      <td>1.333614</td>\n","      <td>0.722372</td>\n","      <td>0.121899</td>\n","      <td>0.758902</td>\n","      <td>0.873528</td>\n","      <td>-2.147193</td>\n","      <td>1.074584</td>\n","      <td>1.733176</td>\n","      <td>1.776885</td>\n","      <td>...</td>\n","      <td>0.807092</td>\n","      <td>0.807092</td>\n","      <td>0.807092</td>\n","      <td>0.807092</td>\n","      <td>0.807092</td>\n","      <td>0.807092</td>\n","      <td>0.807092</td>\n","      <td>0.807092</td>\n","      <td>0.807092</td>\n","      <td>0.807092</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>19.807966</td>\n","      <td>1.031000</td>\n","      <td>1.037631</td>\n","      <td>-0.200127</td>\n","      <td>0.673665</td>\n","      <td>1.149066</td>\n","      <td>-2.142428</td>\n","      <td>1.024425</td>\n","      <td>1.738403</td>\n","      <td>1.801731</td>\n","      <td>...</td>\n","      <td>0.766194</td>\n","      <td>0.766194</td>\n","      <td>0.766194</td>\n","      <td>0.766194</td>\n","      <td>0.766194</td>\n","      <td>0.766194</td>\n","      <td>0.766194</td>\n","      <td>0.766194</td>\n","      <td>0.766194</td>\n","      <td>0.766194</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","      <td>...</td>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","      <td>2.091085</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","      <td>...</td>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","      <td>2.086940</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","      <td>...</td>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","      <td>2.089235</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","      <td>...</td>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","      <td>2.099894</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","      <td>...</td>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","      <td>2.118632</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","      <td>...</td>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","      <td>2.134207</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","      <td>...</td>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","      <td>2.152581</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","      <td>...</td>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","      <td>2.174763</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","      <td>...</td>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","      <td>2.191721</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","      <td>...</td>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","      <td>2.206448</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","      <td>...</td>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","      <td>2.220622</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","      <td>...</td>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","      <td>2.238583</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","      <td>...</td>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","      <td>2.249838</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","      <td>...</td>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","      <td>2.252554</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","      <td>...</td>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","      <td>2.243712</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>59 rows × 26 columns</p>\n","</div>"],"text/plain":["           0         1         2         3         4         5         6   \\\n","0    1.170554  1.170554  1.170554  1.170554  1.170554  1.170554  1.170554   \n","1    1.293010  1.293119  1.293099  1.293119  1.293119  1.293119  1.293119   \n","2    1.336501  1.336545  1.336489  1.336489  1.336545  1.336550  1.336524   \n","3    1.343069  1.343064  1.342984  1.343044  1.343052  1.342984  1.343002   \n","4    1.375174  1.375121  1.375313  1.375036  1.375317  1.375072  1.375220   \n","5    1.402991  1.429855  1.429755  1.429083  1.429697  1.429249  1.430241   \n","6    0.383199  1.477023  1.476209  1.465274  1.468350  1.466837  1.482053   \n","7    2.554446  1.451056  1.465172  1.350047  1.338477  1.325754  1.501209   \n","8    3.329627  1.222258  1.371737  0.955009  0.704728  0.637018  1.490662   \n","9    3.917570  0.776517  1.319047  0.659090  0.084370 -0.128863  1.511409   \n","10   4.441944  0.332891  1.365953  0.627930 -0.239914 -0.712836  1.583809   \n","11   4.941816 -0.083558  1.469703  0.711816 -0.249664 -0.995767  1.675934   \n","12   5.458483 -0.439908  1.593119  0.763294 -0.014556 -1.173434  1.755806   \n","13   5.979119 -0.755277  1.655974  0.670480  0.336401 -1.270998  1.749752   \n","14   6.490006 -1.006512  1.580911  0.352295  0.661160 -1.319030  1.582327   \n","15   6.968133 -1.222418  1.335957 -0.072701  0.788273 -1.485516  1.267325   \n","16   7.411851 -1.349008  1.055357 -0.517487  0.694644 -1.647017  0.971635   \n","17   7.823130 -1.385589  0.781906 -0.906479  0.415009 -1.739556  0.679451   \n","18   8.241763 -1.523832  0.495968 -1.149477  0.028220 -1.793647  0.410195   \n","19   8.704298 -1.664597  0.240144 -1.190564 -0.428132 -1.815625  0.189352   \n","20   9.170307 -1.748594  0.005344 -1.094429 -0.922364 -1.825535 -0.034357   \n","21   9.626358 -1.793868 -0.333325 -0.781577 -1.170820 -1.839692 -0.345917   \n","22  10.083169 -1.818708 -0.690314 -0.235138 -1.214800 -1.850785 -0.665024   \n","23  10.543955 -1.832876 -0.960239  0.254929 -1.147520 -1.863052 -0.905731   \n","24  11.012310 -1.830307 -1.161041  0.604902 -0.897317 -1.873938 -1.123061   \n","25  11.490853 -1.812771 -1.360878  0.803771 -0.462119 -1.887318 -1.266454   \n","26  11.972882 -1.738017 -1.717036  0.809980  0.136249 -1.907433 -1.653563   \n","27  12.453620 -1.543148 -1.891551  0.597400  0.567655 -1.933579 -1.877466   \n","28  12.918081 -1.215436 -1.961745  0.191233  0.834121 -1.971790 -1.962726   \n","29  13.374146 -1.030815 -2.007030 -0.413870  0.888772 -2.011466 -2.007923   \n","30  13.842186 -0.666116 -2.043381 -0.936920  0.713072 -2.043640 -2.044856   \n","31  14.305241 -0.268238 -2.082867 -1.219329  0.361235 -2.072150 -2.082762   \n","32  14.773447  0.142130 -2.112854 -1.427811 -0.223614 -2.066993 -2.111967   \n","33  15.241642  0.518060 -2.143642 -1.321691 -0.794478 -1.940420 -2.146140   \n","34  15.700771  0.861952 -2.145168 -1.064014 -1.180540 -1.530468 -2.170757   \n","35  16.148867  1.136051 -2.031051 -0.521997 -1.454344 -1.139038 -2.190956   \n","36  16.594830  1.337177 -1.504064  0.066949 -1.438534 -0.821310 -2.200004   \n","37  17.050966  1.526245 -1.010042  0.474849 -1.095749 -0.418915 -2.201171   \n","38  17.518845  1.648091 -0.574141  0.755716 -0.660435 -0.150772 -2.196371   \n","39  17.986134  1.683339 -0.173734  0.835238 -0.117456  0.129841 -2.188532   \n","40  18.444338  1.627908  0.165078  0.676528  0.319953  0.385245 -2.172081   \n","41  18.899122  1.520819  0.449638  0.387301  0.620823  0.622740 -2.161063   \n","42  19.349394  1.333614  0.722372  0.121899  0.758902  0.873528 -2.147193   \n","43  19.807966  1.031000  1.037631 -0.200127  0.673665  1.149066 -2.142428   \n","44   2.091085  2.091085  2.091085  2.091085  2.091085  2.091085  2.091085   \n","45   2.086940  2.086940  2.086940  2.086940  2.086940  2.086940  2.086940   \n","46   2.089235  2.089235  2.089235  2.089235  2.089235  2.089235  2.089235   \n","47   2.099894  2.099894  2.099894  2.099894  2.099894  2.099894  2.099894   \n","48   2.118632  2.118632  2.118632  2.118632  2.118632  2.118632  2.118632   \n","49   2.134207  2.134207  2.134207  2.134207  2.134207  2.134207  2.134207   \n","50   2.152581  2.152581  2.152581  2.152581  2.152581  2.152581  2.152581   \n","51   2.174763  2.174763  2.174763  2.174763  2.174763  2.174763  2.174763   \n","52   2.191721  2.191721  2.191721  2.191721  2.191721  2.191721  2.191721   \n","53   2.206448  2.206448  2.206448  2.206448  2.206448  2.206448  2.206448   \n","54   2.220622  2.220622  2.220622  2.220622  2.220622  2.220622  2.220622   \n","55   2.238583  2.238583  2.238583  2.238583  2.238583  2.238583  2.238583   \n","56   2.249838  2.249838  2.249838  2.249838  2.249838  2.249838  2.249838   \n","57   2.252554  2.252554  2.252554  2.252554  2.252554  2.252554  2.252554   \n","58   2.243712  2.243712  2.243712  2.243712  2.243712  2.243712  2.243712   \n","\n","          7         8         9   ...        16        17        18        19  \\\n","0   1.170554  1.170554  1.170554  ...  1.170554  1.170554  1.170554  1.170554   \n","1   1.293119  1.293119  1.293119  ...  1.293119  1.293119  1.293119  1.293119   \n","2   1.336545  1.336550  1.336550  ...  1.336545  1.336545  1.336545  1.336545   \n","3   1.343064  1.343072  1.343072  ...  1.343087  1.343087  1.343087  1.343087   \n","4   1.375094  1.375217  1.375252  ...  1.375198  1.375198  1.375198  1.375198   \n","5   1.429848  1.429969  1.430094  ...  1.429824  1.429824  1.429824  1.429824   \n","6   1.477890  1.479513  1.479953  ...  1.476856  1.476856  1.476856  1.476856   \n","7   1.470335  1.484572  1.486199  ...  1.459361  1.459361  1.459361  1.459361   \n","8   1.362946  1.428571  1.433815  ...  1.313279  1.313279  1.313279  1.313279   \n","9   1.249411  1.401388  1.413216  ...  1.131709  1.131709  1.131709  1.131709   \n","10  1.210077  1.453936  1.470719  ...  1.022225  1.022225  1.022225  1.022225   \n","11  1.232261  1.573056  1.590914  ...  0.993613  0.993613  0.993613  0.993613   \n","12  1.293082  1.747336  1.764788  ...  1.003666  1.003666  1.003666  1.003666   \n","13  1.350975  1.921169  1.937463  ...  1.038651  1.038651  1.038651  1.038651   \n","14  1.392717  2.070171  2.081344  ...  1.059593  1.059593  1.059593  1.059593   \n","15  1.390456  2.157733  2.162774  ...  1.039972  1.039972  1.039972  1.039972   \n","16  1.381395  2.187262  2.177594  ...  1.019893  1.019893  1.019893  1.019893   \n","17  1.347985  2.175492  2.145929  ...  1.022656  1.022656  1.022656  1.022656   \n","18  1.307204  2.104525  2.053375  ...  1.014515  1.014515  1.014515  1.014515   \n","19  1.301259  2.032043  1.969257  ...  1.011796  1.011796  1.011796  1.011796   \n","20  1.312565  2.004714  1.945456  ...  1.005286  1.005286  1.005286  1.005286   \n","21  1.317422  1.973236  1.934422  ...  1.000456  1.000456  1.000456  1.000456   \n","22  1.314112  1.983895  1.946602  ...  0.996543  0.996543  0.996543  0.996543   \n","23  1.306029  1.975133  1.943418  ...  0.992995  0.992995  0.992995  0.992995   \n","24  1.301821  1.951219  1.918502  ...  0.987020  0.987020  0.987020  0.987020   \n","25  1.323995  1.949044  1.924143  ...  1.002380  1.002380  1.002380  1.002380   \n","26  1.342226  1.953604  1.949266  ...  1.021721  1.021721  1.021721  1.021721   \n","27  1.350266  1.956860  1.966073  ...  1.044224  1.044224  1.044224  1.044224   \n","28  1.346412  1.944442  1.964050  ...  1.038683  1.038683  1.038683  1.038683   \n","29  1.341111  1.958741  1.973480  ...  1.026558  1.026558  1.026558  1.026558   \n","30  1.333787  1.975123  1.978959  ...  1.022614  1.022614  1.022614  1.022614   \n","31  1.315732  1.974259  1.966286  ...  1.029202  1.029202  1.029202  1.029202   \n","32  1.317303  1.971722  1.958439  ...  1.042325  1.042325  1.042325  1.042325   \n","33  1.311600  1.967133  1.949926  ...  1.055226  1.055226  1.055226  1.055226   \n","34  1.299288  1.947541  1.930276  ...  1.054437  1.054437  1.054437  1.054437   \n","35  1.268897  1.912685  1.885200  ...  1.033006  1.033006  1.033006  1.033006   \n","36  1.241379  1.870603  1.826644  ...  1.009450  1.009450  1.009450  1.009450   \n","37  1.221927  1.836219  1.782140  ...  0.985361  0.985361  0.985361  0.985361   \n","38  1.207668  1.812740  1.753622  ...  0.962587  0.962587  0.962587  0.962587   \n","39  1.189313  1.782250  1.740371  ...  0.939099  0.939099  0.939099  0.939099   \n","40  1.157489  1.753652  1.741264  ...  0.905069  0.905069  0.905069  0.905069   \n","41  1.122325  1.734101  1.752351  ...  0.865020  0.865020  0.865020  0.865020   \n","42  1.074584  1.733176  1.776885  ...  0.807092  0.807092  0.807092  0.807092   \n","43  1.024425  1.738403  1.801731  ...  0.766194  0.766194  0.766194  0.766194   \n","44  2.091085  2.091085  2.091085  ...  2.091085  2.091085  2.091085  2.091085   \n","45  2.086940  2.086940  2.086940  ...  2.086940  2.086940  2.086940  2.086940   \n","46  2.089235  2.089235  2.089235  ...  2.089235  2.089235  2.089235  2.089235   \n","47  2.099894  2.099894  2.099894  ...  2.099894  2.099894  2.099894  2.099894   \n","48  2.118632  2.118632  2.118632  ...  2.118632  2.118632  2.118632  2.118632   \n","49  2.134207  2.134207  2.134207  ...  2.134207  2.134207  2.134207  2.134207   \n","50  2.152581  2.152581  2.152581  ...  2.152581  2.152581  2.152581  2.152581   \n","51  2.174763  2.174763  2.174763  ...  2.174763  2.174763  2.174763  2.174763   \n","52  2.191721  2.191721  2.191721  ...  2.191721  2.191721  2.191721  2.191721   \n","53  2.206448  2.206448  2.206448  ...  2.206448  2.206448  2.206448  2.206448   \n","54  2.220622  2.220622  2.220622  ...  2.220622  2.220622  2.220622  2.220622   \n","55  2.238583  2.238583  2.238583  ...  2.238583  2.238583  2.238583  2.238583   \n","56  2.249838  2.249838  2.249838  ...  2.249838  2.249838  2.249838  2.249838   \n","57  2.252554  2.252554  2.252554  ...  2.252554  2.252554  2.252554  2.252554   \n","58  2.243712  2.243712  2.243712  ...  2.243712  2.243712  2.243712  2.243712   \n","\n","          20        21        22        23        24        25  \n","0   1.170554  1.170554  1.170554  1.170554  1.170554  1.170554  \n","1   1.293119  1.293119  1.293119  1.293119  1.293119  1.293119  \n","2   1.336545  1.336545  1.336545  1.336545  1.336545  1.336545  \n","3   1.343087  1.343087  1.343087  1.343087  1.343087  1.343087  \n","4   1.375198  1.375198  1.375198  1.375198  1.375198  1.375198  \n","5   1.429824  1.429824  1.429824  1.429824  1.429824  1.429824  \n","6   1.476856  1.476856  1.476856  1.476856  1.476856  1.476856  \n","7   1.459361  1.459361  1.459361  1.459361  1.459361  1.459361  \n","8   1.313279  1.313279  1.313279  1.313279  1.313279  1.313279  \n","9   1.131709  1.131709  1.131709  1.131709  1.131709  1.131709  \n","10  1.022225  1.022225  1.022225  1.022225  1.022225  1.022225  \n","11  0.993613  0.993613  0.993613  0.993613  0.993613  0.993613  \n","12  1.003666  1.003666  1.003666  1.003666  1.003666  1.003666  \n","13  1.038651  1.038651  1.038651  1.038651  1.038651  1.038651  \n","14  1.059593  1.059593  1.059593  1.059593  1.059593  1.059593  \n","15  1.039972  1.039972  1.039972  1.039972  1.039972  1.039972  \n","16  1.019893  1.019893  1.019893  1.019893  1.019893  1.019893  \n","17  1.022656  1.022656  1.022656  1.022656  1.022656  1.022656  \n","18  1.014515  1.014515  1.014515  1.014515  1.014515  1.014515  \n","19  1.011796  1.011796  1.011796  1.011796  1.011796  1.011796  \n","20  1.005286  1.005286  1.005286  1.005286  1.005286  1.005286  \n","21  1.000456  1.000456  1.000456  1.000456  1.000456  1.000456  \n","22  0.996543  0.996543  0.996543  0.996543  0.996543  0.996543  \n","23  0.992995  0.992995  0.992995  0.992995  0.992995  0.992995  \n","24  0.987020  0.987020  0.987020  0.987020  0.987020  0.987020  \n","25  1.002380  1.002380  1.002380  1.002380  1.002380  1.002380  \n","26  1.021721  1.021721  1.021721  1.021721  1.021721  1.021721  \n","27  1.044224  1.044224  1.044224  1.044224  1.044224  1.044224  \n","28  1.038683  1.038683  1.038683  1.038683  1.038683  1.038683  \n","29  1.026558  1.026558  1.026558  1.026558  1.026558  1.026558  \n","30  1.022614  1.022614  1.022614  1.022614  1.022614  1.022614  \n","31  1.029202  1.029202  1.029202  1.029202  1.029202  1.029202  \n","32  1.042325  1.042325  1.042325  1.042325  1.042325  1.042325  \n","33  1.055226  1.055226  1.055226  1.055226  1.055226  1.055226  \n","34  1.054437  1.054437  1.054437  1.054437  1.054437  1.054437  \n","35  1.033006  1.033006  1.033006  1.033006  1.033006  1.033006  \n","36  1.009450  1.009450  1.009450  1.009450  1.009450  1.009450  \n","37  0.985361  0.985361  0.985361  0.985361  0.985361  0.985361  \n","38  0.962587  0.962587  0.962587  0.962587  0.962587  0.962587  \n","39  0.939099  0.939099  0.939099  0.939099  0.939099  0.939099  \n","40  0.905069  0.905069  0.905069  0.905069  0.905069  0.905069  \n","41  0.865020  0.865020  0.865020  0.865020  0.865020  0.865020  \n","42  0.807092  0.807092  0.807092  0.807092  0.807092  0.807092  \n","43  0.766194  0.766194  0.766194  0.766194  0.766194  0.766194  \n","44  2.091085  2.091085  2.091085  2.091085  2.091085  2.091085  \n","45  2.086940  2.086940  2.086940  2.086940  2.086940  2.086940  \n","46  2.089235  2.089235  2.089235  2.089235  2.089235  2.089235  \n","47  2.099894  2.099894  2.099894  2.099894  2.099894  2.099894  \n","48  2.118632  2.118632  2.118632  2.118632  2.118632  2.118632  \n","49  2.134207  2.134207  2.134207  2.134207  2.134207  2.134207  \n","50  2.152581  2.152581  2.152581  2.152581  2.152581  2.152581  \n","51  2.174763  2.174763  2.174763  2.174763  2.174763  2.174763  \n","52  2.191721  2.191721  2.191721  2.191721  2.191721  2.191721  \n","53  2.206448  2.206448  2.206448  2.206448  2.206448  2.206448  \n","54  2.220622  2.220622  2.220622  2.220622  2.220622  2.220622  \n","55  2.238583  2.238583  2.238583  2.238583  2.238583  2.238583  \n","56  2.249838  2.249838  2.249838  2.249838  2.249838  2.249838  \n","57  2.252554  2.252554  2.252554  2.252554  2.252554  2.252554  \n","58  2.243712  2.243712  2.243712  2.243712  2.243712  2.243712  \n","\n","[59 rows x 26 columns]"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["df_pred"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.000000</td>\n","      <td>1.560060</td>\n","      <td>-0.854437</td>\n","      <td>0.720639</td>\n","      <td>0.691729</td>\n","      <td>0.944008</td>\n","      <td>2.700632</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.465116</td>\n","      <td>1.689858</td>\n","      <td>-0.514359</td>\n","      <td>0.333295</td>\n","      <td>0.942289</td>\n","      <td>0.681604</td>\n","      <td>2.785811</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.930233</td>\n","      <td>1.753589</td>\n","      <td>-0.154209</td>\n","      <td>-0.124995</td>\n","      <td>0.992368</td>\n","      <td>0.412951</td>\n","      <td>2.845461</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.395349</td>\n","      <td>1.748068</td>\n","      <td>0.212022</td>\n","      <td>-0.556775</td>\n","      <td>0.831727</td>\n","      <td>0.140540</td>\n","      <td>2.879232</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.860465</td>\n","      <td>1.673573</td>\n","      <td>0.569904</td>\n","      <td>-0.870579</td>\n","      <td>0.494812</td>\n","      <td>-0.133144</td>\n","      <td>2.887018</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2.325581</td>\n","      <td>1.533811</td>\n","      <td>0.905607</td>\n","      <td>-1.000151</td>\n","      <td>0.053178</td>\n","      <td>-0.405638</td>\n","      <td>2.868949</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2.790698</td>\n","      <td>1.335526</td>\n","      <td>1.206832</td>\n","      <td>-0.918192</td>\n","      <td>-0.399688</td>\n","      <td>-0.674530</td>\n","      <td>2.825381</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>3.255814</td>\n","      <td>1.087824</td>\n","      <td>1.463513</td>\n","      <td>-0.641980</td>\n","      <td>-0.767958</td>\n","      <td>-0.937474</td>\n","      <td>2.756889</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>3.720930</td>\n","      <td>0.801365</td>\n","      <td>1.668203</td>\n","      <td>-0.229808</td>\n","      <td>-0.973515</td>\n","      <td>-1.192211</td>\n","      <td>2.664256</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>4.186047</td>\n","      <td>0.487549</td>\n","      <td>1.816168</td>\n","      <td>0.231092</td>\n","      <td>-0.972449</td>\n","      <td>-1.436585</td>\n","      <td>2.548458</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>4.651163</td>\n","      <td>0.157822</td>\n","      <td>1.905225</td>\n","      <td>0.642811</td>\n","      <td>-0.764575</td>\n","      <td>-1.668554</td>\n","      <td>2.410650</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>5.116279</td>\n","      <td>-0.176870</td>\n","      <td>1.935436</td>\n","      <td>0.917559</td>\n","      <td>-0.393801</td>\n","      <td>-1.886207</td>\n","      <td>2.252158</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>5.581395</td>\n","      <td>-0.506443</td>\n","      <td>1.908720</td>\n","      <td>0.996574</td>\n","      <td>0.061000</td>\n","      <td>-2.087774</td>\n","      <td>2.074456</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>6.046512</td>\n","      <td>-0.821885</td>\n","      <td>1.828463</td>\n","      <td>0.862921</td>\n","      <td>0.502832</td>\n","      <td>-2.271633</td>\n","      <td>1.879159</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>6.511628</td>\n","      <td>-1.115352</td>\n","      <td>1.699162</td>\n","      <td>0.545212</td>\n","      <td>0.837490</td>\n","      <td>-2.436322</td>\n","      <td>1.668004</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>6.976744</td>\n","      <td>-1.380170</td>\n","      <td>1.526119</td>\n","      <td>0.111347</td>\n","      <td>0.993853</td>\n","      <td>-2.580544</td>\n","      <td>1.442838</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>7.441860</td>\n","      <td>-1.610795</td>\n","      <td>1.315203</td>\n","      <td>-0.346239</td>\n","      <td>0.939015</td>\n","      <td>-2.703172</td>\n","      <td>1.205599</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>7.906977</td>\n","      <td>-1.802741</td>\n","      <td>1.072663</td>\n","      <td>-0.730422</td>\n","      <td>0.685027</td>\n","      <td>-2.803255</td>\n","      <td>0.958309</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>8.372093</td>\n","      <td>-1.952503</td>\n","      <td>0.804994</td>\n","      <td>-0.959957</td>\n","      <td>0.286020</td>\n","      <td>-2.880023</td>\n","      <td>0.703051</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>8.837209</td>\n","      <td>-2.057485</td>\n","      <td>0.518845</td>\n","      <td>-0.986439</td>\n","      <td>-0.173427</td>\n","      <td>-2.932885</td>\n","      <td>0.441961</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>9.302326</td>\n","      <td>-2.115938</td>\n","      <td>0.220954</td>\n","      <td>-0.804305</td>\n","      <td>-0.596103</td>\n","      <td>-2.961438</td>\n","      <td>0.177211</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>9.767442</td>\n","      <td>-2.126912</td>\n","      <td>-0.081893</td>\n","      <td>-0.451965</td>\n","      <td>-0.892498</td>\n","      <td>-2.965464</td>\n","      <td>-0.089004</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>10.232558</td>\n","      <td>-2.090237</td>\n","      <td>-0.382893</td>\n","      <td>-0.003872</td>\n","      <td>-0.999585</td>\n","      <td>-2.944932</td>\n","      <td>-0.354477</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>10.697674</td>\n","      <td>-2.006509</td>\n","      <td>-0.675252</td>\n","      <td>0.444975</td>\n","      <td>-0.894252</td>\n","      <td>-2.899997</td>\n","      <td>-0.617010</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>11.162791</td>\n","      <td>-1.877113</td>\n","      <td>-0.952210</td>\n","      <td>0.799049</td>\n","      <td>-0.598498</td>\n","      <td>-2.831004</td>\n","      <td>-0.874419</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>11.627907</td>\n","      <td>-1.704255</td>\n","      <td>-1.207074</td>\n","      <td>0.982727</td>\n","      <td>-0.175061</td>\n","      <td>-2.738478</td>\n","      <td>-1.124556</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>12.093023</td>\n","      <td>-1.491017</td>\n","      <td>-1.433278</td>\n","      <td>0.956673</td>\n","      <td>0.285841</td>\n","      <td>-2.623132</td>\n","      <td>-1.365315</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>12.558140</td>\n","      <td>-1.241423</td>\n","      <td>-1.624455</td>\n","      <td>0.726448</td>\n","      <td>0.685888</td>\n","      <td>-2.485858</td>\n","      <td>-1.594656</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>13.023256</td>\n","      <td>-0.960518</td>\n","      <td>-1.774551</td>\n","      <td>0.341306</td>\n","      <td>0.939879</td>\n","      <td>-2.327724</td>\n","      <td>-1.810607</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>13.488372</td>\n","      <td>-0.654444</td>\n","      <td>-1.877989</td>\n","      <td>-0.116555</td>\n","      <td>0.994010</td>\n","      <td>-2.149972</td>\n","      <td>-2.011292</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>13.953488</td>\n","      <td>-0.330492</td>\n","      <td>-1.929876</td>\n","      <td>-0.549761</td>\n","      <td>0.837182</td>\n","      <td>-1.954013</td>\n","      <td>-2.194933</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>14.418605</td>\n","      <td>0.002871</td>\n","      <td>-1.926281</td>\n","      <td>-0.866538</td>\n","      <td>0.503039</td>\n","      <td>-1.741415</td>\n","      <td>-2.359874</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>14.883721</td>\n","      <td>0.336052</td>\n","      <td>-1.864565</td>\n","      <td>-1.000002</td>\n","      <td>0.062565</td>\n","      <td>-1.513904</td>\n","      <td>-2.504591</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>15.348837</td>\n","      <td>0.658485</td>\n","      <td>-1.743757</td>\n","      <td>-0.922043</td>\n","      <td>-0.390993</td>\n","      <td>-1.273345</td>\n","      <td>-2.627708</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>15.813953</td>\n","      <td>0.958914</td>\n","      <td>-1.564952</td>\n","      <td>-0.649103</td>\n","      <td>-0.761665</td>\n","      <td>-1.021741</td>\n","      <td>-2.728011</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>16.279070</td>\n","      <td>1.225853</td>\n","      <td>-1.331663</td>\n","      <td>-0.238785</td>\n","      <td>-0.970834</td>\n","      <td>-0.761211</td>\n","      <td>-2.804462</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>16.744186</td>\n","      <td>1.448211</td>\n","      <td>-1.050060</td>\n","      <td>0.222075</td>\n","      <td>-0.973830</td>\n","      <td>-0.493987</td>\n","      <td>-2.856214</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>17.209302</td>\n","      <td>1.616059</td>\n","      <td>-0.729007</td>\n","      <td>0.635585</td>\n","      <td>-0.769602</td>\n","      <td>-0.222387</td>\n","      <td>-2.882622</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>17.674419</td>\n","      <td>1.721456</td>\n","      <td>-0.379819</td>\n","      <td>0.913575</td>\n","      <td>-0.401274</td>\n","      <td>0.051193</td>\n","      <td>-2.883258</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>18.139535</td>\n","      <td>1.759203</td>\n","      <td>-0.015710</td>\n","      <td>0.996587</td>\n","      <td>0.052806</td>\n","      <td>0.324305</td>\n","      <td>-2.857916</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>18.604651</td>\n","      <td>1.727385</td>\n","      <td>0.349052</td>\n","      <td>0.866830</td>\n","      <td>0.495796</td>\n","      <td>0.594465</td>\n","      <td>-2.806627</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>19.069767</td>\n","      <td>1.627587</td>\n","      <td>0.700142</td>\n","      <td>0.552081</td>\n","      <td>0.833240</td>\n","      <td>0.859181</td>\n","      <td>-2.729662</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>19.534884</td>\n","      <td>1.464716</td>\n","      <td>1.024199</td>\n","      <td>0.119611</td>\n","      <td>0.993425</td>\n","      <td>1.115970</td>\n","      <td>-2.627536</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>20.000000</td>\n","      <td>1.246492</td>\n","      <td>1.309683</td>\n","      <td>-0.338443</td>\n","      <td>0.942632</td>\n","      <td>1.362393</td>\n","      <td>-2.501014</td>\n","      <td>1.312562</td>\n","      <td>1.944263</td>\n","      <td>1.897807</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>59 rows × 26 columns</p>\n","</div>"],"text/plain":["           0         1         2         3         4         5         6   \\\n","0    0.000000  1.560060 -0.854437  0.720639  0.691729  0.944008  2.700632   \n","1    0.465116  1.689858 -0.514359  0.333295  0.942289  0.681604  2.785811   \n","2    0.930233  1.753589 -0.154209 -0.124995  0.992368  0.412951  2.845461   \n","3    1.395349  1.748068  0.212022 -0.556775  0.831727  0.140540  2.879232   \n","4    1.860465  1.673573  0.569904 -0.870579  0.494812 -0.133144  2.887018   \n","5    2.325581  1.533811  0.905607 -1.000151  0.053178 -0.405638  2.868949   \n","6    2.790698  1.335526  1.206832 -0.918192 -0.399688 -0.674530  2.825381   \n","7    3.255814  1.087824  1.463513 -0.641980 -0.767958 -0.937474  2.756889   \n","8    3.720930  0.801365  1.668203 -0.229808 -0.973515 -1.192211  2.664256   \n","9    4.186047  0.487549  1.816168  0.231092 -0.972449 -1.436585  2.548458   \n","10   4.651163  0.157822  1.905225  0.642811 -0.764575 -1.668554  2.410650   \n","11   5.116279 -0.176870  1.935436  0.917559 -0.393801 -1.886207  2.252158   \n","12   5.581395 -0.506443  1.908720  0.996574  0.061000 -2.087774  2.074456   \n","13   6.046512 -0.821885  1.828463  0.862921  0.502832 -2.271633  1.879159   \n","14   6.511628 -1.115352  1.699162  0.545212  0.837490 -2.436322  1.668004   \n","15   6.976744 -1.380170  1.526119  0.111347  0.993853 -2.580544  1.442838   \n","16   7.441860 -1.610795  1.315203 -0.346239  0.939015 -2.703172  1.205599   \n","17   7.906977 -1.802741  1.072663 -0.730422  0.685027 -2.803255  0.958309   \n","18   8.372093 -1.952503  0.804994 -0.959957  0.286020 -2.880023  0.703051   \n","19   8.837209 -2.057485  0.518845 -0.986439 -0.173427 -2.932885  0.441961   \n","20   9.302326 -2.115938  0.220954 -0.804305 -0.596103 -2.961438  0.177211   \n","21   9.767442 -2.126912 -0.081893 -0.451965 -0.892498 -2.965464 -0.089004   \n","22  10.232558 -2.090237 -0.382893 -0.003872 -0.999585 -2.944932 -0.354477   \n","23  10.697674 -2.006509 -0.675252  0.444975 -0.894252 -2.899997 -0.617010   \n","24  11.162791 -1.877113 -0.952210  0.799049 -0.598498 -2.831004 -0.874419   \n","25  11.627907 -1.704255 -1.207074  0.982727 -0.175061 -2.738478 -1.124556   \n","26  12.093023 -1.491017 -1.433278  0.956673  0.285841 -2.623132 -1.365315   \n","27  12.558140 -1.241423 -1.624455  0.726448  0.685888 -2.485858 -1.594656   \n","28  13.023256 -0.960518 -1.774551  0.341306  0.939879 -2.327724 -1.810607   \n","29  13.488372 -0.654444 -1.877989 -0.116555  0.994010 -2.149972 -2.011292   \n","30  13.953488 -0.330492 -1.929876 -0.549761  0.837182 -1.954013 -2.194933   \n","31  14.418605  0.002871 -1.926281 -0.866538  0.503039 -1.741415 -2.359874   \n","32  14.883721  0.336052 -1.864565 -1.000002  0.062565 -1.513904 -2.504591   \n","33  15.348837  0.658485 -1.743757 -0.922043 -0.390993 -1.273345 -2.627708   \n","34  15.813953  0.958914 -1.564952 -0.649103 -0.761665 -1.021741 -2.728011   \n","35  16.279070  1.225853 -1.331663 -0.238785 -0.970834 -0.761211 -2.804462   \n","36  16.744186  1.448211 -1.050060  0.222075 -0.973830 -0.493987 -2.856214   \n","37  17.209302  1.616059 -0.729007  0.635585 -0.769602 -0.222387 -2.882622   \n","38  17.674419  1.721456 -0.379819  0.913575 -0.401274  0.051193 -2.883258   \n","39  18.139535  1.759203 -0.015710  0.996587  0.052806  0.324305 -2.857916   \n","40  18.604651  1.727385  0.349052  0.866830  0.495796  0.594465 -2.806627   \n","41  19.069767  1.627587  0.700142  0.552081  0.833240  0.859181 -2.729662   \n","42  19.534884  1.464716  1.024199  0.119611  0.993425  1.115970 -2.627536   \n","43  20.000000  1.246492  1.309683 -0.338443  0.942632  1.362393 -2.501014   \n","44        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","45        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","46        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","47        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","48        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","49        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","50        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","51        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","52        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","53        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","54        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","55        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","56        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","57        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","58        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n","\n","          7         8         9   ...  16  17  18  19  20  21  22  23  24  25  \n","0   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","1   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","2   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","3   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","4   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","5   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","6   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","7   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","8   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","9   1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","10  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","11  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","12  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","13  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","14  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","15  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","16  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","17  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","18  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","19  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","20  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","21  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","22  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","23  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","24  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","25  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","26  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","27  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","28  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","29  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","30  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","31  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","32  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","33  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","34  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","35  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","36  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","37  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","38  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","39  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","40  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","41  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","42  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","43  1.312562  1.944263  1.897807  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","44       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","45       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","46       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","47       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","48       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","49       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","50       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","51       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","52       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","53       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","54       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","55       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","56       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","57       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","58       NaN       NaN       NaN  ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN  \n","\n","[59 rows x 26 columns]"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["df_actual"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_name = f\"big_train_{dt.now()}_\"\n","\n","current_dir = os.getcwd()\n","\n","if not os.path.exists(\"./pre_trained_models/\"):\n","    os.makedirs(\"./pre_trained_models/\")\n","\n","path = os.path.join(current_dir, \"./pre_trained_models/\")\n","\n","\n","ckpt_dir = f\"./pre_trained_models/{model_name}\"\n","\n","# checkpoints.save_checkpoint(\n","#     ckpt_dir=path, target=state, step=batch_count, overwrite=True, prefix=model_name\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vz702qVy_AKz"},"outputs":[{"ename":"AttributeError","evalue":"'ArrayImpl' object has no attribute 'categorical'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m     memory_usage_bytes \u001b[38;5;241m=\u001b[39m bytes_per_element \u001b[38;5;241m*\u001b[39m total_elements\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m memory_usage_bytes\n\u001b[0;32m---> 12\u001b[0m cat_memory_usage \u001b[38;5;241m=\u001b[39m jax_array_memory_usage(\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical\u001b[49m)\n\u001b[1;32m     13\u001b[0m num_memory_usage \u001b[38;5;241m=\u001b[39m jax_array_memory_usage(batch\u001b[38;5;241m.\u001b[39mnumeric)\n\u001b[1;32m     14\u001b[0m memory_usage \u001b[38;5;241m=\u001b[39m cat_memory_usage \u001b[38;5;241m+\u001b[39m num_memory_usage\n","\u001b[0;31mAttributeError\u001b[0m: 'ArrayImpl' object has no attribute 'categorical'"]}],"source":["def jax_array_memory_usage(array):\n","    \"\"\"Calculate the memory usage of a JAX array in bytes.\"\"\"\n","    # Get the number of bytes per element based on the data type\n","    bytes_per_element = array.dtype.itemsize\n","    # Calculate the total number of elements in the array\n","    total_elements = np.prod(array.shape)\n","    # Calculate total memory usage\n","    memory_usage_bytes = bytes_per_element * total_elements\n","    return memory_usage_bytes\n","\n","\n","cat_memory_usage = jax_array_memory_usage(batch.categorical)\n","num_memory_usage = jax_array_memory_usage(batch.numeric)\n","memory_usage = cat_memory_usage + num_memory_usage\n","memory_usage_gb = memory_usage / 1024 / 1024 / 1024\n","print(f\"Memory usage: {memory_usage} bytes\")\n","print(f\"Memory usage: {memory_usage_gb} gb\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[[[False False False]\n","   [ True  True  True]\n","   [ True  True  True]\n","   [ True  True  True]\n","   [ True  True  True]]\n","\n","  [[False False False]\n","   [False False False]\n","   [ True  True  True]\n","   [ True  True  True]\n","   [ True  True  True]]\n","\n","  [[False False False]\n","   [False False False]\n","   [False False False]\n","   [ True  True  True]\n","   [ True  True  True]]\n","\n","  [[False False False]\n","   [False False False]\n","   [False False False]\n","   [False False False]\n","   [ True  True  True]]\n","\n","  [[False False False]\n","   [False False False]\n","   [False False False]\n","   [False False False]\n","   [False False False]]]\n","\n","\n"," [[[False False False]\n","   [ True  True  True]\n","   [ True  True  True]\n","   [ True  True  True]\n","   [ True  True  True]]\n","\n","  [[False False False]\n","   [False False False]\n","   [ True  True  True]\n","   [ True  True  True]\n","   [ True  True  True]]\n","\n","  [[False False False]\n","   [False False False]\n","   [False False False]\n","   [ True  True  True]\n","   [ True  True  True]]\n","\n","  [[False False False]\n","   [False False False]\n","   [False False False]\n","   [False False False]\n","   [ True  True  True]]\n","\n","  [[False False False]\n","   [False False False]\n","   [False False False]\n","   [False False False]\n","   [False False False]]]]\n"]}],"source":["import jax\n","import jax.numpy as jnp\n","\n","\n","def create_future_mask(seq_len):\n","    # Create a basic mask for future sequences\n","    mask = jnp.triu(jnp.ones((seq_len, seq_len)), k=1).astype(bool)\n","    return mask\n","\n","\n","def create_custom_mask(batch_size, seq_len, n_columns):\n","    # Create the future sequence mask\n","    future_mask = create_future_mask(seq_len)\n","\n","    # Expand the mask to match the required dimensions\n","    mask = jnp.expand_dims(future_mask, axis=0)  # Shape: (1, seq_len, seq_len)\n","    mask = jnp.expand_dims(mask, axis=-1)  # Shape: (1, seq_len, seq_len, 1)\n","    mask = jnp.tile(\n","        mask, (batch_size, 1, 1, n_columns)\n","    )  # Shape: (batch_size, seq_len, seq_len, n_columns)\n","\n","    return mask\n","\n","\n","# Example usage:\n","batch_size = 2\n","seq_len = 5\n","n_columns = 3\n","embedding_dim = 4\n","\n","mask = create_custom_mask(batch_size, seq_len, n_columns)\n","\n","print(mask)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import jax\n","import jax.numpy as jnp\n","from jax import random\n","\n","\n","# Create the custom mask function\n","def create_future_mask(seq_len):\n","    # Create a basic mask for future sequences\n","    mask = jnp.triu(jnp.ones((seq_len, seq_len)), k=1).astype(bool)\n","    return mask\n","\n","\n","def create_custom_mask(batch_size, seq_len, n_columns):\n","    # Create the future sequence mask\n","    future_mask = create_future_mask(seq_len)\n","\n","    # Expand the mask to match the required dimensions\n","    mask = jnp.expand_dims(future_mask, axis=0)  # Shape: (1, seq_len, seq_len)\n","    mask = jnp.expand_dims(mask, axis=-1)  # Shape: (1, seq_len, seq_len, 1)\n","    mask = jnp.tile(\n","        mask, (batch_size, 1, 1, n_columns)\n","    )  # Shape: (batch_size, seq_len, seq_len, n_columns)\n","\n","    return mask\n","\n","\n","# Generate random input data\n","key = random.PRNGKey(0)\n","batch_size = 2\n","seq_len = 5\n","n_columns = 3\n","embedding_dim = 4\n","\n","input_data = random.normal(key, (batch_size, seq_len, n_columns, embedding_dim))\n","print(\"Input Data:\")\n","print(input_data)\n","\n","# Create the mask\n","mask = create_custom_mask(batch_size, seq_len, n_columns)\n","print(\"Mask:\")\n","print(mask)\n","\n","# Apply the mask to the input data (for demonstration purposes, we'll just print the mask applied to some dummy attention scores)\n","dummy_attention_scores = random.normal(key, (batch_size, seq_len, seq_len, n_columns))\n","masked_attention_scores = jnp.where(mask, -jnp.inf, dummy_attention_scores)\n","\n","print(\"Dummy Attention Scores:\")\n","print(dummy_attention_scores)\n","\n","print(\"Masked Attention Scores:\")\n","print(masked_attention_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0;31mInit signature:\u001b[0m\n","\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mnum_heads\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupportsDType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mparam_dtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupportsDType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'jax.numpy.float32'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mqkv_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mout_features\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mbroadcast_dropout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdropout_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mprecision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNoneType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mkernel_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mvariance_scaling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x139d34160\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mbias_init\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mzeros\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x11908cee0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mattention_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mdot_product_attention\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x139d4b400\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mdecode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mnormalize_qk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mqkv_dot_general\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mout_dot_general\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mqkv_dot_general_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mout_dot_general_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mparent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Sentinel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mflax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Sentinel\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x139cfebc0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n","\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDocstring:\u001b[0m     \n","Multi-head dot-product attention.\n","Alias for ``MultiHeadDotProductAttention``.\n","\n","**NOTE**: ``MultiHeadAttention`` is a wrapper of ``MultiHeadDotProductAttention``,\n","and so their implementations are identical. However ``MultiHeadAttention`` layers\n","will, by default, be named ``MultiHeadAttention_{index}``, whereas ``MultiHeadDotProductAttention``\n","will be named ``MultiHeadDotProductAttention_{index}``. Therefore, this could affect\n","checkpointing, param collection names and RNG threading (since the layer name is\n","used when generating new RNG's) within the module.\n","\n","Example usage::\n","\n","  >>> import flax.linen as nn\n","  >>> import jax\n","\n","  >>> layer = nn.MultiHeadAttention(num_heads=8, qkv_features=16)\n","  >>> key1, key2, key3, key4, key5, key6 = jax.random.split(jax.random.key(0), 6)\n","  >>> shape = (4, 3, 2, 5)\n","  >>> q, k, v = jax.random.uniform(key1, shape), jax.random.uniform(key2, shape), jax.random.uniform(key3, shape)\n","  >>> variables = layer.init(jax.random.key(0), q)\n","\n","  >>> # different inputs for inputs_q, inputs_k and inputs_v\n","  >>> out = layer.apply(variables, q, k, v)\n","  >>> # equivalent to layer.apply(variables, inputs_q=q, inputs_k=k, inputs_v=k)\n","  >>> out = layer.apply(variables, q, k)\n","  >>> # equivalent to layer.apply(variables, inputs_q=q, inputs_k=q) and layer.apply(variables, inputs_q=q, inputs_k=q, inputs_v=q)\n","  >>> out = layer.apply(variables, q)\n","\n","  >>> attention_kwargs = dict(\n","  ...     num_heads=8,\n","  ...     qkv_features=16,\n","  ...     kernel_init=nn.initializers.ones,\n","  ...     bias_init=nn.initializers.zeros,\n","  ...     dropout_rate=0.5,\n","  ...     deterministic=False,\n","  ...     )\n","  >>> class Module(nn.Module):\n","  ...   attention_kwargs: dict\n","  ...\n","  ...   @nn.compact\n","  ...   def __call__(self, x, dropout_rng=None):\n","  ...     out1 = nn.MultiHeadAttention(**self.attention_kwargs)(x, dropout_rng=dropout_rng)\n","  ...     out2 = nn.MultiHeadAttention(**self.attention_kwargs)(x, dropout_rng=dropout_rng)\n","  ...     return out1, out2\n","  >>> module = Module(attention_kwargs)\n","  >>> variables = module.init({'params': key1, 'dropout': key2}, q)\n","\n","  >>> # out1 and out2 are different.\n","  >>> out1, out2 = module.apply(variables, q, rngs={'dropout': key3})\n","  >>> # out3 and out4 are different.\n","  >>> # out1 and out3 are different. out2 and out4 are different.\n","  >>> out3, out4 = module.apply(variables, q, rngs={'dropout': key4})\n","  >>> # out1 and out2 are the same.\n","  >>> out1, out2 = module.apply(variables, q, dropout_rng=key5)\n","  >>> # out1 and out2 are the same as out3 and out4.\n","  >>> # providing a `dropout_rng` arg will take precedence over the `rngs` arg in `.apply`\n","  >>> out3, out4 = module.apply(variables, q, rngs={'dropout': key6}, dropout_rng=key5)\n","\n","Attributes:\n","  num_heads: number of attention heads. Features (i.e. inputs_q.shape[-1])\n","    should be divisible by the number of heads.\n","  dtype: the dtype of the computation (default: infer from inputs and params)\n","  param_dtype: the dtype passed to parameter initializers (default: float32)\n","  qkv_features: dimension of the key, query, and value.\n","  out_features: dimension of the last projection\n","  broadcast_dropout: bool: use a broadcasted dropout along batch dims.\n","  dropout_rate: dropout rate\n","  deterministic: if false, the attention weight is masked randomly using\n","    dropout, whereas if true, the attention weights are deterministic.\n","  precision: numerical precision of the computation see ``jax.lax.Precision``\n","    for details.\n","  kernel_init: initializer for the kernel of the Dense layers.\n","  bias_init: initializer for the bias of the Dense layers.\n","  use_bias: bool: whether pointwise QKVO dense transforms use bias.\n","  attention_fn: dot_product_attention or compatible function. Accepts query,\n","    key, value, and returns output of shape ``[bs, dim1, dim2, ..., dimN,,\n","    num_heads, value_channels]``\n","  decode: whether to prepare and use an autoregressive cache.\n","  normalize_qk: should QK normalization be applied (arxiv.org/abs/2302.05442).\n","\u001b[0;31mFile:\u001b[0m           ~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py\n","\u001b[0;31mType:\u001b[0m           type\n","\u001b[0;31mSubclasses:\u001b[0m     "]}],"source":["from flax import linen as nn\n","\n","?nn.MultiHeadAttention"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input Data:\n","[[[[-5.47799706e-01 -1.17179680e+00  1.45061789e-02  2.34819144e-01\n","     3.00504017e+00  1.99740767e-01 -6.31268919e-01 -2.68845528e-01\n","    -5.21487474e-01 -1.72483146e+00 -2.67246771e+00 -1.67209733e+00\n","    -1.23799145e-01 -7.75377810e-01  7.31753230e-01 -4.72956657e-01]\n","   [-7.59660363e-01 -1.43394160e+00  9.79405999e-01  2.70364374e-01\n","    -7.26617202e-02  1.33200324e+00  1.11467469e+00  5.46375573e-01\n","    -3.29602033e-01 -6.35213614e-01  8.06641698e-01  1.48840249e+00\n","     6.16176844e-01 -4.41124678e-01  5.02394319e-01 -4.20634806e-01]\n","   [-2.35249791e-02 -1.27048820e-01 -7.40331471e-01 -1.69789433e+00\n","    -5.50638080e-01  3.32435369e-01 -6.02500677e-01 -7.93417037e-01\n","    -1.91679239e+00  4.30762082e-01  2.93178469e-01  3.57544348e-02\n","     2.85551995e-01  1.14142168e+00 -6.61235869e-01  1.30349076e+00]]\n","\n","  [[ 7.65341893e-02 -5.83034575e-01  3.56375240e-02 -1.11470902e+00\n","     6.09569371e-01 -1.52801716e+00  1.40855372e+00 -1.50325167e+00\n","    -1.90268196e-02  1.37721896e+00  1.01575285e-01  7.35573828e-01\n","    -7.44182229e-01 -6.10773265e-01 -1.37155894e-02  7.49079645e-01]\n","   [ 1.16729426e+00  9.47832465e-01 -8.75902995e-02  1.43540037e+00\n","     2.03567296e-01 -6.49375856e-01 -1.14499569e+00  5.55262744e-01\n","    -6.96689665e-01 -6.59611166e-01  1.26914454e+00 -2.01910585e-01\n","     1.20939435e-02 -8.60836446e-01 -6.29101872e-01 -1.26618731e+00]\n","   [-2.29058251e-01  2.14313731e-01 -9.98690546e-01  3.07292295e+00\n","     8.91804874e-01 -8.39625478e-01 -2.47639561e+00  1.25158325e-01\n","    -1.26054823e+00  2.00455189e-02  1.84192151e-01  1.27778888e+00\n","    -4.27830726e-01  1.40636250e-01 -3.03655230e-02  9.94564891e-01]]\n","\n","  [[-4.26839054e-01 -1.31841564e+00  2.12921351e-02 -1.13130033e+00\n","    -8.87557119e-02 -1.60422766e+00 -1.29964411e+00 -2.36143488e-02\n","    -8.03564370e-01 -1.38650641e-01 -2.79372156e-01  1.14396751e+00\n","     1.53220305e-02 -5.14247715e-01 -1.96691751e+00 -1.34604469e-01]\n","   [ 4.32796031e-01 -2.88100958e-01 -9.03109431e-01  5.05412340e-01\n","    -2.29584381e-01  8.81282806e-01  1.62935090e+00  6.68148637e-01\n","     1.38726163e+00  1.37816763e+00  5.73498487e-01 -1.76900423e+00\n","     5.86095035e-01 -2.48563290e+00  9.43803370e-01 -9.23078775e-01]\n","   [ 7.28776217e-01  6.11684382e-01 -8.76088321e-01 -2.35505253e-01\n","     5.72645701e-02 -8.46090689e-02  1.09683156e+00 -3.53859104e-02\n","    -1.28413618e+00 -5.97052932e-01  1.14283180e+00 -4.17192310e-01\n","    -2.70076931e-01 -1.86461449e+00  3.05952460e-01 -4.18166190e-01]]\n","\n","  [[ 1.44522059e+00  1.64912164e+00 -7.56208837e-01  5.34757972e-01\n","    -2.73642004e-01 -8.95813257e-02 -1.04655921e+00 -5.23219824e-01\n","    -1.22036362e+00  1.34282291e+00 -2.11059317e-01 -1.98725924e-01\n","    -4.71270829e-02 -9.41687942e-01  5.07678270e-01  9.22263920e-01]\n","   [-1.02429318e+00 -1.09060490e+00 -5.73331773e-01 -2.11118296e-01\n","    -3.39075446e-01 -5.28774381e-01 -2.20028353e+00  2.52022833e-01\n","     2.83047974e-01 -1.32400799e+00 -1.34488463e-03  2.74970587e-02\n","     9.03180122e-01  4.39890772e-01 -1.04267824e+00  3.93869549e-01]\n","   [ 1.03338212e-02 -8.50608766e-01  1.10901201e+00 -2.46981367e-01\n","    -6.83523893e-01  5.87789357e-01 -9.90404487e-02 -3.85973677e-02\n","     1.02486563e+00  1.41465291e-01 -7.02347100e-01  5.33523560e-01\n","    -7.11001575e-01  1.38313800e-01  4.06129807e-01  3.89108121e-01]]\n","\n","  [[ 1.51913390e-01  4.76801693e-01 -1.01600420e+00 -8.82864177e-01\n","     1.72873759e+00  6.46490276e-01 -5.12281835e-01 -1.61252022e-01\n","    -9.83342111e-01  1.15742540e+00  1.37689519e+00 -1.10410190e+00\n","    -2.04914427e+00 -1.95826247e-01  1.91862571e+00 -1.90753222e-01]\n","   [-1.02089787e+00  1.98495656e-01  1.28144848e+00 -7.18914986e-01\n","     7.41300404e-01  1.10979998e+00  1.65606886e-01  2.62002647e-01\n","    -1.63593483e+00  1.71451676e+00  5.42055726e-01 -2.21169099e-01\n","     1.66384590e+00  8.35620105e-01 -1.07018888e+00  5.10219753e-01]\n","   [ 1.67486739e+00  6.18863344e-01 -1.04402602e+00  7.32049704e-01\n","    -1.30217361e+00 -4.35524791e-01 -2.62460202e-01  1.00086892e+00\n","    -2.18341812e-01 -4.92205441e-01  2.60721356e-01 -5.64928949e-01\n","     1.16850710e+00 -8.29534948e-01  2.40424323e+00 -1.90497309e-01]]]\n","\n","\n"," [[[-1.17544866e+00 -5.48851155e-02  4.22603786e-02 -9.39402223e-01\n","     1.19028628e+00  6.26563072e-01 -6.09275997e-01 -5.55905521e-01\n","     3.07423204e-01 -1.43776655e+00 -1.11876059e+00 -6.92955732e-01\n","    -3.66843373e-01 -5.77220023e-01 -1.76732528e+00 -5.36239088e-01]\n","   [-1.52670097e+00  2.10521555e+00  2.44599628e+00  9.18931007e-01\n","    -2.62253940e-01 -5.68299472e-01 -5.81631541e-01  6.20614350e-01\n","     5.50635517e-01  2.74599284e-01  4.67253476e-01 -1.10525206e-01\n","     1.07619023e+00  1.48619711e+00 -4.15436402e-02 -4.93129492e-01]\n","   [-1.15018487e+00  1.39044428e+00  1.21777141e+00 -1.12656450e+00\n","    -2.26464343e+00 -2.13983357e-01 -1.28403401e+00  8.43076110e-01\n","    -1.39442325e-01  4.79474306e-01  2.97692697e-02  1.01177029e-01\n","    -5.35952747e-01  1.29347610e+00 -2.72579998e-01  1.19361842e+00]]\n","\n","  [[ 1.78025946e-01 -1.24072433e+00 -1.24420023e+00 -7.88696587e-01\n","    -1.24152648e+00  8.79596531e-01 -8.59584987e-01  5.26383221e-01\n","    -1.23334563e+00  2.34329653e+00  1.34136105e+00 -1.19314837e+00\n","    -5.59304714e-01 -1.01250517e+00 -1.15559116e-01  2.71430075e-01]\n","   [ 8.51941943e-01 -2.61823058e+00  2.40780830e-01 -6.89047337e-01\n","    -4.41132605e-01 -6.86932445e-01  3.18634421e-01  1.11395156e+00\n","    -8.35723430e-03  1.87544599e-01  3.90780754e-02 -1.57513559e+00\n","    -1.47010815e+00  5.99129558e-01  7.97059953e-01  3.26195434e-02]\n","   [-2.98961550e-01  1.48740637e+00 -3.82947892e-01  1.14175880e+00\n","    -3.67538333e-01 -1.28722620e+00  8.72689664e-01  6.97225928e-01\n","    -1.00847840e+00  1.60017836e+00  7.27395475e-01  3.37876618e-01\n","     4.25743312e-01 -4.18708235e-01  2.54765630e-01 -1.64596891e+00]]\n","\n","  [[ 1.17007220e+00 -2.85560250e-01  8.03896785e-02  1.06136692e+00\n","     6.25034332e-01  6.90889597e-01 -3.52775425e-01  1.00555420e+00\n","    -7.11774349e-01  3.81468028e-01 -9.88594592e-01  5.26896000e-01\n","     1.23935473e+00  2.66446471e+00  1.42381716e+00  1.09026179e-01]\n","   [-3.75006080e-01 -8.10260057e-01 -1.37713313e+00 -1.14200628e+00\n","    -2.31188595e-01 -7.40914106e-01  1.16216815e+00  1.53247893e+00\n","     2.25940440e-03 -1.72462237e+00  2.02629972e+00 -2.57482409e-01\n","    -5.32267630e-01 -6.84871256e-01 -1.75036514e+00 -2.82968432e-02]\n","   [ 1.96031988e+00 -1.53299510e+00  6.99366212e-01 -1.46556711e+00\n","     5.78366339e-01 -8.32008839e-01  1.45459580e+00  1.24557531e+00\n","    -2.77160287e-01 -2.00246349e-01  1.55093566e-01  2.98580498e-01\n","    -1.18777287e+00 -8.52076173e-01 -2.16620350e+00  2.31011868e+00]]\n","\n","  [[-2.05782633e-02 -1.59692302e-01 -4.24988836e-01  9.43649888e-01\n","     1.41065109e+00 -2.37810683e+00 -6.81668758e-01  1.50250053e+00\n","     1.03178585e+00  6.43335998e-01 -1.02595067e+00 -1.47315252e+00\n","     6.29622936e-01 -1.09131232e-01  7.66469061e-01  1.53373218e+00]\n","   [-6.64848804e-01  4.23595160e-01  1.78632402e+00 -1.13268232e+00\n","    -6.82671487e-01  1.91094875e+00 -1.32285452e+00  2.75485635e-01\n","    -2.42395353e+00 -9.20824051e-01  3.77879590e-01 -5.66891693e-02\n","     7.68425688e-02 -1.57620870e-02 -2.06549072e+00  1.88852146e-01]\n","   [ 4.37522344e-02 -6.57358587e-01 -4.76909764e-02  6.15530252e-01\n","     6.57291710e-01  8.83487642e-01 -2.92197406e-01  5.73906779e-01\n","     1.74268353e+00  3.74617696e-01 -3.24656463e+00  1.78566039e-01\n","     1.55177820e+00  3.41335297e-01  2.62867391e-01  5.86450934e-01]]\n","\n","  [[-6.82086170e-01  1.48331121e-01  1.01442468e+00  1.09393620e+00\n","     1.71937191e+00 -3.05081576e-01  8.12685311e-01  6.35167778e-01\n","     6.74835026e-01 -1.08194244e+00 -4.07744199e-01 -5.79516403e-02\n","    -6.18456125e-01  8.59635890e-01  1.58947989e-01 -1.56896442e-01]\n","   [-1.05287802e+00  9.88370299e-01 -9.71435189e-01  8.59063506e-01\n","    -1.86965708e-02 -9.40730609e-03 -1.60416067e+00  5.51001310e-01\n","     7.77331412e-01 -9.69766319e-01  2.54476130e-01  3.56859773e-01\n","     1.22686468e-01 -9.85618770e-01  6.24602139e-01 -6.67222321e-01]\n","   [-5.12909532e-01 -8.08211267e-01  1.81276202e+00  2.86694139e-01\n","    -6.36618197e-01 -9.69957590e-01 -3.89256239e-01 -1.40181601e+00\n","     1.18410575e+00  6.15966260e-01  1.01148725e+00  8.69927526e-01\n","     9.67692792e-01 -8.70647803e-02 -1.00636196e+00 -1.79708213e-01]]]]\n","Mask:\n","[[[[False  True  True  True  True]\n","   [False False  True  True  True]\n","   [False False False  True  True]\n","   [False False False False  True]\n","   [False False False False False]]\n","\n","  [[False  True  True  True  True]\n","   [False False  True  True  True]\n","   [False False False  True  True]\n","   [False False False False  True]\n","   [False False False False False]]\n","\n","  [[False  True  True  True  True]\n","   [False False  True  True  True]\n","   [False False False  True  True]\n","   [False False False False  True]\n","   [False False False False False]]\n","\n","  [[False  True  True  True  True]\n","   [False False  True  True  True]\n","   [False False False  True  True]\n","   [False False False False  True]\n","   [False False False False False]]]\n","\n","\n"," [[[False  True  True  True  True]\n","   [False False  True  True  True]\n","   [False False False  True  True]\n","   [False False False False  True]\n","   [False False False False False]]\n","\n","  [[False  True  True  True  True]\n","   [False False  True  True  True]\n","   [False False False  True  True]\n","   [False False False False  True]\n","   [False False False False False]]\n","\n","  [[False  True  True  True  True]\n","   [False False  True  True  True]\n","   [False False False  True  True]\n","   [False False False False  True]\n","   [False False False False False]]\n","\n","  [[False  True  True  True  True]\n","   [False False  True  True  True]\n","   [False False False  True  True]\n","   [False False False False  True]\n","   [False False False False False]]]]\n"]},{"ename":"ValueError","evalue":"Incompatible shapes for broadcasting: shapes=[(2, 4, 5, 5), (), (2, 4, 15, 15)]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/util.py:287\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 287\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/util.py:280\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 280\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:155\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 155\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(2, 4, 5, 5), (), (2, 4, 15, 15)]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Initialize and apply the model\u001b[39;00m\n\u001b[1;32m     82\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(n_heads\u001b[38;5;241m=\u001b[39mnum_heads, embedding_dim\u001b[38;5;241m=\u001b[39membedding_dim)\n\u001b[0;32m---> 83\u001b[0m variables \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(variables, input_data, mask)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n","Cell \u001b[0;32mIn[27], line 28\u001b[0m, in \u001b[0;36mMyModel.__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     25\u001b[0m attn_logits \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbthd,bThd->bhtT\u001b[39m\u001b[38;5;124m\"\u001b[39m, q, k)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Apply mask: broadcast mask to match attention logits shape\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m attn_logits \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Compute attention weights\u001b[39;00m\n\u001b[1;32m     31\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39msoftmax(attn_logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:1141\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(acondition, if_true, if_false, size, fill_value, condition, x, y)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize and fill_value arguments cannot be used in three-term where function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_where\u001b[49m\u001b[43m(\u001b[49m\u001b[43macondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_false\u001b[49m\u001b[43m)\u001b[49m\n","    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:448\u001b[0m, in \u001b[0;36m_where\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    446\u001b[0m   condition \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mne(condition, lax\u001b[38;5;241m.\u001b[39m_zero(condition))\n\u001b[1;32m    447\u001b[0m x, y \u001b[38;5;241m=\u001b[39m promote_dtypes(x, y)\n\u001b[0;32m--> 448\u001b[0m condition_arr, x_arr, y_arr \u001b[38;5;241m=\u001b[39m \u001b[43m_broadcast_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m   is_always_empty \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mis_empty_shape(x_arr\u001b[38;5;241m.\u001b[39mshape)\n","    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:407\u001b[0m, in \u001b[0;36m_broadcast_arrays\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shapes \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(core\u001b[38;5;241m.\u001b[39mdefinitely_equal_shape(shapes[\u001b[38;5;241m0\u001b[39m], s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m shapes):\n\u001b[1;32m    406\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [lax\u001b[38;5;241m.\u001b[39masarray(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 407\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, result_shape) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    169\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(2, 4, 5, 5), (), (2, 4, 15, 15)]"]}],"source":["import jax\n","import jax.numpy as jnp\n","from jax import random\n","from flax import linen as nn\n","\n","\n","class MyModel(nn.Module):\n","    n_heads: int\n","    embedding_dim: int\n","\n","    @nn.compact\n","    def __call__(self, x, mask):\n","        batch_size, seq_len, n_columns, embedding_dim = x.shape\n","\n","        # Merge seq_len and n_columns dimensions\n","        x_reshaped = x.reshape(batch_size, seq_len * n_columns, embedding_dim)\n","\n","        # Project the inputs to query, key, and value\n","        qkv_features = self.embedding_dim // self.n_heads\n","        q = nn.DenseGeneral(features=(self.n_heads, qkv_features))(x_reshaped)\n","        k = nn.DenseGeneral(features=(self.n_heads, qkv_features))(x_reshaped)\n","        v = nn.DenseGeneral(features=(self.n_heads, qkv_features))(x_reshaped)\n","\n","        # Compute attention logits\n","        attn_logits = jnp.einsum(\"bthd,bThd->bhtT\", q, k)\n","\n","        # Apply mask: broadcast mask to match attention logits shape\n","        attn_logits = jnp.where(mask, -jnp.inf, attn_logits)\n","\n","        # Compute attention weights\n","        attn_weights = nn.softmax(attn_logits, axis=-1)\n","\n","        # Compute the attention output\n","        attn_output = jnp.einsum(\"bhtT,bThd->bthd\", attn_weights, v)\n","\n","        # Combine heads and reshape back to original dimensions\n","        attn_output = attn_output.reshape(\n","            batch_size, seq_len, n_columns, self.embedding_dim\n","        )\n","\n","        return attn_output\n","\n","\n","def create_future_mask(seq_len):\n","    # Create a basic mask for future sequences\n","    mask = jnp.triu(jnp.ones((seq_len, seq_len)), k=1).astype(bool)\n","    return mask\n","\n","\n","def create_custom_mask(batch_size, seq_len, n_columns, num_heads):\n","    # Create the future sequence mask\n","    future_mask = create_future_mask(seq_len)\n","\n","    # Expand the mask to match the required dimensions for attention\n","    mask = jnp.expand_dims(future_mask, axis=0)  # Shape: (1, seq_len, seq_len)\n","    mask = jnp.expand_dims(mask, axis=1)  # Shape: (1, 1, seq_len, seq_len)\n","    mask = jnp.tile(\n","        mask, (batch_size, num_heads, 1, 1)\n","    )  # Shape: (batch_size, num_heads, seq_len, seq_len)\n","\n","    return mask\n","\n","\n","# Generate random input data\n","key = random.PRNGKey(0)\n","batch_size = 2\n","seq_len = 5\n","n_columns = 3\n","embedding_dim = 16\n","num_heads = 4\n","\n","input_data = random.normal(key, (batch_size, seq_len, n_columns, embedding_dim))\n","print(\"Input Data:\")\n","print(input_data)\n","\n","# Create the mask\n","mask = create_custom_mask(batch_size, seq_len, n_columns, num_heads)\n","print(\"Mask:\")\n","print(mask)\n","\n","# Initialize and apply the model\n","model = MyModel(n_heads=num_heads, embedding_dim=embedding_dim)\n","variables = model.init(key, input_data, mask)\n","output = model.apply(variables, input_data, mask)\n","\n","print(\"Output:\")\n","print(output)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Incompatible shapes for broadcasting: shapes=[(2, 4, 5, 5), (2, 5, 4, 3, 3), ()]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/util.py:287\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 287\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/util.py:280\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 280\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:155\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 155\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(2, 4, 5, 5), (2, 5, 4, 3, 3), ()]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 58\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# print(\"Mask:\")\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# print(mask)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Initialize and apply the model\u001b[39;00m\n\u001b[1;32m     57\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(n_heads\u001b[38;5;241m=\u001b[39mnum_heads)\n\u001b[0;32m---> 58\u001b[0m variables \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(variables, input_data, mask)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n","Cell \u001b[0;32mIn[31], line 13\u001b[0m, in \u001b[0;36mMyModel.__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Apply multi-head attention with the custom mask\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiHeadAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqkv_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn\n","    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:546\u001b[0m, in \u001b[0;36mMultiHeadDotProductAttention.__call__\u001b[0;34m(self, inputs_q, inputs_k, inputs_v, inputs_kv, mask, deterministic, dropout_rng, sow_weights)\u001b[0m\n\u001b[1;32m    532\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_fn(\n\u001b[1;32m    533\u001b[0m     query,\n\u001b[1;32m    534\u001b[0m     key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    544\u001b[0m   )  \u001b[38;5;66;03m# pytype: disable=wrong-keyword-args\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 546\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm_deterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# back to the original inputs dimensions\u001b[39;00m\n\u001b[1;32m    559\u001b[0m out \u001b[38;5;241m=\u001b[39m DenseGeneral(\n\u001b[1;32m    560\u001b[0m   features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    561\u001b[0m   axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    570\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    571\u001b[0m )(x)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:198\u001b[0m, in \u001b[0;36mdot_product_attention\u001b[0;34m(query, key, value, bias, mask, broadcast_dropout, dropout_rng, dropout_rate, deterministic, dtype, precision, module)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m key\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk, v lengths must match.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# compute attention weights\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mdot_product_attention_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m  \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m  \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m  \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# return weighted sum over values for each query position\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[1;32m    214\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...hqk,...khd->...qhd\u001b[39m\u001b[38;5;124m'\u001b[39m, attn_weights, value, precision\u001b[38;5;241m=\u001b[39mprecision\n\u001b[1;32m    215\u001b[0m )\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:112\u001b[0m, in \u001b[0;36mdot_product_attention_weights\u001b[0;34m(query, key, bias, mask, broadcast_dropout, dropout_rng, dropout_rate, deterministic, dtype, precision, module)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m   big_neg \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mfinfo(dtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[0;32m--> 112\u001b[0m   attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbig_neg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# normalize the attention weights\u001b[39;00m\n\u001b[1;32m    115\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(attn_weights)\u001b[38;5;241m.\u001b[39mastype(dtype)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:1141\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(acondition, if_true, if_false, size, fill_value, condition, x, y)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize and fill_value arguments cannot be used in three-term where function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_where\u001b[49m\u001b[43m(\u001b[49m\u001b[43macondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_false\u001b[49m\u001b[43m)\u001b[49m\n","    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:448\u001b[0m, in \u001b[0;36m_where\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    446\u001b[0m   condition \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mne(condition, lax\u001b[38;5;241m.\u001b[39m_zero(condition))\n\u001b[1;32m    447\u001b[0m x, y \u001b[38;5;241m=\u001b[39m promote_dtypes(x, y)\n\u001b[0;32m--> 448\u001b[0m condition_arr, x_arr, y_arr \u001b[38;5;241m=\u001b[39m \u001b[43m_broadcast_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m   is_always_empty \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mis_empty_shape(x_arr\u001b[38;5;241m.\u001b[39mshape)\n","    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:407\u001b[0m, in \u001b[0;36m_broadcast_arrays\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shapes \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(core\u001b[38;5;241m.\u001b[39mdefinitely_equal_shape(shapes[\u001b[38;5;241m0\u001b[39m], s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m shapes):\n\u001b[1;32m    406\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [lax\u001b[38;5;241m.\u001b[39masarray(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 407\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, result_shape) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    169\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(2, 4, 5, 5), (2, 5, 4, 3, 3), ()]"]}],"source":["import jax\n","import jax.numpy as jnp\n","from jax import random\n","from flax import linen as nn\n","\n","\n","class MyModel(nn.Module):\n","    n_heads: int\n","\n","    @nn.compact\n","    def __call__(self, x, mask):\n","        # Apply multi-head attention with the custom mask\n","        attn = nn.MultiHeadAttention(\n","            num_heads=self.n_heads, qkv_features=16, use_bias=False\n","        )(x, mask=mask)\n","        return attn\n","\n","\n","def create_future_mask(seq_len):\n","    # Create a basic mask for future sequences\n","    mask = jnp.triu(jnp.ones((seq_len, seq_len)), k=1).astype(bool)\n","    return mask\n","\n","\n","def create_custom_mask(batch_size, seq_len, num_heads):\n","    # Create the future sequence mask\n","    future_mask = create_future_mask(seq_len)\n","\n","    # Expand the mask to match the required dimensions for attention\n","    mask = jnp.expand_dims(future_mask, axis=0)  # Shape: (1, seq_len, seq_len)\n","    mask = jnp.expand_dims(mask, axis=1)  # Shape: (1, 1, seq_len, seq_len)\n","    mask = jnp.tile(\n","        mask, (batch_size, num_heads, 1, 1)\n","    )  # Shape: (batch_size, num_heads, seq_len, seq_len)\n","\n","    return mask\n","\n","\n","# Generate random input data\n","key = random.PRNGKey(0)\n","batch_size = 2\n","seq_len = 5\n","n_columns = 3\n","embedding_dim = 16\n","num_heads = 4\n","\n","input_data = random.normal(key, (batch_size, seq_len, n_columns, embedding_dim))\n","# print(\"Input Data:\")\n","# print(input_data)\n","\n","# Create the mask\n","mask = create_custom_mask(batch_size, seq_len, num_heads)\n","# print(\"Mask:\")\n","# print(mask)\n","\n","# Initialize and apply the model\n","model = MyModel(n_heads=num_heads)\n","variables = model.init(key, input_data, mask)\n","output = model.apply(variables, input_data, mask)\n","\n","print(\"Output:\")\n","print(output)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(2, 15, 1, 16, 16)\n"]},{"ename":"TypeError","evalue":"cannot reshape array of shape (2, 15, 1, 16, 16) (size 7680) into shape (2, 5, 3, 5, 3) (size 450)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[46], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Initialize and apply the model\u001b[39;00m\n\u001b[1;32m     45\u001b[0m model \u001b[38;5;241m=\u001b[39m MyModel(n_heads\u001b[38;5;241m=\u001b[39mnum_heads)\n\u001b[0;32m---> 46\u001b[0m variables \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(variables, input_data, mask)\n","    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n","Cell \u001b[0;32mIn[46], line 20\u001b[0m, in \u001b[0;36mMyModel.__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# ValueError: Incompatible shapes for broadcasting: shapes=[(2, 15, 1, 16, 16),\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#                                                           (2, 4, 15, 15), ()]\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Apply multi-head attention with the custom mask\u001b[39;00m\n\u001b[1;32m     22\u001b[0m attn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMultiHeadAttention(\n\u001b[1;32m     23\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads, qkv_features\u001b[38;5;241m=\u001b[39membedding_dim, use_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     24\u001b[0m )(x, mask\u001b[38;5;241m=\u001b[39mmask)\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:136\u001b[0m, in \u001b[0;36m_compute_newshape\u001b[0;34m(a, newshape)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(d, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mshape(a), \u001b[38;5;241m*\u001b[39mnewshape)) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    135\u001b[0m       np\u001b[38;5;241m.\u001b[39msize(a) \u001b[38;5;241m!=\u001b[39m math\u001b[38;5;241m.\u001b[39mprod(newshape)):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot reshape array of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mshape(a)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39msize(a)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minto shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morig_newshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmath\u001b[38;5;241m.\u001b[39mprod(newshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;241m-\u001b[39mcore\u001b[38;5;241m.\u001b[39mdivide_shape_sizes(np\u001b[38;5;241m.\u001b[39mshape(a), newshape)\n\u001b[1;32m    139\u001b[0m              \u001b[38;5;28;01mif\u001b[39;00m core\u001b[38;5;241m.\u001b[39mdefinitely_equal(d, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m newshape)\n","\u001b[0;31mTypeError\u001b[0m: cannot reshape array of shape (2, 15, 1, 16, 16) (size 7680) into shape (2, 5, 3, 5, 3) (size 450)"]}],"source":["import jax\n","import jax.numpy as jnp\n","from jax import random\n","from flax import linen as nn\n","\n","\n","class MyModel(nn.Module):\n","    n_heads: int\n","\n","    @nn.compact\n","    def __call__(self, x, mask):\n","        # Reshape input to combine seq_len and n_columns\n","        batch_size, seq_len, n_columns, embedding_dim = x.shape\n","        x = x.reshape(batch_size, seq_len * n_columns, embedding_dim)\n","\n","        mask = nn.make_causal_mask(x)\n","        print(mask.shape)  #                                        (2, 15, 1, 16, 16)\n","        # ValueError: Incompatible shapes for broadcasting: shapes=[(2, 15, 1, 16, 16),\n","        #                                                           (2, 4, 15, 15), ()]\n","        mask = mask.reshape(batch_size, seq_len, n_columns, seq_len, n_columns)\n","        # Apply multi-head attention with the custom mask\n","        attn = nn.MultiHeadAttention(\n","            num_heads=self.n_heads, qkv_features=embedding_dim, use_bias=False\n","        )(x, mask=mask)\n","        print(attn.shape)\n","        # Reshape the output back to the original shape\n","        attn = attn.reshape(batch_size, seq_len, n_columns, embedding_dim)\n","        return attn\n","\n","\n","# Generate random input data\n","key = random.PRNGKey(0)\n","batch_size = 2\n","seq_len = 5\n","n_columns = 3\n","embedding_dim = 16\n","num_heads = 4\n","\n","input_data = random.normal(key, (batch_size, seq_len, n_columns, embedding_dim))\n","\n","# Create the mask\n","mask = create_custom_mask(batch_size, seq_len, num_heads, n_columns)\n","\n","# Initialize and apply the model\n","model = MyModel(n_heads=num_heads)\n","variables = model.init(key, input_data, mask)\n","output = model.apply(variables, input_data, mask)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["(2, 5, 3, 1, 16, 16)"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["nn.make_causal_mask(jnp.ones((2, 5, 3, 16))).shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"\"Decoder\" object has no attribute \"token_embedding\". If \"token_embedding\" is defined in '.setup()', remember these fields are only accessible from inside 'init' or 'apply'.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[47], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m    107\u001b[0m deterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/module.py:692\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], Module):\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28mself\u001b[39m, args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m], args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 692\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_wrapped_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fun(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/module.py:1224\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1223\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[38;5;28mself\u001b[39m, fun)):\n\u001b[0;32m-> 1224\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mrun_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1226\u001b[0m   y \u001b[38;5;241m=\u001b[39m run_fun(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","Cell \u001b[0;32mIn[47], line 69\u001b[0m, in \u001b[0;36mDecoder.__call__\u001b[0;34m(self, x, deterministic)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# Embedding and positional encoding\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     seq_len \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding\u001b[49m(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(jnp\u001b[38;5;241m.\u001b[39marange(seq_len))\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Create the causal mask\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mmake_causal_mask(x)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/module.py:1317\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m   msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m If \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is defined in \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m.setup()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m, remember these fields \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mare only accessible from inside \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1316\u001b[0m   )\n\u001b[0;32m-> 1317\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(msg)\n","\u001b[0;31mAttributeError\u001b[0m: \"Decoder\" object has no attribute \"token_embedding\". If \"token_embedding\" is defined in '.setup()', remember these fields are only accessible from inside 'init' or 'apply'."]}],"source":["import jax.numpy as jnp\n","import flax.linen as nn\n","from flax.linen import partitioning\n","from typing import Any\n","\n","\n","class DecoderBlock(nn.Module):\n","    num_heads: int\n","    qkv_dim: int\n","    mlp_dim: int\n","    dropout_rate: float\n","\n","    def setup(self):\n","        self.self_attention = nn.MultiHeadDotProductAttention(\n","            num_heads=self.num_heads,\n","            qkv_features=self.qkv_dim,\n","            kernel_init=nn.initializers.xavier_uniform(),\n","            bias_init=nn.initializers.normal(stddev=1e-6),\n","            dropout_rate=self.dropout_rate,\n","        )\n","        self.mlp = nn.Sequential(\n","            [nn.Dense(self.mlp_dim), nn.relu, nn.Dense(self.qkv_dim)]\n","        )\n","        self.layer_norm1 = nn.LayerNorm()\n","        self.layer_norm2 = nn.LayerNorm()\n","        self.dropout = nn.Dropout(rate=self.dropout_rate)\n","\n","    def __call__(self, x, causal_mask, deterministic):\n","        # Self-attention block\n","        x = self.layer_norm1(x)\n","        attn_out = self.self_attention(\n","            query=x, key=x, value=x, mask=causal_mask, deterministic=deterministic\n","        )\n","        x = x + self.dropout(attn_out, deterministic=deterministic)\n","\n","        # Feed-forward block\n","        x = self.layer_norm2(x)\n","        mlp_out = self.mlp(x)\n","        x = x + self.dropout(mlp_out, deterministic=deterministic)\n","\n","        return x\n","\n","\n","class Decoder(nn.Module):\n","    vocab_size: int\n","    num_layers: int\n","    num_heads: int\n","    qkv_dim: int\n","    mlp_dim: int\n","    max_len: int\n","    dropout_rate: float\n","\n","    def setup(self):\n","        self.token_embedding = nn.Embed(self.vocab_size, self.qkv_dim)\n","        self.position_embedding = nn.Embed(self.max_len, self.qkv_dim)\n","        self.decoder_blocks = [\n","            DecoderBlock(\n","                num_heads=self.num_heads,\n","                qkv_dim=self.qkv_dim,\n","                mlp_dim=self.mlp_dim,\n","                dropout_rate=self.dropout_rate,\n","            )\n","            for _ in range(self.num_layers)\n","        ]\n","\n","    def __call__(self, x, deterministic=True):\n","        # Embedding and positional encoding\n","        seq_len = x.shape[1]\n","        x = self.token_embedding(x) + self.position_embedding(jnp.arange(seq_len))\n","\n","        # Create the causal mask\n","        causal_mask = nn.make_causal_mask(x)\n","\n","        # Apply decoder blocks\n","        for block in self.decoder_blocks:\n","            x = block(x, causal_mask, deterministic)\n","\n","        return x\n","\n","\n","# Example usage\n","vocab_size = 32000\n","num_layers = 6\n","num_heads = 8\n","qkv_dim = 512\n","mlp_dim = 2048\n","max_len = 512\n","dropout_rate = 0.1\n","\n","decoder = Decoder(\n","    vocab_size=vocab_size,\n","    num_layers=num_layers,\n","    num_heads=num_heads,\n","    qkv_dim=qkv_dim,\n","    mlp_dim=mlp_dim,\n","    max_len=max_len,\n","    dropout_rate=dropout_rate,\n",")\n","\n","# Create a random input sequence of token IDs\n","import jax.random as random\n","\n","key = random.PRNGKey(0)\n","x = random.randint(key, (1, max_len), 0, vocab_size)\n","\n","# Forward pass\n","deterministic = True\n","output = decoder(x, deterministic=deterministic)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Incompatible shapes for broadcasting: shapes=[(1, 10, 1, 64, 64), (1, 8, 10, 10), ()]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/util.py:287\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 287\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/util.py:280\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 280\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:155\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 155\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(1, 10, 1, 64, 64), (1, 8, 10, 10), ()]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[48], line 59\u001b[0m\n\u001b[1;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\n\u001b[1;32m     55\u001b[0m     key, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     56\u001b[0m )  \u001b[38;5;66;03m# Example input: batch size 1, sequence length 10, embedding size 64\u001b[39;00m\n\u001b[1;32m     58\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleDecoder(num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, mlp_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(params, x)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n","    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n","Cell \u001b[0;32mIn[48], line 47\u001b[0m, in \u001b[0;36mSimpleDecoder.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m mask \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mmake_causal_mask(x)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m---> 47\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mDecoderBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n","Cell \u001b[0;32mIn[48], line 17\u001b[0m, in \u001b[0;36mDecoderBlock.__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Self-attention block\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm()(x)\n\u001b[0;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqkv_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxavier_uniform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.1\u001b[39m)(x, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m     residual \u001b[38;5;241m=\u001b[39m x\n","    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:701\u001b[0m, in \u001b[0;36mSelfAttention.__call__\u001b[0;34m(self, inputs_q, mask, deterministic, dropout_rng, sow_weights)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Applies multi-head dot product self-attention on the input data.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \n\u001b[1;32m    680\u001b[0m \u001b[38;5;124;03mProjects the inputs into multi-headed query, key, and value vectors,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;124;03m  output of shape ``[batch_sizes..., length, features]``.\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    694\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    695\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSelfAttention will be deprecated soon. Use \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    696\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`MultiHeadDotProductAttention.__call__(inputs_q)` instead. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    699\u001b[0m   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    700\u001b[0m )\n\u001b[0;32m--> 701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m  \u001b[49m\u001b[43minputs_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msow_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msow_weights\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:546\u001b[0m, in \u001b[0;36mMultiHeadDotProductAttention.__call__\u001b[0;34m(self, inputs_q, inputs_k, inputs_v, inputs_kv, mask, deterministic, dropout_rng, sow_weights)\u001b[0m\n\u001b[1;32m    532\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_fn(\n\u001b[1;32m    533\u001b[0m     query,\n\u001b[1;32m    534\u001b[0m     key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    544\u001b[0m   )  \u001b[38;5;66;03m# pytype: disable=wrong-keyword-args\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 546\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm_deterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# back to the original inputs dimensions\u001b[39;00m\n\u001b[1;32m    559\u001b[0m out \u001b[38;5;241m=\u001b[39m DenseGeneral(\n\u001b[1;32m    560\u001b[0m   features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    561\u001b[0m   axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    570\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    571\u001b[0m )(x)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:198\u001b[0m, in \u001b[0;36mdot_product_attention\u001b[0;34m(query, key, value, bias, mask, broadcast_dropout, dropout_rng, dropout_rate, deterministic, dtype, precision, module)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m key\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk, v lengths must match.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# compute attention weights\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mdot_product_attention_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m  \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m  \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m  \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# return weighted sum over values for each query position\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[1;32m    214\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...hqk,...khd->...qhd\u001b[39m\u001b[38;5;124m'\u001b[39m, attn_weights, value, precision\u001b[38;5;241m=\u001b[39mprecision\n\u001b[1;32m    215\u001b[0m )\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:112\u001b[0m, in \u001b[0;36mdot_product_attention_weights\u001b[0;34m(query, key, bias, mask, broadcast_dropout, dropout_rng, dropout_rate, deterministic, dtype, precision, module)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m   big_neg \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mfinfo(dtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[0;32m--> 112\u001b[0m   attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbig_neg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# normalize the attention weights\u001b[39;00m\n\u001b[1;32m    115\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(attn_weights)\u001b[38;5;241m.\u001b[39mastype(dtype)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:1141\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(acondition, if_true, if_false, size, fill_value, condition, x, y)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize and fill_value arguments cannot be used in three-term where function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_where\u001b[49m\u001b[43m(\u001b[49m\u001b[43macondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_false\u001b[49m\u001b[43m)\u001b[49m\n","    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:448\u001b[0m, in \u001b[0;36m_where\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    446\u001b[0m   condition \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mne(condition, lax\u001b[38;5;241m.\u001b[39m_zero(condition))\n\u001b[1;32m    447\u001b[0m x, y \u001b[38;5;241m=\u001b[39m promote_dtypes(x, y)\n\u001b[0;32m--> 448\u001b[0m condition_arr, x_arr, y_arr \u001b[38;5;241m=\u001b[39m \u001b[43m_broadcast_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m   is_always_empty \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mis_empty_shape(x_arr\u001b[38;5;241m.\u001b[39mshape)\n","    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:407\u001b[0m, in \u001b[0;36m_broadcast_arrays\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shapes \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(core\u001b[38;5;241m.\u001b[39mdefinitely_equal_shape(shapes[\u001b[38;5;241m0\u001b[39m], s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m shapes):\n\u001b[1;32m    406\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [lax\u001b[38;5;241m.\u001b[39masarray(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 407\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, result_shape) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    169\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(1, 10, 1, 64, 64), (1, 8, 10, 10), ()]"]}],"source":["import jax\n","import jax.numpy as jnp\n","from flax import linen as nn\n","from flax.training import train_state\n","from typing import Any\n","\n","\n","class DecoderBlock(nn.Module):\n","    num_heads: int\n","    embed_dim: int\n","    mlp_dim: int\n","\n","    @nn.compact\n","    def __call__(self, x, mask=None):\n","        # Self-attention block\n","        x = nn.LayerNorm()(x)\n","        x = nn.SelfAttention(\n","            num_heads=self.num_heads,\n","            qkv_features=self.embed_dim,\n","            kernel_init=nn.initializers.xavier_uniform(),\n","        )(x, mask=mask)\n","        x = nn.Dropout(0.1)(x, deterministic=True)\n","        residual = x\n","\n","        # Feed-forward block\n","        x = nn.LayerNorm()(x)\n","        x = nn.Dense(self.mlp_dim, kernel_init=nn.initializers.xavier_uniform())(x)\n","        x = nn.relu(x)\n","        x = nn.Dense(self.embed_dim, kernel_init=nn.initializers.xavier_uniform())(x)\n","        x = nn.Dropout(0.1)(x, deterministic=True)\n","\n","        return residual + x\n","\n","\n","class SimpleDecoder(nn.Module):\n","    num_layers: int\n","    num_heads: int\n","    embed_dim: int\n","    mlp_dim: int\n","\n","    @nn.compact\n","    def __call__(self, x):\n","        # Create a causal mask\n","        mask = nn.make_causal_mask(x)\n","\n","        for _ in range(self.num_layers):\n","            x = DecoderBlock(self.num_heads, self.embed_dim, self.mlp_dim)(x, mask)\n","\n","        return x\n","\n","\n","# Example usage\n","key = jax.random.PRNGKey(0)\n","x = jax.random.normal(\n","    key, (1, 10, 64)\n",")  # Example input: batch size 1, sequence length 10, embedding size 64\n","\n","model = SimpleDecoder(num_layers=2, num_heads=8, embed_dim=64, mlp_dim=256)\n","params = model.init(key, x)\n","y = model.apply(params, x)\n","\n","print(y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Incompatible shapes for broadcasting: shapes=[(1, 10, 1, 64, 64), (1, 8, 10, 10), ()]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/util.py:287\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 287\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/util.py:280\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 280\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:155\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 155\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(1, 10, 1, 64, 64), (1, 8, 10, 10), ()]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[49], line 59\u001b[0m\n\u001b[1;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\n\u001b[1;32m     55\u001b[0m     key, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     56\u001b[0m )  \u001b[38;5;66;03m# Example input: batch size 1, sequence length 10, embedding size 64\u001b[39;00m\n\u001b[1;32m     58\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleDecoder(num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, mlp_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m y \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(params, x)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n","    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n","Cell \u001b[0;32mIn[49], line 47\u001b[0m, in \u001b[0;36mSimpleDecoder.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m mask \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mmake_causal_mask(x)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m---> 47\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mDecoderBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n","    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n","Cell \u001b[0;32mIn[49], line 17\u001b[0m, in \u001b[0;36mDecoderBlock.__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Self-attention block\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLayerNorm()(x)\n\u001b[0;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqkv_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxavier_uniform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.1\u001b[39m)(x, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m     residual \u001b[38;5;241m=\u001b[39m x\n","    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:701\u001b[0m, in \u001b[0;36mSelfAttention.__call__\u001b[0;34m(self, inputs_q, mask, deterministic, dropout_rng, sow_weights)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Applies multi-head dot product self-attention on the input data.\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \n\u001b[1;32m    680\u001b[0m \u001b[38;5;124;03mProjects the inputs into multi-headed query, key, and value vectors,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;124;03m  output of shape ``[batch_sizes..., length, features]``.\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    694\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    695\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSelfAttention will be deprecated soon. Use \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    696\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`MultiHeadDotProductAttention.__call__(inputs_q)` instead. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    699\u001b[0m   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    700\u001b[0m )\n\u001b[0;32m--> 701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m  \u001b[49m\u001b[43minputs_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msow_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msow_weights\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:546\u001b[0m, in \u001b[0;36mMultiHeadDotProductAttention.__call__\u001b[0;34m(self, inputs_q, inputs_k, inputs_v, inputs_kv, mask, deterministic, dropout_rng, sow_weights)\u001b[0m\n\u001b[1;32m    532\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_fn(\n\u001b[1;32m    533\u001b[0m     query,\n\u001b[1;32m    534\u001b[0m     key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    543\u001b[0m     module\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    544\u001b[0m   )  \u001b[38;5;66;03m# pytype: disable=wrong-keyword-args\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 546\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mm_deterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# back to the original inputs dimensions\u001b[39;00m\n\u001b[1;32m    559\u001b[0m out \u001b[38;5;241m=\u001b[39m DenseGeneral(\n\u001b[1;32m    560\u001b[0m   features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    561\u001b[0m   axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    570\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m    571\u001b[0m )(x)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:198\u001b[0m, in \u001b[0;36mdot_product_attention\u001b[0;34m(query, key, value, bias, mask, broadcast_dropout, dropout_rng, dropout_rate, deterministic, dtype, precision, module)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m key\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk, v lengths must match.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# compute attention weights\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mdot_product_attention_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m  \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m  \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbroadcast_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdropout_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m  \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# return weighted sum over values for each query position\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39meinsum(\n\u001b[1;32m    214\u001b[0m   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...hqk,...khd->...qhd\u001b[39m\u001b[38;5;124m'\u001b[39m, attn_weights, value, precision\u001b[38;5;241m=\u001b[39mprecision\n\u001b[1;32m    215\u001b[0m )\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/flax/linen/attention.py:112\u001b[0m, in \u001b[0;36mdot_product_attention_weights\u001b[0;34m(query, key, bias, mask, broadcast_dropout, dropout_rng, dropout_rate, deterministic, dtype, precision, module)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m   big_neg \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mfinfo(dtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[0;32m--> 112\u001b[0m   attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbig_neg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# normalize the attention weights\u001b[39;00m\n\u001b[1;32m    115\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(attn_weights)\u001b[38;5;241m.\u001b[39mastype(dtype)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:1141\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(acondition, if_true, if_false, size, fill_value, condition, x, y)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1140\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize and fill_value arguments cannot be used in three-term where function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_where\u001b[49m\u001b[43m(\u001b[49m\u001b[43macondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_false\u001b[49m\u001b[43m)\u001b[49m\n","    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:448\u001b[0m, in \u001b[0;36m_where\u001b[0;34m(condition, x, y)\u001b[0m\n\u001b[1;32m    446\u001b[0m   condition \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mne(condition, lax\u001b[38;5;241m.\u001b[39m_zero(condition))\n\u001b[1;32m    447\u001b[0m x, y \u001b[38;5;241m=\u001b[39m promote_dtypes(x, y)\n\u001b[0;32m--> 448\u001b[0m condition_arr, x_arr, y_arr \u001b[38;5;241m=\u001b[39m \u001b[43m_broadcast_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m   is_always_empty \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mis_empty_shape(x_arr\u001b[38;5;241m.\u001b[39mshape)\n","    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/numpy/util.py:407\u001b[0m, in \u001b[0;36m_broadcast_arrays\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m shapes \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(core\u001b[38;5;241m.\u001b[39mdefinitely_equal_shape(shapes[\u001b[38;5;241m0\u001b[39m], s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m shapes):\n\u001b[1;32m    406\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [lax\u001b[38;5;241m.\u001b[39masarray(arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m--> 407\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, result_shape) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/jax/_src/lax/lax.py:171\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    169\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(1, 10, 1, 64, 64), (1, 8, 10, 10), ()]"]}],"source":["import jax\n","import jax.numpy as jnp\n","from flax import linen as nn\n","from flax.training import train_state\n","from typing import Any\n","\n","\n","class DecoderBlock(nn.Module):\n","    num_heads: int\n","    embed_dim: int\n","    mlp_dim: int\n","\n","    @nn.compact\n","    def __call__(self, x, mask=None):\n","        # Self-attention block\n","        x = nn.LayerNorm()(x)\n","        x = nn.SelfAttention(\n","            num_heads=self.num_heads,\n","            qkv_features=self.embed_dim,\n","            kernel_init=nn.initializers.xavier_uniform(),\n","        )(x, mask=mask)\n","        x = nn.Dropout(0.1)(x, deterministic=True)\n","        residual = x\n","\n","        # Feed-forward block\n","        x = nn.LayerNorm()(x)\n","        x = nn.Dense(self.mlp_dim, kernel_init=nn.initializers.xavier_uniform())(x)\n","        x = nn.relu(x)\n","        x = nn.Dense(self.embed_dim, kernel_init=nn.initializers.xavier_uniform())(x)\n","        x = nn.Dropout(0.1)(x, deterministic=True)\n","\n","        return residual + x\n","\n","\n","class SimpleDecoder(nn.Module):\n","    num_layers: int\n","    num_heads: int\n","    embed_dim: int\n","    mlp_dim: int\n","\n","    @nn.compact\n","    def __call__(self, x):\n","        # Create a causal mask\n","        mask = nn.make_causal_mask(x)\n","\n","        for _ in range(self.num_layers):\n","            x = DecoderBlock(self.num_heads, self.embed_dim, self.mlp_dim)(x, mask)\n","\n","        return x\n","\n","\n","# Example usage\n","key = jax.random.PRNGKey(0)\n","x = jax.random.normal(\n","    key, (1, 10, 64)\n",")  # Example input: batch size 1, sequence length 10, embedding size 64\n","\n","model = SimpleDecoder(num_layers=2, num_heads=8, embed_dim=64, mlp_dim=256)\n","params = model.init(key, x)\n","y = model.apply(params, x)\n","\n","print(y)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Exception encountered when calling SimpleDecoder.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: True (of type <class 'bool'>)\u001b[0m\n\nArguments received by SimpleDecoder.call():\n  • x=tf.Tensor(shape=(1, 10, 64), dtype=float32)\n  • training=True\n  • mask=tf.Tensor(shape=(1, 1, 10, 10), dtype=float32)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[50], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m causal_mask[tf\u001b[38;5;241m.\u001b[39mnewaxis, tf\u001b[38;5;241m.\u001b[39mnewaxis, :, :]  \u001b[38;5;66;03m# (1, 1, seq_len, seq_len)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Apply the decoder\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n","File \u001b[0;32m~/Hephaestus/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","Cell \u001b[0;32mIn[50], line 52\u001b[0m, in \u001b[0;36mSimpleDecoder.call\u001b[0;34m(self, x, training, mask)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, training, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m---> 52\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling SimpleDecoder.call().\n\n\u001b[1mOnly input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: True (of type <class 'bool'>)\u001b[0m\n\nArguments received by SimpleDecoder.call():\n  • x=tf.Tensor(shape=(1, 10, 64), dtype=float32)\n  • training=True\n  • mask=tf.Tensor(shape=(1, 1, 10, 10), dtype=float32)"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import (\n","    Layer,\n","    LayerNormalization,\n","    Dense,\n","    Dropout,\n","    MultiHeadAttention,\n",")\n","\n","\n","class DecoderBlock(Layer):\n","    def __init__(self, num_heads, embed_dim, mlp_dim, dropout_rate=0.1):\n","        super(DecoderBlock, self).__init__()\n","        self.num_heads = num_heads\n","        self.embed_dim = embed_dim\n","        self.mlp_dim = mlp_dim\n","        self.dropout_rate = dropout_rate\n","\n","        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = tf.keras.Sequential(\n","            [Dense(mlp_dim, activation=\"relu\"), Dense(embed_dim)]\n","        )\n","        self.layernorm1 = LayerNormalization()\n","        self.layernorm2 = LayerNormalization()\n","        self.dropout1 = Dropout(dropout_rate)\n","        self.dropout2 = Dropout(dropout_rate)\n","\n","    def call(self, x, training, mask=None):\n","        attn_output = self.attention(x, x, attention_mask=mask)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)\n","\n","        ffn_output = self.dense_proj(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        return self.layernorm2(out1 + ffn_output)\n","\n","\n","class SimpleDecoder(Layer):\n","    def __init__(self, num_layers, num_heads, embed_dim, mlp_dim, dropout_rate=0.1):\n","        super(SimpleDecoder, self).__init__()\n","        self.num_layers = num_layers\n","        self.embed_dim = embed_dim\n","        self.mlp_dim = mlp_dim\n","\n","        self.dec_layers = [\n","            DecoderBlock(num_heads, embed_dim, mlp_dim, dropout_rate)\n","            for _ in range(num_layers)\n","        ]\n","\n","    def call(self, x, training, mask=None):\n","        for i in range(self.num_layers):\n","            x = self.dec_layers[i](x, training, mask)\n","        return x\n","\n","\n","def create_causal_mask(seq_len):\n","    mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","    return mask\n","\n","\n","# Example usage\n","batch_size = 1\n","seq_len = 10\n","embed_dim = 64\n","\n","# Input tensor\n","x = tf.random.normal((batch_size, seq_len, embed_dim))\n","\n","# Create the model\n","num_layers = 2\n","num_heads = 8\n","mlp_dim = 256\n","dropout_rate = 0.1\n","\n","decoder = SimpleDecoder(num_layers, num_heads, embed_dim, mlp_dim, dropout_rate)\n","\n","# Create a causal mask\n","causal_mask = create_causal_mask(seq_len)\n","causal_mask = causal_mask[tf.newaxis, tf.newaxis, :, :]  # (1, 1, seq_len, seq_len)\n","\n","# Apply the decoder\n","output = decoder(x, training=True, mask=causal_mask)\n","\n","print(output)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP1swsXnq2jqt/Hz4IBTZm2","gpuType":"V100","machine_shape":"hm","mount_file_id":"1zHmvVqlKJh0x9vjCRSkYKkko5buvMMEd","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"01ca2dcb14e647dfb1e68750ca6de39c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a51aa7b827db4f768dce6315dbebf379","IPY_MODEL_634935ff0a6b4d65b72f87359d9f82fc","IPY_MODEL_09b63c98bee543dfb557a007d50c41d1"],"layout":"IPY_MODEL_dc4a8a3b0a8b4ef49dcd6269a1b32e14"}},"0481743e80634f7a8e718fb528950e54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"055681c8159b4b6c8104d4e06279803c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09b63c98bee543dfb557a007d50c41d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fb36a08ca1540228919af722727572c","placeholder":"​","style":"IPY_MODEL_3bd47d84fac442d4bcf49dceeb699d45","value":" 807/807 [04:29&lt;00:00,  3.10it/s]"}},"0e26d28846fb449789510e4748c01c6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25ce5408138f4cb28e163f1fdffdee5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b35a70a3b9e4f3b87cf2a7280439ac6","placeholder":"​","style":"IPY_MODEL_0e26d28846fb449789510e4748c01c6a","value":"100%"}},"2ac0cceac38e43adb00a10b778fad2df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddb1791795134ac0a754748f9793c214","placeholder":"​","style":"IPY_MODEL_d95aea2b915b44f38b5090ee186abafd","value":" 807/807 [04:31&lt;00:00,  2.80it/s]"}},"2fb36a08ca1540228919af722727572c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31d37adf891a4da68675945509051210":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_055681c8159b4b6c8104d4e06279803c","placeholder":"​","style":"IPY_MODEL_9f5e5843559b4f9e8f3440ebd85743f8","value":" 807/807 [04:52&lt;00:00,  3.03it/s]"}},"3bd47d84fac442d4bcf49dceeb699d45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4151c7353de54909aa9deecbe8c1e1e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"462d9e93a94f44868fec1ece82f0a241":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47793d3155614c8cbd7d3337dcb2f895":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8093c8ddeec4c55ac6cc5f8f7df30bf","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4151c7353de54909aa9deecbe8c1e1e1","value":3}},"4bc81f6d94394d7f90e070d8b11a7059":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4da9993585914618a35d8d5382fef850":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"4dd64f565fcf4b259ef24944493477bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_25ce5408138f4cb28e163f1fdffdee5f","IPY_MODEL_47793d3155614c8cbd7d3337dcb2f895","IPY_MODEL_f9ec11073e484927a4cc1ee2e3da132f"],"layout":"IPY_MODEL_62e750769d364ab2b80d5b9646b10ea6"}},"5e15315a77864badafe48c2baf31b030":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dfd9dd713862479bba73b6c0a12a1902","IPY_MODEL_88be535ce5a945f4979fe72d9f086365","IPY_MODEL_31d37adf891a4da68675945509051210"],"layout":"IPY_MODEL_4da9993585914618a35d8d5382fef850"}},"5edd903b07594d128ded9b4844835159":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bc81f6d94394d7f90e070d8b11a7059","max":807,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d3bc58d04d5f42a5a24bd467a9569e01","value":807}},"5f1fb4d40e154e27baa0d4f330df540e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62e750769d364ab2b80d5b9646b10ea6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"634935ff0a6b4d65b72f87359d9f82fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4de75777bba4d72b61936baf3d84cbb","max":807,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0481743e80634f7a8e718fb528950e54","value":807}},"6f5ca39406174a4fb1bee9eb1e35fccb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70c7b641da5c4b82bbd5b2e1750f3b39":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"851b29291123491a821b1ce8088ca785":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"88be535ce5a945f4979fe72d9f086365":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8382d8bbb7f42f6a0f4d4028203615f","max":807,"min":0,"orientation":"horizontal","style":"IPY_MODEL_851b29291123491a821b1ce8088ca785","value":807}},"8b35a70a3b9e4f3b87cf2a7280439ac6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"948787e6434f419a86d9d7da831e2572":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ced4485d5c641eda90ab0634e0691d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f5e5843559b4f9e8f3440ebd85743f8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a317a54aa1a349f490e388aacb2d1e4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c470a61e692a4459988f63f0024f7eac","IPY_MODEL_5edd903b07594d128ded9b4844835159","IPY_MODEL_2ac0cceac38e43adb00a10b778fad2df"],"layout":"IPY_MODEL_c7f61bdee3ca4ca0a3f7d021aa558deb"}},"a51aa7b827db4f768dce6315dbebf379":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f1fb4d40e154e27baa0d4f330df540e","placeholder":"​","style":"IPY_MODEL_948787e6434f419a86d9d7da831e2572","value":"100%"}},"acee2c1ed91e44188d5ca40bddace4b1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c470a61e692a4459988f63f0024f7eac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70c7b641da5c4b82bbd5b2e1750f3b39","placeholder":"​","style":"IPY_MODEL_462d9e93a94f44868fec1ece82f0a241","value":"100%"}},"c7f61bdee3ca4ca0a3f7d021aa558deb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"cc57eeb92d32434ca82b7c3f669865d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3bc58d04d5f42a5a24bd467a9569e01":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d95aea2b915b44f38b5090ee186abafd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc4a8a3b0a8b4ef49dcd6269a1b32e14":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"ddb1791795134ac0a754748f9793c214":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfd9dd713862479bba73b6c0a12a1902":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_acee2c1ed91e44188d5ca40bddace4b1","placeholder":"​","style":"IPY_MODEL_cc57eeb92d32434ca82b7c3f669865d1","value":"100%"}},"e4de75777bba4d72b61936baf3d84cbb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8093c8ddeec4c55ac6cc5f8f7df30bf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8382d8bbb7f42f6a0f4d4028203615f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9ec11073e484927a4cc1ee2e3da132f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ced4485d5c641eda90ab0634e0691d3","placeholder":"​","style":"IPY_MODEL_6f5ca39406174a4fb1bee9eb1e35fccb","value":" 3/3 [13:53&lt;00:00, 275.36s/it]"}}}}},"nbformat":4,"nbformat_minor":0}
